"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[8615],{5038:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>t,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module-4-vla/chapter-3","title":"Chapter 24: Voice Input Processing","description":"Implementing voice recognition and processing systems for humanoid robots","source":"@site/docs/module-4-vla/chapter-3.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-3","permalink":"/hackathon-book-robotics/docs/module-4-vla/chapter-3","draft":false,"unlisted":false,"editUrl":"https://github.com/noormusarrat/hackathon-book-robotics/edit/main/docs/module-4-vla/chapter-3.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Chapter 24: Voice Input Processing","description":"Implementing voice recognition and processing systems for humanoid robots"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 23: LLM Integration for Robotics","permalink":"/hackathon-book-robotics/docs/module-4-vla/chapter-2"},"next":{"title":"Chapter 25: Natural Language Understanding","permalink":"/hackathon-book-robotics/docs/module-4-vla/chapter-4"}}');var o=i(4848),r=i(8453);const t={sidebar_position:3,title:"Chapter 24: Voice Input Processing",description:"Implementing voice recognition and processing systems for humanoid robots"},a="Chapter 24: Voice Input Processing",c={},l=[{value:"1. Why this concept matters for humanoids",id:"1-why-this-concept-matters-for-humanoids",level:2},{value:"2. Theory",id:"2-theory",level:2},{value:"3. Implementation",id:"3-implementation",level:2},{value:"4. Hardware/GPU Notes",id:"4-hardwaregpu-notes",level:2},{value:"5. Simulation Path",id:"5-simulation-path",level:2},{value:"6. Real-World Path",id:"6-real-world-path",level:2},{value:"7. Spec-Build-Test checklist",id:"7-spec-build-test-checklist",level:2},{value:"8. APA citations",id:"8-apa-citations",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-24-voice-input-processing",children:"Chapter 24: Voice Input Processing"})}),"\n",(0,o.jsx)(n.h2,{id:"1-why-this-concept-matters-for-humanoids",children:"1. Why this concept matters for humanoids"}),"\n",(0,o.jsx)(n.p,{children:"Voice input processing is fundamental to creating natural human-robot interaction for humanoid robots. Unlike traditional robots that require physical interfaces or pre-programmed commands, humanoid robots with voice input capabilities can respond to natural human speech, making them more intuitive and accessible. This capability enables seamless communication in environments where physical interfaces might be impractical, and allows robots to understand and respond to complex, nuanced verbal commands in real-time. Voice input processing is essential for social robots, personal assistants, and collaborative robots that work alongside humans."}),"\n",(0,o.jsx)(n.h2,{id:"2-theory",children:"2. Theory"}),"\n",(0,o.jsx)(n.p,{children:"Voice input processing for robotics involves converting human speech into text that can be understood and acted upon by the robot. This process typically involves several stages:"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Audio Capture"}),": Recording speech from the environment using microphones. For humanoid robots, this often involves microphone arrays to enable spatial audio processing and noise cancellation."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Preprocessing"}),": Cleaning the audio signal by removing background noise, normalizing volume, and segmenting speech from non-speech audio. This step is crucial for improving recognition accuracy in real-world environments."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Feature Extraction"}),": Converting the audio signal into features that can be processed by speech recognition models. Common approaches include Mel-frequency cepstral coefficients (MFCCs) or spectrograms."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Automatic Speech Recognition (ASR)"}),": Converting the audio features into text using machine learning models. Modern ASR systems use deep neural networks trained on large datasets of speech."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Post-processing"}),": Cleaning and formatting the recognized text to improve accuracy and prepare it for language understanding systems."]}),"\n",(0,o.jsx)(n.p,{children:"For robotics applications, voice input processing must be robust to environmental conditions, responsive to real-time input, and integrated with the robot's overall language understanding system."}),"\n",(0,o.jsx)(n.h2,{id:"3-implementation",children:"3. Implementation"}),"\n",(0,o.jsx)(n.p,{children:"Let's implement a voice input processing node for our humanoid robot:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# vla_robot_control/vla_robot_control/voice_processor.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport pyaudio\nimport numpy as np\nimport webrtcvad\nimport collections\nimport threading\nimport queue\nimport speech_recognition as sr\nfrom vosk import Model, KaldiRecognizer\nimport json\nimport time\n\nclass VoiceProcessor(Node):\n    def __init__(self, node):\n        super().__init__(\'voice_processor\')\n        self.node = node\n\n        # Initialize speech recognition\n        self.setup_speech_recognition()\n\n        # Audio parameters\n        self.rate = 16000  # Sample rate\n        self.chunk = 1024  # Audio chunk size\n        self.channels = 1  # Mono audio\n        self.format = pyaudio.paInt16\n\n        # Voice activity detection\n        self.vad = webrtcvad.Vad(2)  # Aggressiveness mode 2\n        self.frame_duration = 30  # ms\n        self.frame_size = int(self.rate * self.frame_duration / 1000)\n\n        # Audio buffer for voice activity detection\n        self.audio_buffer = collections.deque(maxlen=30)  # 30 frames = 900ms\n        self.speech_buffer = collections.deque(maxlen=100)  # 3s of speech\n\n        # Setup publisher for recognized text\n        self.text_pub = self.node.create_publisher(String, \'/vla/command\', 10)\n\n        # Setup audio stream\n        self.audio = pyaudio.PyAudio()\n        self.stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk,\n            stream_callback=self.audio_callback\n        )\n\n        # Thread-safe queue for processing\n        self.audio_queue = queue.Queue()\n        self.processing_thread = threading.Thread(target=self.process_audio, daemon=True)\n        self.processing_thread.start()\n\n        self.get_logger().info(\'Voice Processor initialized\')\n\n    def setup_speech_recognition(self):\n        """\n        Setup speech recognition system\n        """\n        try:\n            # Option 1: Use Vosk for offline speech recognition\n            # self.model = Model("model")  # Download and specify model path\n            # self.recognizer = KaldiRecognizer(self.model, self.rate)\n\n            # Option 2: Use speech_recognition library with various backends\n            self.rec = sr.Recognizer()\n            self.microphone = sr.Microphone()\n\n            # Adjust for ambient noise\n            with self.microphone as source:\n                self.rec.adjust_for_ambient_noise(source)\n\n        except Exception as e:\n            self.node.get_logger().warn(f\'Could not initialize speech recognition: {e}\')\n            # Fallback to online recognition\n            pass\n\n    def audio_callback(self, in_data, frame_count, time_info, status):\n        """\n        Callback for audio stream\n        """\n        # Add audio data to processing queue\n        self.audio_queue.put(in_data)\n        return (None, pyaudio.paContinue)\n\n    def process_audio(self):\n        """\n        Process audio data in a separate thread\n        """\n        while rclpy.ok():\n            try:\n                # Get audio data from queue\n                if not self.audio_queue.empty():\n                    audio_data = self.audio_queue.get(timeout=0.1)\n\n                    # Convert to numpy array for processing\n                    audio_array = np.frombuffer(audio_data, dtype=np.int16)\n\n                    # Perform voice activity detection\n                    if self.is_speech(audio_array):\n                        # Add to speech buffer\n                        self.speech_buffer.extend(audio_array)\n                    else:\n                        # If we have accumulated speech, process it\n                        if len(self.speech_buffer) > 0:\n                            self.process_speech_buffer()\n                            self.speech_buffer.clear()\n                else:\n                    time.sleep(0.01)  # Sleep briefly if no data\n            except queue.Empty:\n                continue\n            except Exception as e:\n                self.node.get_logger().error(f\'Error processing audio: {e}\')\n\n    def is_speech(self, audio_data):\n        """\n        Detect if the audio frame contains speech using WebRTC VAD\n        """\n        # VAD requires 16kHz, 16-bit, mono audio\n        # We need to check if this frame contains speech\n        try:\n            # Convert to bytes for VAD\n            audio_bytes = audio_data.tobytes()\n\n            # The VAD needs smaller frames (10, 20, or 30ms)\n            # For our 1024 samples at 16kHz, that\'s about 64ms chunks\n            frame_size = self.frame_size * 2  # 2 bytes per sample\n\n            speech_detected = False\n            for i in range(0, len(audio_bytes), frame_size):\n                frame = audio_bytes[i:i+frame_size]\n                if len(frame) == frame_size:  # Ensure frame is the right size\n                    if self.vad.is_speech(frame, self.rate):\n                        speech_detected = True\n                        break\n\n            return speech_detected\n        except Exception as e:\n            self.node.get_logger().warn(f\'VAD error: {e}\')\n            return False\n\n    def process_speech_buffer(self):\n        """\n        Process accumulated speech buffer and recognize text\n        """\n        if len(self.speech_buffer) == 0:\n            return\n\n        # Convert speech buffer to audio data\n        speech_data = np.array(self.speech_buffer).astype(np.int16)\n\n        # Convert to audio data for speech recognition\n        audio_bytes = speech_data.tobytes()\n\n        try:\n            # Create audio data object for speech recognition\n            audio_data = sr.AudioData(audio_bytes, self.rate, 2)  # 2 bytes per sample\n\n            # Recognize speech using Google Speech Recognition (or alternatives)\n            # For offline recognition, you might use:\n            # text = self.recognize_offline(audio_data)\n\n            # For online recognition:\n            text = self.rec.recognize_google(audio_data)\n\n            if text and len(text.strip()) > 0:\n                self.node.get_logger().info(f\'Recognized: {text}\')\n\n                # Publish recognized text\n                msg = String()\n                msg.data = text\n                self.text_pub.publish(msg)\n\n        except sr.UnknownValueError:\n            self.node.get_logger().info(\'Speech recognition could not understand audio\')\n        except sr.RequestError as e:\n            self.node.get_logger().error(f\'Speech recognition error: {e}\')\n        except Exception as e:\n            self.node.get_logger().error(f\'Error in speech processing: {e}\')\n\n    def recognize_offline(self, audio_data):\n        """\n        Offline speech recognition using Vosk\n        """\n        # This would use the Vosk model for offline recognition\n        # result = self.recognizer.AcceptWaveform(audio_data.get_wav_data())\n        # if result:\n        #     result_json = json.loads(self.recognizer.Result())\n        #     return result_json.get("text", "")\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    # Create a dummy node to pass to VoiceProcessor\n    dummy_node = Node(\'dummy_node\')\n    voice_processor = VoiceProcessor(dummy_node)\n\n    try:\n        # Keep the node running\n        rclpy.spin(dummy_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        voice_processor.stream.stop_stream()\n        voice_processor.stream.close()\n        voice_processor.audio.terminate()\n        dummy_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.p,{children:"Create a configuration file for voice processing:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'# vla_robot_control/config/voice_processing.yaml\nvoice_processing:\n  sample_rate: 16000\n  chunk_size: 1024\n  channels: 1\n  format: paInt16\n  vad_aggressiveness: 2\n  silence_threshold: 0.5\n  min_speech_duration: 0.5  # seconds\n  max_speech_duration: 5.0  # seconds\n  language: "en-US"\n  sensitivity: 0.5\n  noise_suppression: 2\n  auto_gain_control: true\n  voice_activity_timeout: 3.0  # seconds of silence before considering speech ended\n'})}),"\n",(0,o.jsx)(n.p,{children:"Create a launch file for the voice processing system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:"\x3c!-- vla_robot_control/launch/voice_processing.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    config = os.path.join(\n        get_package_share_directory('vla_robot_control'),\n        'config',\n        'voice_processing.yaml'\n    )\n\n    return LaunchDescription([\n        Node(\n            package='vla_robot_control',\n            executable='voice_processor',\n            name='voice_processor',\n            parameters=[config],\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,o.jsx)(n.h2,{id:"4-hardwaregpu-notes",children:"4. Hardware/GPU Notes"}),"\n",(0,o.jsx)(n.p,{children:"Voice input processing for humanoid robots has specific hardware requirements:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Microphone Array"}),": Preferably 4-8 microphones for beamforming and noise cancellation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio Processing"}),": Dedicated audio processing unit or DSP for real-time processing"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"CPU Requirements"}),": Multi-core processor for concurrent audio processing and robot control"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Memory"}),": 2-4GB RAM for audio buffers and recognition models (offline)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Network"}),": Stable connection for online speech recognition (if used)"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Offline vs Online Recognition"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Offline"}),": Higher initial setup but no network dependency, privacy guaranteed"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Online"}),": Lower hardware requirements but needs network connection, potential privacy concerns"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Audio Quality Considerations"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Use high-quality microphones with wide frequency response"}),"\n",(0,o.jsx)(n.li,{children:"Implement noise cancellation for real-world environments"}),"\n",(0,o.jsx)(n.li,{children:"Consider directional microphones for focused speech capture"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"5-simulation-path",children:"5. Simulation Path"}),"\n",(0,o.jsx)(n.p,{children:"To implement voice processing in simulation:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Audio Simulation Setup"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Use Gazebo audio plugin or simulate audio input\n# Create virtual microphone that receives audio from simulation\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Voice Command Injection"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Simulate voice commands by publishing directly to the command topic\nros2 topic pub /vla/command std_msgs/String \"data: 'move forward'\"\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Testing Framework"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Create test scenarios with pre-recorded audio or text commands\nimport unittest\nfrom std_msgs.msg import String\n\nclass TestVoiceProcessor(unittest.TestCase):\n    def test_voice_command_recognition(self):\n        # Simulate voice input and verify correct text output\n        pass\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Integration Testing"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Test voice command flow from audio input to robot action"}),"\n",(0,o.jsx)(n.li,{children:"Validate response times and accuracy in simulated environment"}),"\n",(0,o.jsx)(n.li,{children:"Verify safety mechanisms work with voice commands"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"6-real-world-path",children:"6. Real-World Path"}),"\n",(0,o.jsx)(n.p,{children:"For real-world deployment:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Hardware Integration"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Mount microphone array on the robot in optimal position"}),"\n",(0,o.jsx)(n.li,{children:"Ensure proper audio routing and amplification"}),"\n",(0,o.jsx)(n.li,{children:"Test audio quality in target environments"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Calibration"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Adjust sensitivity based on environment noise levels"}),"\n",(0,o.jsx)(n.li,{children:"Calibrate for different speakers and accents"}),"\n",(0,o.jsx)(n.li,{children:"Set appropriate thresholds for voice activity detection"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Deployment Considerations"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement fallback mechanisms for recognition failures"}),"\n",(0,o.jsx)(n.li,{children:"Add privacy controls for sensitive environments"}),"\n",(0,o.jsx)(n.li,{children:"Ensure compliance with audio recording regulations"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Performance Optimization"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Optimize for real-time processing (aim for 200ms response)"}),"\n",(0,o.jsx)(n.li,{children:"Implement caching for common commands"}),"\n",(0,o.jsx)(n.li,{children:"Add adaptive noise cancellation"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"7-spec-build-test-checklist",children:"7. Spec-Build-Test checklist"}),"\n",(0,o.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Voice processing node implemented and running"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Audio capture working with microphone array"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Voice activity detection functioning correctly"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Speech recognition converting audio to text"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Recognized text published to command topic"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Noise cancellation implemented and tested"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Response time under 500ms for real-time interaction"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Accuracy tested with various speakers and accents"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Fallback mechanisms for recognition failures"]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Privacy controls implemented for audio processing"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"8-apa-citations",children:"8. APA citations"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Hinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A. R., Jaitly, N., ... & Kingsbury, B. (2012). Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. ",(0,o.jsx)(n.em,{children:"IEEE Signal Processing Magazine"}),", 29(6), 82-97."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Panayotov, V., Chen, G., Povey, D., & Khudanpur, S. (2015). Librispeech: an ASR corpus based on public domain audio books. ",(0,o.jsx)(n.em,{children:"2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"}),", 5206-5210."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Saon, G., Soltau, H., Povey, D., & Khudanpur, S. (2020). A streaming transformer transducer approach for end-to-end speech recognition. ",(0,o.jsx)(n.em,{children:"arXiv preprint arXiv:2006.14941"}),"."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Vinyals, O., Senior, A., Vozila, K., & Jaitly, N. (2017). Grammar as a foreign language. ",(0,o.jsx)(n.em,{children:"Advances in Neural Information Processing Systems"}),", 30."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Zhang, Y., Pezeshki, M., Brakel, P., Zhang, S., Bengio, Y., & Pal, C. (2017). Towards end-to-end speech recognition with deep convolutional neural networks. ",(0,o.jsx)(n.em,{children:"arXiv preprint arXiv:1701.02720"}),"."]}),"\n"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var s=i(6540);const o={},r=s.createContext(o);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);