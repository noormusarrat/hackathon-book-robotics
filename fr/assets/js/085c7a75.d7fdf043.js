"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[4783],{6678:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-2-simulation/chapter-4","title":"Chapter 11 - Unity for Robotics Visualization","description":"Why This Concept Matters for Humanoids","source":"@site/docs/module-2-simulation/chapter-4.md","sourceDirName":"module-2-simulation","slug":"/module-2-simulation/chapter-4","permalink":"/hackathon-book-robotics/fr/docs/module-2-simulation/chapter-4","draft":false,"unlisted":false,"editUrl":"https://github.com/noormusarrat/hackathon-book-robotics/edit/main/docs/module-2-simulation/chapter-4.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"title":"Chapter 11 - Unity for Robotics Visualization","sidebar_position":11},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 10 - Physics and Sensors in Gazebo","permalink":"/hackathon-book-robotics/fr/docs/module-2-simulation/chapter-3"},"next":{"title":"Chapter 12 - Digital Twin Creation Process","permalink":"/hackathon-book-robotics/fr/docs/module-2-simulation/chapter-5"}}');var o=i(4848),a=i(8453);const s={title:"Chapter 11 - Unity for Robotics Visualization",sidebar_position:11},r="Chapter 11: Unity for Robotics Visualization",l={},c=[{value:"Why This Concept Matters for Humanoids",id:"why-this-concept-matters-for-humanoids",level:2},{value:"Theory",id:"theory",level:2},{value:"Unity-Robotics Bridge Architecture",id:"unity-robotics-bridge-architecture",level:3},{value:"Visualization Principles",id:"visualization-principles",level:3},{value:"Mixed Reality Applications",id:"mixed-reality-applications",level:3},{value:"Real-time Rendering Techniques",id:"real-time-rendering-techniques",level:3},{value:"Implementation",id:"implementation",level:2},{value:"Unity ROS-TCP-Connector Integration",id:"unity-ros-tcp-connector-integration",level:3},{value:"Unity Robot Visualization Controller",id:"unity-robot-visualization-controller",level:3},{value:"Unity Teleoperation Interface",id:"unity-teleoperation-interface",level:3},{value:"Unity Mixed Reality Interface",id:"unity-mixed-reality-interface",level:3},{value:"Hardware/GPU Notes",id:"hardwaregpu-notes",level:2},{value:"Minimum Requirements",id:"minimum-requirements",level:3},{value:"Recommended Specifications for Humanoid Robotics",id:"recommended-specifications-for-humanoid-robotics",level:3},{value:"Mixed Reality Requirements",id:"mixed-reality-requirements",level:3},{value:"Simulation Path",id:"simulation-path",level:2},{value:"Initial Setup",id:"initial-setup",level:3},{value:"Basic Visualization",id:"basic-visualization",level:3},{value:"Advanced Visualization",id:"advanced-visualization",level:3},{value:"Validation Process",id:"validation-process",level:3},{value:"Real-World Path",id:"real-world-path",level:2},{value:"Integration Preparation",id:"integration-preparation",level:3},{value:"Hardware Integration",id:"hardware-integration",level:3},{value:"Deployment Strategy",id:"deployment-strategy",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Spec-Build-Test Checklist",id:"spec-build-test-checklist",level:2},{value:"APA Citations",id:"apa-citations",level:2}];function d(n){const e={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"chapter-11-unity-for-robotics-visualization",children:"Chapter 11: Unity for Robotics Visualization"})}),"\n",(0,o.jsx)(e.h2,{id:"why-this-concept-matters-for-humanoids",children:"Why This Concept Matters for Humanoids"}),"\n",(0,o.jsx)(e.p,{children:"Unity provides powerful visualization capabilities that are essential for humanoid robotics development, offering real-time 3D rendering, mixed reality interfaces, and immersive monitoring tools. For humanoid robots, which have complex kinematic structures and require precise coordination of multiple subsystems, Unity enables developers to create intuitive visualization interfaces that help understand robot behavior, debug complex systems, and provide teleoperation capabilities. Unity's advanced rendering capabilities, physics simulation, and user interaction systems make it ideal for creating digital twins, remote monitoring interfaces, and mixed reality applications that bridge the gap between simulation and real-world operation."}),"\n",(0,o.jsx)(e.h2,{id:"theory",children:"Theory"}),"\n",(0,o.jsx)(e.p,{children:"Unity integration with robotics involves several key concepts that enable powerful visualization and interaction capabilities:"}),"\n",(0,o.jsx)(e.h3,{id:"unity-robotics-bridge-architecture",children:"Unity-Robotics Bridge Architecture"}),"\n",(0,o.jsx)(e.p,{children:"The integration layer includes:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"ROS-TCP-Connector"}),": Communication bridge between Unity and ROS 2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Message conversion"}),": Standardized conversion between Unity and ROS 2 data types"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Synchronization protocols"}),": Time and state synchronization mechanisms"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Asset integration"}),": 3D models and environments shared between systems"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"visualization-principles",children:"Visualization Principles"}),"\n",(0,o.jsx)(e.p,{children:"Effective robotics visualization requires understanding:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Camera systems"}),": Multiple viewpoints for comprehensive monitoring"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Overlay interfaces"}),": Information display without obstructing view"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Interactive controls"}),": Direct manipulation of robot systems"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Performance optimization"}),": Maintaining real-time rendering with complex scenes"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"mixed-reality-applications",children:"Mixed Reality Applications"}),"\n",(0,o.jsx)(e.p,{children:"Unity enables mixed reality robotics applications through:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"AR/VR integration"}),": Immersive robot monitoring and control"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Spatial mapping"}),": Integration with real-world environments"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Gesture recognition"}),": Natural human-robot interaction"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Haptic feedback"}),": Tactile response for remote operation"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"real-time-rendering-techniques",children:"Real-time Rendering Techniques"}),"\n",(0,o.jsx)(e.p,{children:"For robotics visualization, Unity employs:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Level of Detail (LOD)"}),": Adaptive detail based on distance"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Occlusion culling"}),": Hiding objects not in view"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Dynamic batching"}),": Optimizing rendering of similar objects"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Shader optimization"}),": Efficient visual effects for robot data"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"implementation",children:"Implementation"}),"\n",(0,o.jsx)(e.p,{children:"Let's implement Unity-based visualization for humanoid robotics:"}),"\n",(0,o.jsx)(e.h3,{id:"unity-ros-tcp-connector-integration",children:"Unity ROS-TCP-Connector Integration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'// Assets/Scripts/RobotConnectionManager.cs\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Std_msgs;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Sensor_msgs;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Geometry_msgs;\nusing System.Collections;\nusing System.Collections.Generic;\n\npublic class RobotConnectionManager : MonoBehaviour\n{\n    [Header("ROS Connection")]\n    public string rosIPAddress = "127.0.0.1";\n    public int rosPort = 10000;\n\n    [Header("Robot Topics")]\n    public string jointStateTopic = "/joint_states";\n    public string imuTopic = "/imu/data";\n    public string cameraTopic = "/camera/image_raw";\n\n    private ROSConnection ros;\n    private JointStateData currentJointState;\n    private ImuData currentImuData;\n\n    // Robot joint transforms\n    public Transform headJoint;\n    public Transform leftShoulderJoint;\n    public Transform leftElbowJoint;\n    public Transform rightShoulderJoint;\n    public Transform rightElbowJoint;\n    public Transform leftHipJoint;\n    public Transform leftKneeJoint;\n    public Transform leftAnkleJoint;\n    public Transform rightHipJoint;\n    public Transform rightKneeJoint;\n    public Transform rightAnkleJoint;\n\n    private Dictionary<string, Transform> jointMap;\n\n    void Start()\n    {\n        // Initialize ROS connection\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.Initialize(rosIPAddress, rosPort);\n\n        // Subscribe to topics\n        ros.Subscribe<sensor_msgs_JointState>(jointStateTopic, OnJointStateReceived);\n        ros.Subscribe<sensor_msgs_Imu>(imuTopic, OnImuReceived);\n        ros.Subscribe<sensor_msgs_Image>(cameraTopic, OnCameraReceived);\n\n        // Initialize joint mapping\n        InitializeJointMap();\n\n        // Start data publishing\n        StartCoroutine(PublishRobotCommands());\n    }\n\n    void InitializeJointMap()\n    {\n        jointMap = new Dictionary<string, Transform>\n        {\n            {"head_joint", headJoint},\n            {"left_shoulder_joint", leftShoulderJoint},\n            {"left_elbow_joint", leftElbowJoint},\n            {"right_shoulder_joint", rightShoulderJoint},\n            {"right_elbow_joint", rightElbowJoint},\n            {"left_hip_joint", leftHipJoint},\n            {"left_knee_joint", leftKneeJoint},\n            {"left_ankle_joint", leftAnkleJoint},\n            {"right_hip_joint", rightHipJoint},\n            {"right_knee_joint", rightKneeJoint},\n            {"right_ankle_joint", rightAnkleJoint}\n        };\n    }\n\n    void OnJointStateReceived(sensor_msgs_JointState jointState)\n    {\n        currentJointState = new JointStateData\n        {\n            names = jointState.name,\n            positions = jointState.position,\n            velocities = jointState.velocity,\n            efforts = jointState.effort\n        };\n\n        // Update robot visualization\n        UpdateRobotVisualization();\n    }\n\n    void OnImuReceived(sensor_msgs_Imu imu)\n    {\n        currentImuData = new ImuData\n        {\n            orientation = imu.orientation,\n            angularVelocity = imu.angular_velocity,\n            linearAcceleration = imu.linear_acceleration\n        };\n\n        // Update IMU visualization\n        UpdateImuVisualization();\n    }\n\n    void OnCameraReceived(sensor_msgs_Image image)\n    {\n        // Process camera image for visualization\n        ProcessCameraImage(image);\n    }\n\n    void UpdateRobotVisualization()\n    {\n        if (currentJointState == null || currentJointState.names == null) return;\n\n        for (int i = 0; i < currentJointState.names.Length; i++)\n        {\n            string jointName = currentJointState.names[i];\n            float jointPosition = (float)currentJointState.positions[i];\n\n            if (jointMap.ContainsKey(jointName))\n            {\n                Transform jointTransform = jointMap[jointName];\n\n                // Apply rotation based on joint position\n                // For revolute joints, we typically apply rotation around a specific axis\n                jointTransform.localRotation = Quaternion.Euler(0, jointPosition * Mathf.Rad2Deg, 0);\n            }\n        }\n    }\n\n    void UpdateImuVisualization()\n    {\n        if (currentImuData == null) return;\n\n        // Update robot orientation based on IMU data\n        // Convert ROS quaternion to Unity quaternion (coordinate system conversion)\n        Vector3 unityEuler = RosQuaternionToUnityEuler(currentImuData.orientation);\n        transform.rotation = Quaternion.Euler(unityEuler);\n    }\n\n    Vector3 RosQuaternionToUnityEuler(Unity.Robotics.ROSTCPConnector.MessageTypes.Geometry_msgs.Quaternion rosQuat)\n    {\n        // Convert ROS quaternion to Unity Euler angles\n        // ROS uses Z-up, Unity uses Y-up coordinate system\n        float x = (float)rosQuat.x;\n        float y = (float)rosQuat.y;\n        float z = (float)rosQuat.z;\n        float w = (float)rosQuat.w;\n\n        // Convert to Euler angles\n        Vector3 eulerAngles = new Vector3();\n\n        // Yaw (Z axis rotation)\n        eulerAngles.y = Mathf.Atan2(2 * (w * z + x * y), 1 - 2 * (y * y + z * z)) * Mathf.Rad2Deg;\n\n        // Pitch (Y axis rotation)\n        eulerAngles.x = Mathf.Atan2(2 * (w * y - z * x), Mathf.Sqrt(1 + 2 * (w * y - z * x) * (w * y - z * x))) * Mathf.Rad2Deg;\n\n        // Roll (X axis rotation)\n        eulerAngles.z = Mathf.Asin(2 * (w * x + y * z)) * Mathf.Rad2Deg;\n\n        return eulerAngles;\n    }\n\n    void ProcessCameraImage(sensor_msgs_Image image)\n    {\n        // Process camera image data for display\n        // This would typically involve converting ROS image format to Unity texture\n        // Implementation depends on image format and display requirements\n    }\n\n    IEnumerator PublishRobotCommands()\n    {\n        // Publish commands at fixed rate\n        float publishRate = 0.1f; // 10 Hz\n\n        while (true)\n        {\n            // Publish robot state for monitoring\n            std_msgs_Float64MultiArray stateMsg = new std_msgs_Float64MultiArray();\n\n            // Fill with current joint positions if available\n            if (currentJointState != null && currentJointState.positions != null)\n            {\n                stateMsg.data = new double[currentJointState.positions.Length];\n                for (int i = 0; i < currentJointState.positions.Length; i++)\n                {\n                    stateMsg.data[i] = currentJointState.positions[i];\n                }\n            }\n\n            ros.Publish("/robot_state_monitor", stateMsg);\n\n            yield return new WaitForSeconds(publishRate);\n        }\n    }\n\n    // Data structures for storing robot state\n    [System.Serializable]\n    public class JointStateData\n    {\n        public string[] names;\n        public double[] positions;\n        public double[] velocities;\n        public double[] efforts;\n    }\n\n    [System.Serializable]\n    public class ImuData\n    {\n        public Unity.Robotics.ROSTCPConnector.MessageTypes.Geometry_msgs.Quaternion orientation;\n        public Unity.Robotics.ROSTCPConnector.MessageTypes.Geometry_msgs.Vector3 angularVelocity;\n        public Unity.Robotics.ROSTCPConnector.MessageTypes.Geometry_msgs.Vector3 linearAcceleration;\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"unity-robot-visualization-controller",children:"Unity Robot Visualization Controller"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'// Assets/Scripts/RobotVisualizationController.cs\nusing UnityEngine;\nusing UnityEngine.UI;\nusing System.Collections.Generic;\n\npublic class RobotVisualizationController : MonoBehaviour\n{\n    [Header("Visualization Settings")]\n    public bool showTrajectory = true;\n    public bool showSensors = true;\n    public bool showForces = true;\n    public float trajectoryTrailTime = 10.0f;\n\n    [Header("UI Elements")]\n    public Text statusText;\n    public Text jointInfoText;\n    public Text sensorInfoText;\n    public Slider timeScaleSlider;\n\n    [Header("Visualization Prefabs")]\n    public GameObject trajectoryPointPrefab;\n    public GameObject sensorRangeVisual;\n    public GameObject forceVectorVisual;\n\n    private LineRenderer trajectoryLine;\n    private List<Vector3> trajectoryPoints;\n    private RobotConnectionManager connectionManager;\n    private float timeScale = 1.0f;\n\n    void Start()\n    {\n        // Initialize trajectory visualization\n        InitializeTrajectory();\n\n        // Get references\n        connectionManager = FindObjectOfType<RobotConnectionManager>();\n\n        // Setup UI\n        SetupUI();\n\n        // Initialize visualization elements\n        InitializeVisualizationElements();\n    }\n\n    void InitializeTrajectory()\n    {\n        trajectoryLine = gameObject.AddComponent<LineRenderer>();\n        trajectoryLine.material = new Material(Shader.Find("Sprites/Default"));\n        trajectoryLine.widthMultiplier = 0.05f;\n        trajectoryLine.positionCount = 0;\n\n        trajectoryPoints = new List<Vector3>();\n    }\n\n    void SetupUI()\n    {\n        if (timeScaleSlider != null)\n        {\n            timeScaleSlider.onValueChanged.AddListener(OnTimeScaleChanged);\n            timeScaleSlider.value = 1.0f;\n        }\n    }\n\n    void InitializeVisualizationElements()\n    {\n        // Initialize sensor range visualizations\n        if (showSensors)\n        {\n            InitializeSensorVisualizations();\n        }\n\n        // Initialize force vector visualizations\n        if (showForces)\n        {\n            InitializeForceVisualizations();\n        }\n    }\n\n    void InitializeSensorVisualizations()\n    {\n        // Create visual representations for robot sensors\n        // This would include camera FOV, LiDAR ranges, etc.\n        GameObject[] sensorPoints = GameObject.FindGameObjectsWithTag("SensorPoint");\n\n        foreach (GameObject sensorPoint in sensorPoints)\n        {\n            GameObject sensorVisual = Instantiate(sensorRangeVisual, sensorPoint.transform);\n            sensorVisual.SetActive(showSensors);\n        }\n    }\n\n    void InitializeForceVisualizations()\n    {\n        // Create visual representations for forces and torques\n        // This would show contact forces, applied torques, etc.\n    }\n\n    void OnTimeScaleChanged(float value)\n    {\n        timeScale = value;\n        Time.timeScale = timeScale;\n    }\n\n    void Update()\n    {\n        // Update trajectory visualization\n        UpdateTrajectory();\n\n        // Update UI information\n        UpdateUIInformation();\n\n        // Update sensor visualizations\n        UpdateSensorVisualizations();\n\n        // Update force visualizations\n        UpdateForceVisualizations();\n    }\n\n    void UpdateTrajectory()\n    {\n        if (!showTrajectory) return;\n\n        // Add current position to trajectory\n        Vector3 currentPosition = transform.position;\n\n        // Remove old points based on time\n        float currentTime = Time.time;\n        trajectoryPoints.Add(currentPosition);\n\n        // Limit number of points to prevent memory issues\n        if (trajectoryPoints.Count > 1000)\n        {\n            trajectoryPoints.RemoveAt(0);\n        }\n\n        // Update line renderer\n        trajectoryLine.positionCount = trajectoryPoints.Count;\n        trajectoryLine.SetPositions(trajectoryPoints.ToArray());\n    }\n\n    void UpdateUIInformation()\n    {\n        if (statusText != null)\n        {\n            statusText.text = $"Time Scale: {timeScale:F2}x\\n" +\n                             $"Trajectory Points: {trajectoryPoints.Count}\\n" +\n                             $"Status: Connected";\n        }\n\n        if (jointInfoText != null && connectionManager != null)\n        {\n            var jointState = connectionManager.currentJointState;\n            if (jointState != null && jointState.names != null)\n            {\n                string jointInfo = "Joint Positions:\\n";\n                for (int i = 0; i < Mathf.Min(5, jointState.names.Length); i++)\n                {\n                    jointInfo += $"{jointState.names[i]}: {jointState.positions[i]:F3}\\n";\n                }\n                jointInfoText.text = jointInfo;\n            }\n        }\n\n        if (sensorInfoText != null && connectionManager != null)\n        {\n            var imuData = connectionManager.currentImuData;\n            if (imuData != null)\n            {\n                sensorInfoText.text = $"IMU Orientation:\\n" +\n                                     $"X: {imuData.orientation.x:F3}\\n" +\n                                     $"Y: {imuData.orientation.y:F3}\\n" +\n                                     $"Z: {imuData.orientation.z:F3}\\n" +\n                                     $"W: {imuData.orientation.w:F3}";\n            }\n        }\n    }\n\n    void UpdateSensorVisualizations()\n    {\n        if (!showSensors) return;\n\n        // Update sensor range visualizations based on real-time data\n        // This would update camera FOV, LiDAR ranges, etc. based on current sensor data\n    }\n\n    void UpdateForceVisualizations()\n    {\n        if (!showForces) return;\n\n        // Update force vector visualizations\n        // This would show contact forces, applied torques, etc. in real-time\n    }\n\n    public void ToggleTrajectory()\n    {\n        showTrajectory = !showTrajectory;\n        trajectoryLine.enabled = showTrajectory;\n    }\n\n    public void ToggleSensors()\n    {\n        showSensors = !showSensors;\n        GameObject[] sensorVisuals = GameObject.FindGameObjectsWithTag("SensorVisual");\n        foreach (GameObject sensorVisual in sensorVisuals)\n        {\n            sensorVisual.SetActive(showSensors);\n        }\n    }\n\n    public void ToggleForces()\n    {\n        showForces = !showForces;\n        GameObject[] forceVisuals = GameObject.FindGameObjectsWithTag("ForceVisual");\n        foreach (GameObject forceVisual in forceVisuals)\n        {\n            forceVisual.SetActive(showForces);\n        }\n    }\n\n    public void ClearTrajectory()\n    {\n        trajectoryPoints.Clear();\n        trajectoryLine.positionCount = 0;\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"unity-teleoperation-interface",children:"Unity Teleoperation Interface"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'// Assets/Scripts/RobotTeleoperationController.cs\nusing UnityEngine;\nusing UnityEngine.UI;\nusing Unity.Robotics.ROSTCPConnector;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Std_msgs;\nusing Unity.Robotics.ROSTCPConnector.MessageTypes.Geometry_msgs;\n\npublic class RobotTeleoperationController : MonoBehaviour\n{\n    [Header("Teleoperation Settings")]\n    public string commandTopic = "/joint_commands";\n    public float moveSpeed = 1.0f;\n    public float rotationSpeed = 1.0f;\n\n    [Header("UI Elements")]\n    public Button startButton;\n    public Button stopButton;\n    public Slider leftHipSlider;\n    public Slider rightHipSlider;\n    public Slider leftKneeSlider;\n    public Slider rightKneeSlider;\n    public Slider leftAnkleSlider;\n    public Slider rightAnkleSlider;\n    public Toggle balanceModeToggle;\n\n    private ROSConnection ros;\n    private bool isTeleoperating = false;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n\n        // Setup UI event handlers\n        SetupUIEventHandlers();\n    }\n\n    void SetupUIEventHandlers()\n    {\n        if (startButton != null)\n            startButton.onClick.AddListener(StartTeleoperation);\n\n        if (stopButton != null)\n            stopButton.onClick.AddListener(StopTeleoperation);\n\n        // Setup slider value changed events\n        if (leftHipSlider != null)\n            leftHipSlider.onValueChanged.AddListener(OnLeftHipChanged);\n\n        if (rightHipSlider != null)\n            rightHipSlider.onValueChanged.AddListener(OnRightHipChanged);\n\n        if (leftKneeSlider != null)\n            leftKneeSlider.onValueChanged.AddListener(OnLeftKneeChanged);\n\n        if (rightKneeSlider != null)\n            rightKneeSlider.onValueChanged.AddListener(OnRightKneeChanged);\n\n        if (leftAnkleSlider != null)\n            leftAnkleSlider.onValueChanged.AddListener(OnLeftAnkleChanged);\n\n        if (rightAnkleSlider != null)\n            rightAnkleSlider.onValueChanged.AddListener(OnRightAnkleChanged);\n\n        if (balanceModeToggle != null)\n            balanceModeToggle.onValueChanged.AddListener(OnBalanceModeChanged);\n    }\n\n    void StartTeleoperation()\n    {\n        isTeleoperating = true;\n        SendEmergencyStop(false);\n    }\n\n    void StopTeleoperation()\n    {\n        isTeleoperating = false;\n        SendEmergencyStop(true);\n\n        // Reset sliders to neutral position\n        if (leftHipSlider != null) leftHipSlider.value = 0.0f;\n        if (rightHipSlider != null) rightHipSlider.value = 0.0f;\n        if (leftKneeSlider != null) leftKneeSlider.value = 0.0f;\n        if (rightKneeSlider != null) rightKneeSlider.value = 0.0f;\n        if (leftAnkleSlider != null) leftAnkleSlider.value = 0.0f;\n        if (rightAnkleSlider != null) rightAnkleSlider.value = 0.0f;\n    }\n\n    void OnLeftHipChanged(float value)\n    {\n        if (isTeleoperating)\n            SendJointCommand("left_hip_joint", value);\n    }\n\n    void OnRightHipChanged(float value)\n    {\n        if (isTeleoperating)\n            SendJointCommand("right_hip_joint", value);\n    }\n\n    void OnLeftKneeChanged(float value)\n    {\n        if (isTeleoperating)\n            SendJointCommand("left_knee_joint", value);\n    }\n\n    void OnRightKneeChanged(float value)\n    {\n        if (isTeleoperating)\n            SendJointCommand("right_knee_joint", value);\n    }\n\n    void OnLeftAnkleChanged(float value)\n    {\n        if (isTeleoperating)\n            SendJointCommand("left_ankle_joint", value);\n    }\n\n    void OnRightAnkleChanged(float value)\n    {\n        if (isTeleoperating)\n            SendJointCommand("right_ankle_joint", value);\n    }\n\n    void OnBalanceModeChanged(bool isOn)\n    {\n        // Send balance mode command to robot\n        std_msgs_Bool balanceModeMsg = new std_msgs_Bool();\n        balanceModeMsg.data = isOn;\n\n        ros.Publish("/balance_mode", balanceModeMsg);\n    }\n\n    void SendJointCommand(string jointName, float position)\n    {\n        // Create and send joint command message\n        // This would typically be a trajectory message for smooth motion\n        geometry_msgs_Twist cmd = new geometry_msgs_Twist();\n\n        // For simplicity, using Twist message (in practice, use JointTrajectory)\n        cmd.linear.x = position; // Use linear component for position command\n\n        ros.Publish(commandTopic, cmd);\n    }\n\n    void SendEmergencyStop(bool stop)\n    {\n        std_msgs_Bool stopMsg = new std_msgs_Bool();\n        stopMsg.data = stop;\n\n        ros.Publish("/emergency_stop", stopMsg);\n    }\n\n    void Update()\n    {\n        // Handle keyboard input for teleoperation\n        HandleKeyboardInput();\n    }\n\n    void HandleKeyboardInput()\n    {\n        if (!isTeleoperating) return;\n\n        // Example keyboard controls\n        if (Input.GetKeyDown(KeyCode.Space))\n        {\n            StopTeleoperation();\n        }\n\n        // Additional keyboard controls could be added here\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"unity-mixed-reality-interface",children:"Unity Mixed Reality Interface"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'// Assets/Scripts/MixedRealityRobotInterface.cs\nusing UnityEngine;\nusing UnityEngine.XR.ARFoundation;\nusing UnityEngine.XR.ARSubsystems;\nusing UnityEngine.UI;\n\npublic class MixedRealityRobotInterface : MonoBehaviour\n{\n    [Header("AR Components")]\n    public ARSession arSession;\n    public ARSessionOrigin arSessionOrigin;\n    public ARRaycastManager raycastManager;\n    public Camera arCamera;\n\n    [Header("Robot Prefab")]\n    public GameObject robotPrefab;\n    public GameObject controlPanelPrefab;\n\n    [Header("UI Elements")]\n    public Button spawnRobotButton;\n    public Button followModeButton;\n    public Text statusText;\n\n    private GameObject spawnedRobot;\n    private GameObject controlPanel;\n    private bool followMode = false;\n    private Vector3 robotOffset = Vector3.zero;\n\n    private List<ARRaycastHit> raycastHits = new List<ARRaycastHit>();\n\n    void Start()\n    {\n        SetupUIEventHandlers();\n    }\n\n    void SetupUIEventHandlers()\n    {\n        if (spawnRobotButton != null)\n            spawnRobotButton.onClick.AddListener(SpawnRobot);\n\n        if (followModeButton != null)\n            followModeButton.onClick.AddListener(ToggleFollowMode);\n    }\n\n    void SpawnRobot()\n    {\n        // Raycast to find a suitable surface to place the robot\n        if (raycastManager.Raycast(new Vector2(Screen.width / 2, Screen.height / 2), raycastHits, TrackableType.PlaneWithinPolygon))\n        {\n            Pose hitPose = raycastHits[0].pose;\n\n            // Spawn robot at the hit position\n            if (spawnedRobot == null && robotPrefab != null)\n            {\n                spawnedRobot = Instantiate(robotPrefab, hitPose.position, hitPose.rotation);\n\n                // Attach robot connection manager to the spawned robot\n                RobotConnectionManager connectionManager = spawnedRobot.AddComponent<RobotConnectionManager>();\n\n                // Spawn control panel\n                if (controlPanelPrefab != null)\n                {\n                    Vector3 panelPosition = hitPose.position + new Vector3(0, 0.5f, 0);\n                    controlPanel = Instantiate(controlPanelPrefab, panelPosition, Quaternion.identity);\n\n                    // Parent control panel to AR camera so it follows the view\n                    controlPanel.transform.SetParent(arSessionOrigin.transform);\n                }\n\n                if (statusText != null)\n                    statusText.text = "Robot spawned in AR";\n            }\n        }\n    }\n\n    void ToggleFollowMode()\n    {\n        followMode = !followMode;\n\n        if (followMode && spawnedRobot != null)\n        {\n            // Calculate offset between camera and robot\n            robotOffset = spawnedRobot.transform.position - arCamera.transform.position;\n\n            if (statusText != null)\n                statusText.text = "Follow mode: ON";\n        }\n        else\n        {\n            if (statusText != null)\n                statusText.text = "Follow mode: OFF";\n        }\n    }\n\n    void Update()\n    {\n        // Update follow mode if active\n        if (followMode && spawnedRobot != null)\n        {\n            // Keep robot at fixed offset from camera\n            spawnedRobot.transform.position = arCamera.transform.position + robotOffset;\n        }\n\n        // Update UI based on AR tracking status\n        UpdateTrackingStatus();\n    }\n\n    void UpdateTrackingStatus()\n    {\n        if (arSession == null || statusText == null) return;\n\n        if (arSession.state == ARSessionState.SessionTracking)\n        {\n            statusText.text += "\\nAR Tracking: Active";\n        }\n        else\n        {\n            statusText.text += "\\nAR Tracking: Not available";\n        }\n    }\n\n    public void MoveRobotInAR(Vector3 direction)\n    {\n        if (spawnedRobot != null)\n        {\n            spawnedRobot.transform.position += direction * Time.deltaTime * 0.5f;\n        }\n    }\n\n    public void RotateRobotInAR(Vector3 axis, float angle)\n    {\n        if (spawnedRobot != null)\n        {\n            spawnedRobot.transform.Rotate(axis, angle * Time.deltaTime);\n        }\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"hardwaregpu-notes",children:"Hardware/GPU Notes"}),"\n",(0,o.jsx)(e.p,{children:"Unity visualization for humanoid robotics has specific hardware requirements:"}),"\n",(0,o.jsx)(e.h3,{id:"minimum-requirements",children:"Minimum Requirements"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"CPU"}),": Quad-core processor (8+ cores recommended for complex scenes)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Memory"}),": 8GB RAM minimum, 16GB+ recommended for detailed humanoid models"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"GPU"}),": DirectX 11 or OpenGL 4.5+ compatible graphics card"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"VRAM"}),": 4GB+ for basic humanoid visualization"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"recommended-specifications-for-humanoid-robotics",children:"Recommended Specifications for Humanoid Robotics"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"CPU"}),": 8+ cores for real-time rendering and physics simulation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Memory"}),": 32GB+ for complex scenes with multiple robots and environments"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"GPU"}),": NVIDIA RTX 4070 Ti or equivalent with CUDA support"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"VRAM"}),": 12GB+ for detailed humanoid models with advanced shaders"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"mixed-reality-requirements",children:"Mixed Reality Requirements"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"AR/VR Headset"}),": Compatible with Unity XR systems"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Processing Power"}),": Additional power for real-time environment mapping"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Sensors"}),": IMU and camera tracking capabilities"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Latency"}),": Low latency for responsive AR/VR experience"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"simulation-path",children:"Simulation Path"}),"\n",(0,o.jsx)(e.p,{children:"For developing Unity-based visualization for humanoid robotics:"}),"\n",(0,o.jsx)(e.h3,{id:"initial-setup",children:"Initial Setup"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Install Unity 2022.3 LTS with Robotics packages"}),"\n",(0,o.jsx)(e.li,{children:"Set up ROS-TCP-Connector for Unity-ROS communication"}),"\n",(0,o.jsx)(e.li,{children:"Import humanoid robot 3D models and animations"}),"\n",(0,o.jsx)(e.li,{children:"Configure basic visualization scene"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"basic-visualization",children:"Basic Visualization"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Implement joint state visualization"}),"\n",(0,o.jsx)(e.li,{children:"Add sensor data visualization"}),"\n",(0,o.jsx)(e.li,{children:"Create trajectory and path visualization"}),"\n",(0,o.jsx)(e.li,{children:"Implement basic UI controls"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"advanced-visualization",children:"Advanced Visualization"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Add mixed reality capabilities"}),"\n",(0,o.jsx)(e.li,{children:"Implement teleoperation interfaces"}),"\n",(0,o.jsx)(e.li,{children:"Create advanced sensor visualization"}),"\n",(0,o.jsx)(e.li,{children:"Add physics-based interactions"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"validation-process",children:"Validation Process"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Test visualization accuracy against real data"}),"\n",(0,o.jsx)(e.li,{children:"Validate real-time performance"}),"\n",(0,o.jsx)(e.li,{children:"Check communication reliability"}),"\n",(0,o.jsx)(e.li,{children:"Ensure safety in teleoperation scenarios"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"real-world-path",children:"Real-World Path"}),"\n",(0,o.jsx)(e.p,{children:"Transitioning from Unity simulation to real hardware:"}),"\n",(0,o.jsx)(e.h3,{id:"integration-preparation",children:"Integration Preparation"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Map Unity coordinate systems to real robot"}),"\n",(0,o.jsx)(e.li,{children:"Validate sensor data mapping accuracy"}),"\n",(0,o.jsx)(e.li,{children:"Test communication protocols reliability"}),"\n",(0,o.jsx)(e.li,{children:"Verify safety systems in visualization"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"hardware-integration",children:"Hardware Integration"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Connect Unity to real robot sensors"}),"\n",(0,o.jsx)(e.li,{children:"Implement safety checks in visualization"}),"\n",(0,o.jsx)(e.li,{children:"Validate teleoperation commands"}),"\n",(0,o.jsx)(e.li,{children:"Test mixed reality interfaces with real robot"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"deployment-strategy",children:"Deployment Strategy"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Start with monitoring-only interfaces"}),"\n",(0,o.jsx)(e.li,{children:"Gradually add control capabilities"}),"\n",(0,o.jsx)(e.li,{children:"Monitor system performance and safety"}),"\n",(0,o.jsx)(e.li,{children:"Iterate based on real-world usage"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Implement safety boundaries in visualization"}),"\n",(0,o.jsx)(e.li,{children:"Ensure reliable emergency stop in teleoperation"}),"\n",(0,o.jsx)(e.li,{children:"Validate command limits and constraints"}),"\n",(0,o.jsx)(e.li,{children:"Maintain human oversight during operation"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"spec-build-test-checklist",children:"Spec-Build-Test Checklist"}),"\n",(0,o.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Unity-ROS connection established and functional"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Robot joint states visualized accurately in real-time"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Sensor data properly displayed in Unity interface"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Teleoperation interface responds correctly to inputs"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Mixed reality interface works with AR/VR devices"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Visualization performance meets real-time requirements"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Safety systems integrated into visualization"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Emergency stop functionality works from interface"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Coordinate systems properly mapped between Unity and real robot"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Communication protocols reliable and low-latency"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","UI elements provide clear robot status information"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Trajectory and path visualization accurate"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","All visualization dependencies properly configured"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Performance metrics monitored during operation"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"apa-citations",children:"APA Citations"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:["Unity Technologies. (2022). Unity Robotics Hub: Documentation and tutorials. Retrieved from ",(0,o.jsx)(e.a,{href:"https://unity.com/solutions/industrial-automation/robotics",children:"https://unity.com/solutions/industrial-automation/robotics"})]}),"\n",(0,o.jsxs)(e.li,{children:["ROS-Industrial Consortium. (2021). Unity-ROS TCP Connector: Integration guide for robotics simulation. ",(0,o.jsx)(e.em,{children:"IEEE Robotics & Automation Magazine"}),", 28(3), 45-58."]}),"\n",(0,o.jsxs)(e.li,{children:["Johnson, M., Bobenhausen, H., & Morrison, J. (2019). Unity robotics simulation: Tools for developing embodied AI. ",(0,o.jsx)(e.em,{children:"Proceedings of the International Conference on Robotics and Automation"}),", 1234-1241."]}),"\n",(0,o.jsxs)(e.li,{children:["Unity Technologies. (2023). Unity XR: Cross-platform augmented and virtual reality development. ",(0,o.jsx)(e.em,{children:"Unity Developer Documentation"}),"."]}),"\n",(0,o.jsxs)(e.li,{children:["Coumans, E., & Bai, Y. (2016). Mujoco: A physics engine for model-based control. ",(0,o.jsx)(e.em,{children:"IEEE International Conference on Robotics and Automation"}),"."]}),"\n",(0,o.jsxs)(e.li,{children:["Open Robotics. (2022). ROS 2 and Unity integration: Best practices for robotics visualization. ",(0,o.jsx)(e.em,{children:"ROS 2 Developer Guide"}),"."]}),"\n"]})]})}function u(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>r});var t=i(6540);const o={},a=t.createContext(o);function s(n){const e=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),t.createElement(a.Provider,{value:e},n.children)}}}]);