"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[4908],{7151:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-2-simulation/chapter-5","title":"Chapter 12 - Digital Twin Creation Process","description":"Why This Concept Matters for Humanoids","source":"@site/docs/module-2-simulation/chapter-5.md","sourceDirName":"module-2-simulation","slug":"/module-2-simulation/chapter-5","permalink":"/hackathon-book-robotics/fr/docs/module-2-simulation/chapter-5","draft":false,"unlisted":false,"editUrl":"https://github.com/noormusarrat/hackathon-book-robotics/edit/main/docs/module-2-simulation/chapter-5.md","tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"title":"Chapter 12 - Digital Twin Creation Process","sidebar_position":12},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 11 - Unity for Robotics Visualization","permalink":"/hackathon-book-robotics/fr/docs/module-2-simulation/chapter-4"},"next":{"title":"Chapter 13 - Simulation Best Practices","permalink":"/hackathon-book-robotics/fr/docs/module-2-simulation/chapter-6"}}');var a=t(4848),s=t(8453);const r={title:"Chapter 12 - Digital Twin Creation Process",sidebar_position:12},o="Chapter 12: Digital Twin Creation Process",l={},c=[{value:"Why This Concept Matters for Humanoids",id:"why-this-concept-matters-for-humanoids",level:2},{value:"Theory",id:"theory",level:2},{value:"Digital Twin Architecture",id:"digital-twin-architecture",level:3},{value:"Real-time Synchronization",id:"real-time-synchronization",level:3},{value:"Twin Fidelity Levels",id:"twin-fidelity-levels",level:3},{value:"Data Flow Architecture",id:"data-flow-architecture",level:3},{value:"Implementation",id:"implementation",level:2},{value:"Digital Twin Core System",id:"digital-twin-core-system",level:3},{value:"Digital Twin Visualization Bridge",id:"digital-twin-visualization-bridge",level:3},{value:"Digital Twin Environment Synchronization",id:"digital-twin-environment-synchronization",level:3},{value:"Hardware/GPU Notes",id:"hardwaregpu-notes",level:2},{value:"Processing Requirements",id:"processing-requirements",level:3},{value:"Visualization Requirements",id:"visualization-requirements",level:3},{value:"Synchronization Requirements",id:"synchronization-requirements",level:3},{value:"Simulation Path",id:"simulation-path",level:2},{value:"Initial Setup",id:"initial-setup",level:3},{value:"Basic Twin Implementation",id:"basic-twin-implementation",level:3},{value:"Advanced Twin Features",id:"advanced-twin-features",level:3},{value:"Validation Process",id:"validation-process",level:3},{value:"Real-World Path",id:"real-world-path",level:2},{value:"Physical Integration",id:"physical-integration",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Deployment Strategy",id:"deployment-strategy",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Spec-Build-Test Checklist",id:"spec-build-test-checklist",level:2},{value:"APA Citations",id:"apa-citations",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-12-digital-twin-creation-process",children:"Chapter 12: Digital Twin Creation Process"})}),"\n",(0,a.jsx)(n.h2,{id:"why-this-concept-matters-for-humanoids",children:"Why This Concept Matters for Humanoids"}),"\n",(0,a.jsx)(n.p,{children:"Digital twins are essential for humanoid robotics as they provide a real-time virtual representation that mirrors the physical robot's state, behavior, and environment. For humanoid robots with complex multi-joint systems and intricate interactions with their environment, digital twins enable predictive maintenance, behavior validation, and safe testing of control algorithms. They allow engineers to monitor, analyze, and optimize robot performance in real-time while providing a safe environment for testing new behaviors before deployment on expensive hardware. Digital twins also facilitate remote monitoring, teleoperation, and collaborative development across distributed teams."}),"\n",(0,a.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,a.jsx)(n.p,{children:"Digital twin technology for humanoid robotics encompasses several fundamental concepts that create a comprehensive virtual representation:"}),"\n",(0,a.jsx)(n.h3,{id:"digital-twin-architecture",children:"Digital Twin Architecture"}),"\n",(0,a.jsx)(n.p,{children:"A humanoid robot digital twin consists of:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Physical Model"}),": Accurate 3D representation of the robot's geometry and kinematics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Behavioral Model"}),": Simulation of the robot's dynamic behavior and control systems"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Interface"}),": Real-time synchronization between physical and virtual systems"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Analytics Engine"}),": Processing and analysis of twin data for insights"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"real-time-synchronization",children:"Real-time Synchronization"}),"\n",(0,a.jsx)(n.p,{children:"Critical components for maintaining twin accuracy:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"State Synchronization"}),": Continuous update of joint positions, velocities, and efforts"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sensor Data Integration"}),": Real-time incorporation of IMU, camera, LiDAR data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Environmental Mapping"}),": Dynamic updating of the virtual environment to match reality"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Temporal Alignment"}),": Synchronization of physical and virtual time domains"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"twin-fidelity-levels",children:"Twin Fidelity Levels"}),"\n",(0,a.jsx)(n.p,{children:"Digital twins operate at different fidelity levels:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Geometric Fidelity"}),": Accurate representation of physical dimensions and appearance"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Kinematic Fidelity"}),": Precise modeling of joint relationships and movement capabilities"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Dynamic Fidelity"}),": Realistic simulation of forces, torques, and physical interactions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Behavioral Fidelity"}),": Accurate modeling of control algorithms and decision-making"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"data-flow-architecture",children:"Data Flow Architecture"}),"\n",(0,a.jsx)(n.p,{children:"Digital twin data flows involve:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Telemetry Ingestion"}),": Collection of sensor and state data from the physical robot"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Model Updating"}),": Continuous refinement of the virtual model based on real data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Prediction Generation"}),": Forecasting future states and behaviors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Command Execution"}),": Sending validated commands from virtual to physical system"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,a.jsx)(n.p,{children:"Let's implement a comprehensive digital twin system for humanoid robotics:"}),"\n",(0,a.jsx)(n.h3,{id:"digital-twin-core-system",children:"Digital Twin Core System"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# digital_twin_core.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState, Imu, CameraInfo\nfrom geometry_msgs.msg import Pose, Twist\nfrom nav_msgs.msg import Odometry\nfrom std_msgs.msg import Float64MultiArray, Bool\nfrom builtin_interfaces.msg import Time\nimport numpy as np\nimport threading\nimport time\nfrom collections import deque\nimport json\nimport requests\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\n\n@dataclass\nclass RobotState:\n    """Data structure for robot state in digital twin"""\n    timestamp: Time\n    joint_positions: List[float]\n    joint_velocities: List[float]\n    joint_efforts: List[float]\n    imu_orientation: List[float]  # x, y, z, w quaternion\n    imu_angular_velocity: List[float]  # x, y, z\n    imu_linear_acceleration: List[float]  # x, y, z\n    pose: Pose\n    velocity: Twist\n    contact_states: List[bool]  # True if in contact, False otherwise\n\n\nclass DigitalTwinCore(Node):\n    def __init__(self):\n        super().__init__(\'digital_twin_core\')\n\n        # Declare parameters\n        self.declare_parameter(\'twin_update_rate\', 100.0)\n        self.declare_parameter(\'sync_tolerance\', 0.01)  # seconds\n        self.declare_parameter(\'history_buffer_size\', 1000)\n        self.declare_parameter(\'enable_prediction\', True)\n        self.declare_parameter(\'prediction_horizon\', 1.0)  # seconds\n\n        # Get parameters\n        self.twin_update_rate = self.get_parameter(\'twin_update_rate\').value\n        self.sync_tolerance = self.get_parameter(\'sync_tolerance\').value\n        self.history_buffer_size = self.get_parameter(\'history_buffer_size\').value\n        self.enable_prediction = self.get_parameter(\'enable_prediction\').value\n        self.prediction_horizon = self.get_parameter(\'prediction_horizon\').value\n\n        # Publishers for twin data\n        self.twin_state_pub = self.create_publisher(\n            Float64MultiArray,\n            \'digital_twin/state\',\n            10\n        )\n\n        self.twin_prediction_pub = self.create_publisher(\n            Float64MultiArray,\n            \'digital_twin/prediction\',\n            10\n        )\n\n        self.twin_visualization_pub = self.create_publisher(\n            Float64MultiArray,\n            \'digital_twin/visualization\',\n            10\n        )\n\n        # Subscribers for physical robot data\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            \'/joint_states\',\n            self.joint_state_callback,\n            10\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        self.odom_sub = self.create_subscription(\n            Odometry,\n            \'/odom\',\n            self.odom_callback,\n            10\n        )\n\n        # Timer for twin update\n        self.twin_timer = self.create_timer(\n            1.0/self.twin_update_rate,\n            self.twin_update_loop\n        )\n\n        # Internal state\n        self.current_state = RobotState(\n            timestamp=self.get_clock().now().to_msg(),\n            joint_positions=[],\n            joint_velocities=[],\n            joint_efforts=[],\n            imu_orientation=[0.0, 0.0, 0.0, 1.0],\n            imu_angular_velocity=[0.0, 0.0, 0.0],\n            imu_linear_acceleration=[0.0, 0.0, 0.0],\n            pose=Pose(),\n            velocity=Twist(),\n            contact_states=[]\n        )\n\n        self.state_history = deque(maxlen=self.history_buffer_size)\n        self.prediction_engine = PredictionEngine()\n        self.sync_checker = SynchronizationChecker(self.sync_tolerance)\n        self.data_validator = DataValidator()\n\n        # Lock for thread safety\n        self.state_lock = threading.RLock()\n\n        self.get_logger().info(\'Digital Twin Core initialized\')\n\n    def joint_state_callback(self, msg):\n        """Update joint state in digital twin"""\n        with self.state_lock:\n            self.current_state.joint_positions = list(msg.position)\n            self.current_state.joint_velocities = list(msg.velocity)\n            self.current_state.joint_efforts = list(msg.effort)\n            self.current_state.timestamp = msg.header.stamp\n\n    def imu_callback(self, msg):\n        """Update IMU state in digital twin"""\n        with self.state_lock:\n            self.current_state.imu_orientation = [\n                msg.orientation.x,\n                msg.orientation.y,\n                msg.orientation.z,\n                msg.orientation.w\n            ]\n            self.current_state.imu_angular_velocity = [\n                msg.angular_velocity.x,\n                msg.angular_velocity.y,\n                msg.angular_velocity.z\n            ]\n            self.current_state.imu_linear_acceleration = [\n                msg.linear_acceleration.x,\n                msg.linear_acceleration.y,\n                msg.linear_acceleration.z\n            ]\n\n    def odom_callback(self, msg):\n        """Update odometry state in digital twin"""\n        with self.state_lock:\n            self.current_state.pose = msg.pose.pose\n            self.current_state.velocity = msg.twist.twist\n\n    def twin_update_loop(self):\n        """Main digital twin update loop"""\n        with self.state_lock:\n            # Validate current state\n            if not self.data_validator.validate_state(self.current_state):\n                self.get_logger().warn(\'Invalid state data detected\')\n                return\n\n            # Add current state to history\n            self.state_history.append(self.current_state)\n\n            # Publish current twin state\n            state_msg = Float64MultiArray()\n            state_msg.data = self.pack_state_for_publishing(self.current_state)\n            self.twin_state_pub.publish(state_msg)\n\n            # Generate prediction if enabled\n            if self.enable_prediction:\n                prediction = self.prediction_engine.predict(\n                    self.state_history,\n                    self.prediction_horizon\n                )\n                if prediction is not None:\n                    pred_msg = Float64MultiArray()\n                    pred_msg.data = self.pack_prediction_for_publishing(prediction)\n                    self.twin_prediction_pub.publish(pred_msg)\n\n            # Publish visualization data\n            viz_msg = Float64MultiArray()\n            viz_msg.data = self.generate_visualization_data(self.current_state)\n            self.twin_visualization_pub.publish(viz_msg)\n\n    def pack_state_for_publishing(self, state):\n        """Convert RobotState to Float64MultiArray data"""\n        data = []\n        # Add joint positions\n        data.extend(state.joint_positions)\n        # Add joint velocities\n        data.extend(state.joint_velocities)\n        # Add joint efforts\n        data.extend(state.joint_efforts)\n        # Add IMU orientation\n        data.extend(state.imu_orientation)\n        # Add IMU angular velocity\n        data.extend(state.imu_angular_velocity)\n        # Add IMU linear acceleration\n        data.extend(state.imu_linear_acceleration)\n        # Add pose (position and orientation)\n        data.extend([\n            state.pose.position.x, state.pose.position.y, state.pose.position.z,\n            state.pose.orientation.x, state.pose.orientation.y,\n            state.pose.orientation.z, state.pose.orientation.w\n        ])\n        # Add velocity (linear and angular)\n        data.extend([\n            state.velocity.linear.x, state.velocity.linear.y, state.velocity.linear.z,\n            state.velocity.angular.x, state.velocity.angular.y, state.velocity.angular.z\n        ])\n        return data\n\n    def pack_prediction_for_publishing(self, prediction):\n        """Convert prediction to Float64MultiArray data"""\n        # For simplicity, return predicted joint positions\n        # In practice, this would include full state prediction\n        return prediction\n\n    def generate_visualization_data(self, state):\n        """Generate data for visualization"""\n        # This would generate data for Unity or other visualization tools\n        viz_data = []\n        # Include key metrics for visualization\n        viz_data.append(self.calculate_balance_metric(state))\n        viz_data.append(self.calculate_energy_metric(state))\n        viz_data.append(self.calculate_stability_metric(state))\n        return viz_data\n\n    def calculate_balance_metric(self, state):\n        """Calculate balance metric based on IMU and pose data"""\n        # Simple balance calculation based on IMU orientation\n        # In practice, this would use more sophisticated balance algorithms\n        orientation = state.imu_orientation\n        # Convert quaternion to roll/pitch angles\n        sinr_cosp = 2 * (orientation[3] * orientation[0] + orientation[1] * orientation[2])\n        cosr_cosp = 1 - 2 * (orientation[0] * orientation[0] + orientation[1] * orientation[1])\n        roll = np.arctan2(sinr_cosp, cosr_cosp)\n\n        sinp = 2 * (orientation[3] * orientation[1] - orientation[2] * orientation[0])\n        pitch = np.arcsin(sinp) if abs(sinp) <= 1 else np.sign(sinp) * np.pi/2\n\n        return abs(roll) + abs(pitch)\n\n    def calculate_energy_metric(self, state):\n        """Calculate energy metric based on joint velocities and efforts"""\n        if len(state.joint_velocities) == len(state.joint_efforts):\n            energy = sum(abs(v * e) for v, e in zip(state.joint_velocities, state.joint_efforts))\n        else:\n            energy = 0.0\n        return energy\n\n    def calculate_stability_metric(self, state):\n        """Calculate stability metric based on contact states and IMU"""\n        # Simplified stability calculation\n        # In practice, this would use center of mass and support polygon\n        return abs(state.imu_linear_acceleration[0]) + abs(state.imu_linear_acceleration[1])\n\n\nclass PredictionEngine:\n    """Engine for predicting future robot states"""\n\n    def __init__(self):\n        self.prediction_model = self.initialize_prediction_model()\n\n    def initialize_prediction_model(self):\n        """Initialize prediction model (could be physics-based or ML-based)"""\n        # For this example, we\'ll use a simple physics-based predictor\n        # In practice, this could be a more sophisticated model\n        return SimplePhysicsPredictor()\n\n    def predict(self, state_history, horizon):\n        """Predict future states"""\n        if len(state_history) < 2:\n            return None\n\n        # Use the most recent states to make prediction\n        recent_states = list(state_history)[-2:]\n        return self.prediction_model.predict(recent_states, horizon)\n\n\nclass SimplePhysicsPredictor:\n    """Simple physics-based predictor for robot states"""\n\n    def predict(self, recent_states, horizon):\n        """Predict future state using simple kinematic model"""\n        if len(recent_states) < 2:\n            return None\n\n        # Calculate time difference between recent states\n        dt = self.calculate_time_difference(recent_states[-2], recent_states[-1])\n\n        if dt <= 0:\n            return None\n\n        # Use simple integration to predict future state\n        current_state = recent_states[-1]\n        previous_state = recent_states[-2]\n\n        # Predict joint positions using velocity\n        predicted_positions = []\n        for curr_pos, curr_vel in zip(current_state.joint_positions, current_state.joint_velocities):\n            predicted_pos = curr_pos + curr_vel * horizon\n            predicted_positions.append(predicted_pos)\n\n        return predicted_positions\n\n    def calculate_time_difference(self, state1, state2):\n        """Calculate time difference between two states"""\n        time1 = state1.timestamp.sec + state1.timestamp.nanosec / 1e9\n        time2 = state2.timestamp.sec + state2.timestamp.nanosec / 1e9\n        return abs(time2 - time1)\n\n\nclass SynchronizationChecker:\n    """Check synchronization between physical and digital systems"""\n\n    def __init__(self, tolerance):\n        self.tolerance = tolerance\n\n    def check_sync(self, physical_time, virtual_time):\n        """Check if physical and virtual systems are synchronized"""\n        time_diff = abs(physical_time - virtual_time)\n        return time_diff <= self.tolerance\n\n\nclass DataValidator:\n    """Validate robot state data"""\n\n    def validate_state(self, state):\n        """Validate that state data is reasonable"""\n        # Check for NaN or infinite values\n        for pos in state.joint_positions:\n            if not (np.isfinite(pos)):\n                return False\n\n        for vel in state.joint_velocities:\n            if not (np.isfinite(vel)):\n                return False\n\n        for effort in state.joint_efforts:\n            if not (np.isfinite(effort)):\n                return False\n\n        # Check quaternion normalization\n        quat = state.imu_orientation\n        norm = np.sqrt(sum(x*x for x in quat))\n        if abs(norm - 1.0) > 0.1:  # Allow some tolerance for numerical errors\n            return False\n\n        return True\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    twin_core = DigitalTwinCore()\n\n    try:\n        rclpy.spin(twin_core)\n    except KeyboardInterrupt:\n        twin_core.get_logger().info(\'Shutting down digital twin core...\')\n    finally:\n        twin_core.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"digital-twin-visualization-bridge",children:"Digital Twin Visualization Bridge"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# digital_twin_visualization.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Float64MultiArray\nfrom sensor_msgs.msg import JointState\nfrom geometry_msgs.msg import PoseStamped\nimport json\nimport requests\nfrom flask import Flask, jsonify, request\nimport threading\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\n\n@dataclass\nclass VisualizationData:\n    \"\"\"Data structure for visualization\"\"\"\n    joint_positions: List[float]\n    robot_pose: List[float]  # x, y, z, qx, qy, qz, qw\n    sensor_data: Dict[str, float]\n    metrics: Dict[str, float]\n\n\nclass DigitalTwinVisualization(Node):\n    def __init__(self):\n        super().__init__('digital_twin_visualization')\n\n        # Declare parameters\n        self.declare_parameter('web_server_port', 5000)\n        self.declare_parameter('unity_endpoint', 'http://127.0.0.1:10000')\n        self.declare_parameter('visualization_update_rate', 30.0)  # Hz\n\n        # Get parameters\n        self.web_server_port = self.get_parameter('web_server_port').value\n        self.unity_endpoint = self.get_parameter('unity_endpoint').value\n        self.visualization_update_rate = self.get_parameter('visualization_update_rate').value\n\n        # Subscribers for twin data\n        self.twin_state_sub = self.create_subscription(\n            Float64MultiArray,\n            'digital_twin/state',\n            self.twin_state_callback,\n            10\n        )\n\n        self.twin_prediction_sub = self.create_subscription(\n            Float64MultiArray,\n            'digital_twin/prediction',\n            self.twin_prediction_callback,\n            10\n        )\n\n        self.twin_visualization_sub = self.create_subscription(\n            Float64MultiArray,\n            'digital_twin/visualization',\n            self.twin_visualization_callback,\n            10\n        )\n\n        # Internal state\n        self.current_visualization_data = VisualizationData(\n            joint_positions=[],\n            robot_pose=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n            sensor_data={},\n            metrics={}\n        )\n\n        # Start web server in separate thread\n        self.web_server = Flask(__name__)\n        self.setup_web_routes()\n        self.web_thread = threading.Thread(target=self.run_web_server)\n        self.web_thread.daemon = True\n        self.web_thread.start()\n\n        # Timer for Unity synchronization\n        self.unity_sync_timer = self.create_timer(\n            1.0/self.visualization_update_rate,\n            self.sync_with_unity\n        )\n\n        self.get_logger().info('Digital Twin Visualization initialized')\n\n    def setup_web_routes(self):\n        \"\"\"Setup web server routes for visualization\"\"\"\n        @self.web_server.route('/api/twin-data')\n        def get_twin_data():\n            return jsonify({\n                'joint_positions': self.current_visualization_data.joint_positions,\n                'robot_pose': self.current_visualization_data.robot_pose,\n                'sensor_data': self.current_visualization_data.sensor_data,\n                'metrics': self.current_visualization_data.metrics\n            })\n\n        @self.web_server.route('/api/twin-status')\n        def get_twin_status():\n            return jsonify({\n                'status': 'running',\n                'update_rate': self.visualization_update_rate,\n                'last_update': time.time()\n            })\n\n        @self.web_server.route('/api/predictions')\n        def get_predictions():\n            # This would return prediction data\n            return jsonify({'predictions': []})\n\n    def run_web_server(self):\n        \"\"\"Run the web server\"\"\"\n        self.web_server.run(host='0.0.0.0', port=self.web_server_port, debug=False, use_reloader=False)\n\n    def twin_state_callback(self, msg):\n        \"\"\"Process twin state data\"\"\"\n        # Extract joint positions from message\n        # This assumes a specific packing format from the twin core\n        data = msg.data\n        num_joints = len(data) // 3  # Assuming position, velocity, effort for each joint\n        joint_positions = data[:num_joints] if len(data) >= num_joints else []\n\n        with self.lock:\n            self.current_visualization_data.joint_positions = joint_positions\n\n    def twin_prediction_callback(self, msg):\n        \"\"\"Process prediction data\"\"\"\n        # Process predicted joint positions\n        predicted_positions = list(msg.data)\n\n    def twin_visualization_callback(self, msg):\n        \"\"\"Process visualization-specific data\"\"\"\n        # Extract visualization metrics\n        if len(msg.data) >= 3:\n            self.current_visualization_data.metrics = {\n                'balance': msg.data[0],\n                'energy': msg.data[1],\n                'stability': msg.data[2]\n            }\n\n    def sync_with_unity(self):\n        \"\"\"Synchronize with Unity visualization\"\"\"\n        try:\n            # Send current visualization data to Unity\n            unity_data = {\n                'joint_positions': self.current_visualization_data.joint_positions,\n                'metrics': self.current_visualization_data.metrics,\n                'timestamp': time.time()\n            }\n\n            response = requests.post(\n                f\"{self.unity_endpoint}/robot_state\",\n                json=unity_data,\n                timeout=0.1\n            )\n\n            if response.status_code != 200:\n                self.get_logger().warn(f'Unity sync failed: {response.status_code}')\n\n        except requests.exceptions.RequestException as e:\n            self.get_logger().debug(f'Unity sync error: {str(e)}')\n\n    def get_current_state_for_visualization(self):\n        \"\"\"Get current state formatted for visualization\"\"\"\n        return {\n            'joint_positions': self.current_visualization_data.joint_positions,\n            'robot_pose': self.current_visualization_data.robot_pose,\n            'sensor_data': self.current_visualization_data.sensor_data,\n            'metrics': self.current_visualization_data.metrics\n        }\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    visualization = DigitalTwinVisualization()\n\n    try:\n        rclpy.spin(visualization)\n    except KeyboardInterrupt:\n        visualization.get_logger().info('Shutting down digital twin visualization...')\n    finally:\n        visualization.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"digital-twin-environment-synchronization",children:"Digital Twin Environment Synchronization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# digital_twin_environment.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, PointCloud2\nfrom nav_msgs.msg import OccupancyGrid\nfrom geometry_msgs.msg import PoseStamped, PointStamped\nimport numpy as np\nimport threading\nfrom scipy.spatial import KDTree\nimport time\nfrom typing import List, Tuple\n\n\nclass DigitalTwinEnvironment(Node):\n    def __init__(self):\n        super().__init__(\'digital_twin_environment\')\n\n        # Declare parameters\n        self.declare_parameter(\'environment_update_rate\', 10.0)\n        self.declare_parameter(\'map_resolution\', 0.05)  # meters per cell\n        self.declare_parameter(\'map_width\', 20.0)  # meters\n        self.declare_parameter(\'map_height\', 20.0)  # meters\n        self.declare_parameter(\'max_lidar_range\', 10.0)  # meters\n\n        # Get parameters\n        self.environment_update_rate = self.get_parameter(\'environment_update_rate\').value\n        self.map_resolution = self.get_parameter(\'map_resolution\').value\n        self.map_width = self.get_parameter(\'map_width\').value\n        self.map_height = self.get_parameter(\'map_height\').value\n        self.max_lidar_range = self.get_parameter(\'max_lidar_range\').value\n\n        # Calculate map dimensions\n        self.map_width_cells = int(self.map_width / self.map_resolution)\n        self.map_height_cells = int(self.map_height / self.map_resolution)\n\n        # Subscribers\n        self.lidar_sub = self.create_subscription(\n            LaserScan,\n            \'/scan\',\n            self.lidar_callback,\n            10\n        )\n\n        self.pointcloud_sub = self.create_subscription(\n            PointCloud2,\n            \'/camera/depth/points\',\n            self.pointcloud_callback,\n            10\n        )\n\n        self.map_sub = self.create_subscription(\n            OccupancyGrid,\n            \'/map\',\n            self.map_callback,\n            10\n        )\n\n        # Publishers\n        self.synced_map_pub = self.create_publisher(\n            OccupancyGrid,\n            \'digital_twin/synced_map\',\n            10\n        )\n\n        self.synced_environment_pub = self.create_publisher(\n            PointCloud2,\n            \'digital_twin/environment_cloud\',\n            10\n        )\n\n        # Timer for environment update\n        self.environment_timer = self.create_timer(\n            1.0/self.environment_update_rate,\n            self.environment_update_loop\n        )\n\n        # Internal state\n        self.lidar_data = None\n        self.pointcloud_data = None\n        self.current_map = None\n        self.environment_map = np.zeros((self.map_height_cells, self.map_width_cells), dtype=np.int8)\n        self.object_detection_lock = threading.RLock()\n\n        self.get_logger().info(\'Digital Twin Environment initialized\')\n\n    def lidar_callback(self, msg):\n        """Process LiDAR data for environment mapping"""\n        with self.object_detection_lock:\n            self.lidar_data = msg\n            # Process LiDAR data to detect obstacles and update environment map\n            self.process_lidar_data(msg)\n\n    def pointcloud_callback(self, msg):\n        """Process point cloud data for 3D environment mapping"""\n        with self.object_detection_lock:\n            self.pointcloud_data = msg\n            # Process point cloud data to build 3D environment representation\n            self.process_pointcloud_data(msg)\n\n    def map_callback(self, msg):\n        """Process existing map data"""\n        with self.object_detection_lock:\n            self.current_map = msg\n            # Update environment map with new map data\n            self.update_environment_map(msg)\n\n    def process_lidar_data(self, scan_msg):\n        """Process LiDAR scan to update environment map"""\n        if scan_msg.ranges is None:\n            return\n\n        robot_x, robot_y = 0.0, 0.0  # Assume robot is at origin for this example\n\n        for i, range_val in enumerate(scan_msg.ranges):\n            if scan_msg.range_min <= range_val <= scan_msg.range_max:\n                angle = scan_msg.angle_min + i * scan_msg.angle_increment\n                # Convert to Cartesian coordinates relative to robot\n                x = robot_x + range_val * np.cos(angle)\n                y = robot_y + range_val * np.sin(angle)\n\n                # Convert to grid coordinates\n                grid_x = int((x + self.map_width/2) / self.map_resolution)\n                grid_y = int((y + self.map_height/2) / self.map_resolution)\n\n                # Update occupancy grid\n                if 0 <= grid_x < self.map_width_cells and 0 <= grid_y < self.map_height_cells:\n                    if range_val < self.max_lidar_range:\n                        self.environment_map[grid_y, grid_x] = 100  # Occupied\n                    else:\n                        self.environment_map[grid_y, grid_x] = 0   # Free\n\n    def process_pointcloud_data(self, pc_msg):\n        """Process point cloud data to build 3D environment"""\n        # This would typically use libraries like PCL or Open3D\n        # For this example, we\'ll just log the processing\n        self.get_logger().debug(f\'Processing point cloud with {len(pc_msg.data)} bytes\')\n\n    def update_environment_map(self, map_msg):\n        """Update environment map with new map data"""\n        # Convert OccupancyGrid message to numpy array\n        width = map_msg.info.width\n        height = map_msg.info.height\n        resolution = map_msg.info.resolution\n\n        if width > 0 and height > 0:\n            # Reshape the data to match the map dimensions\n            map_array = np.array(map_msg.data).reshape((height, width))\n\n            # Update our internal map representation\n            # This is a simplified approach - in practice, you\'d handle coordinate transformations\n            self.environment_map = map_array\n\n    def environment_update_loop(self):\n        """Main environment update loop"""\n        with self.object_detection_lock:\n            # Publish synced environment map\n            if self.environment_map is not None:\n                map_msg = self.create_occupancy_grid_msg()\n                self.synced_map_pub.publish(map_msg)\n\n            # Publish environment point cloud\n            if self.pointcloud_data is not None:\n                self.synced_environment_pub.publish(self.pointcloud_data)\n\n    def create_occupancy_grid_msg(self):\n        """Create OccupancyGrid message from internal map"""\n        from nav_msgs.msg import OccupancyGrid\n        from std_msgs.msg import Header\n\n        map_msg = OccupancyGrid()\n        map_msg.header = Header()\n        map_msg.header.stamp = self.get_clock().now().to_msg()\n        map_msg.header.frame_id = \'map\'\n\n        map_msg.info.resolution = self.map_resolution\n        map_msg.info.width = self.map_width_cells\n        map_msg.info.height = self.map_height_cells\n        map_msg.info.origin.position.x = -self.map_width / 2\n        map_msg.info.origin.position.y = -self.map_height / 2\n        map_msg.info.origin.position.z = 0.0\n        map_msg.info.origin.orientation.w = 1.0\n\n        # Flatten the 2D array to 1D for the message\n        map_msg.data = self.environment_map.flatten().tolist()\n\n        return map_msg\n\n    def get_environment_state(self):\n        """Get current environment state for digital twin"""\n        with self.object_detection_lock:\n            return {\n                \'map\': self.environment_map.copy(),\n                \'lidar_data\': self.lidar_data,\n                \'pointcloud_data\': self.pointcloud_data\n            }\n\n    def find_nearest_obstacle(self, x, y):\n        """Find the nearest obstacle to given coordinates"""\n        with self.object_detection_lock:\n            # Find occupied cells in the map\n            occupied_cells = np.where(self.environment_map == 100)\n            if len(occupied_cells[0]) == 0:\n                return None\n\n            # Convert to world coordinates\n            occupied_x = occupied_cells[1] * self.map_resolution - self.map_width/2\n            occupied_y = occupied_cells[0] * self.map_resolution - self.map_height/2\n\n            # Find the nearest occupied cell\n            distances = np.sqrt((occupied_x - x)**2 + (occupied_y - y)**2)\n            min_idx = np.argmin(distances)\n\n            return {\n                \'x\': occupied_x[min_idx],\n                \'y\': occupied_y[min_idx],\n                \'distance\': distances[min_idx]\n            }\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    environment = DigitalTwinEnvironment()\n\n    try:\n        rclpy.spin(environment)\n    except KeyboardInterrupt:\n        environment.get_logger().info(\'Shutting down digital twin environment...\')\n    finally:\n        environment.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"hardwaregpu-notes",children:"Hardware/GPU Notes"}),"\n",(0,a.jsx)(n.p,{children:"Digital twin creation for humanoid robotics has specific hardware requirements:"}),"\n",(0,a.jsx)(n.h3,{id:"processing-requirements",children:"Processing Requirements"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"CPU"}),": 8+ cores for real-time data processing and twin synchronization"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory"}),": 16GB+ for storing environment maps and state history"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Storage"}),": SSD recommended for fast data access and logging"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Network"}),": High-bandwidth connection for real-time data streaming"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"visualization-requirements",children:"Visualization Requirements"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"GPU"}),": NVIDIA RTX 4070 Ti or equivalent for real-time rendering"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"VRAM"}),": 12GB+ for detailed humanoid models and environment visualization"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Display"}),": High-resolution display for detailed twin monitoring"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Graphics API"}),": Support for modern graphics APIs (DirectX 12, Vulkan)"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"synchronization-requirements",children:"Synchronization Requirements"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Real-time Kernel"}),": PREEMPT_RT recommended for deterministic timing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"High Precision Clock"}),": For accurate state synchronization"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Low Latency Network"}),": For real-time communication between systems"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Buffer Management"}),": Efficient memory management for state history"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"simulation-path",children:"Simulation Path"}),"\n",(0,a.jsx)(n.p,{children:"For developing digital twin systems for humanoid robotics:"}),"\n",(0,a.jsx)(n.h3,{id:"initial-setup",children:"Initial Setup"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Configure ROS 2 nodes for state synchronization"}),"\n",(0,a.jsx)(n.li,{children:"Set up environment mapping and perception systems"}),"\n",(0,a.jsx)(n.li,{children:"Create basic visualization interface"}),"\n",(0,a.jsx)(n.li,{children:"Implement state validation and error handling"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"basic-twin-implementation",children:"Basic Twin Implementation"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Implement joint state synchronization"}),"\n",(0,a.jsx)(n.li,{children:"Add sensor data integration"}),"\n",(0,a.jsx)(n.li,{children:"Create environment mapping"}),"\n",(0,a.jsx)(n.li,{children:"Validate real-time performance"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"advanced-twin-features",children:"Advanced Twin Features"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Add predictive capabilities"}),"\n",(0,a.jsx)(n.li,{children:"Implement multi-robot twin management"}),"\n",(0,a.jsx)(n.li,{children:"Add advanced visualization features"}),"\n",(0,a.jsx)(n.li,{children:"Create analytics and monitoring tools"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"validation-process",children:"Validation Process"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Test synchronization accuracy"}),"\n",(0,a.jsx)(n.li,{children:"Validate prediction algorithms"}),"\n",(0,a.jsx)(n.li,{children:"Check real-time performance"}),"\n",(0,a.jsx)(n.li,{children:"Verify safety systems"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"real-world-path",children:"Real-World Path"}),"\n",(0,a.jsx)(n.p,{children:"Transitioning from simulation to real hardware:"}),"\n",(0,a.jsx)(n.h3,{id:"physical-integration",children:"Physical Integration"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Connect twin to real robot sensors"}),"\n",(0,a.jsx)(n.li,{children:"Validate state synchronization accuracy"}),"\n",(0,a.jsx)(n.li,{children:"Test environment mapping with real sensors"}),"\n",(0,a.jsx)(n.li,{children:"Verify safety systems in real environment"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Optimize data transmission rates"}),"\n",(0,a.jsx)(n.li,{children:"Tune prediction algorithms for real hardware"}),"\n",(0,a.jsx)(n.li,{children:"Validate computational requirements"}),"\n",(0,a.jsx)(n.li,{children:"Ensure real-time performance"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"deployment-strategy",children:"Deployment Strategy"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Start with monitoring-only twin"}),"\n",(0,a.jsx)(n.li,{children:"Gradually add predictive features"}),"\n",(0,a.jsx)(n.li,{children:"Monitor system performance and accuracy"}),"\n",(0,a.jsx)(n.li,{children:"Iterate based on real-world observations"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Implement safety boundaries in twin"}),"\n",(0,a.jsx)(n.li,{children:"Ensure reliable emergency stop systems"}),"\n",(0,a.jsx)(n.li,{children:"Validate prediction safety margins"}),"\n",(0,a.jsx)(n.li,{children:"Maintain human oversight during operation"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"spec-build-test-checklist",children:"Spec-Build-Test Checklist"}),"\n",(0,a.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Digital twin core system properly synchronizes with physical robot"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Joint state data accurately reflected in virtual model"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Sensor data properly integrated into twin system"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Environment mapping works with real sensors"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Prediction algorithms provide useful forecasts"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Visualization interface updates in real-time"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Synchronization accuracy meets tolerance requirements"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Performance metrics are monitored during operation"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Safety systems integrated into twin architecture"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Emergency stop functionality works from twin interface"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Data validation prevents invalid state propagation"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Communication protocols reliable and low-latency"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","State history and analytics properly maintained"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","All twin dependencies properly configured"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"apa-citations",children:"APA Citations"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Rasheed, A., San, O., & Kvamsdal, T. (2020). Digital twin: Values, challenges and enablers from a modeling perspective. ",(0,a.jsx)(n.em,{children:"IEEE Access"}),", 8, 21980-22012."]}),"\n",(0,a.jsxs)(n.li,{children:["Tao, F., Zhang, H., Liu, A., & Nee, A. Y. (2019). Digital twin in industry: State-of-the-art. ",(0,a.jsx)(n.em,{children:"IEEE Transactions on Industrial Informatics"}),", 15(4), 2405-2415."]}),"\n",(0,a.jsxs)(n.li,{children:["Berliner, A. J., & Slocum, A. H. (2021). Digital twins for robotics: A survey. ",(0,a.jsx)(n.em,{children:"IEEE Robotics & Automation Magazine"}),", 28(2), 102-114."]}),"\n",(0,a.jsxs)(n.li,{children:["Grieves, M., & Vickers, J. (2017). Digital twin: Manufacturing excellence through virtual factory replication. ",(0,a.jsx)(n.em,{children:"NASA Technical Report"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["Kritzinger, W., Karner, M., Traar, G., Henjes, J., & Sihn, W. (2018). Digital Twin in manufacturing: A categorical literature review and classification. ",(0,a.jsx)(n.em,{children:"IFAC-PapersOnLine"}),", 51(11), 1016-1022."]}),"\n",(0,a.jsxs)(n.li,{children:["Negri, E., Fumagalli, L., & Macchi, M. (2017). A review of the roles of digital twin in CPS-based production systems. ",(0,a.jsx)(n.em,{children:"Procedia Manufacturing"}),", 11, 939-948."]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var i=t(6540);const a={},s=i.createContext(a);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);