"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[40],{4372:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-1-ros2/chapter-6","title":"Chapter 6 - ROS 2 Pipeline Implementation","description":"Why This Concept Matters for Humanoids","source":"@site/docs/module-1-ros2/chapter-6.md","sourceDirName":"module-1-ros2","slug":"/module-1-ros2/chapter-6","permalink":"/hackathon-book-robotics/fr/docs/module-1-ros2/chapter-6","draft":false,"unlisted":false,"editUrl":"https://github.com/noormusarrat/hackathon-book-robotics/edit/main/docs/module-1-ros2/chapter-6.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Chapter 6 - ROS 2 Pipeline Implementation","sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: ROS 2 for Humanoid Robots","permalink":"/hackathon-book-robotics/fr/docs/module-1-ros2/chapter-5"},"next":{"title":"Chapter 7: ROS 2 Best Practices","permalink":"/hackathon-book-robotics/fr/docs/module-1-ros2/chapter-7"}}');var s=i(4848),o=i(8453);const r={title:"Chapter 6 - ROS 2 Pipeline Implementation",sidebar_position:6},a="Chapter 6: ROS 2 Pipeline Implementation",l={},c=[{value:"Why This Concept Matters for Humanoids",id:"why-this-concept-matters-for-humanoids",level:2},{value:"Theory",id:"theory",level:2},{value:"Implementation",id:"implementation",level:2},{value:"Hardware/GPU Notes",id:"hardwaregpu-notes",level:2},{value:"Simulation Path",id:"simulation-path",level:2},{value:"Real-World Path",id:"real-world-path",level:2},{value:"Spec-Build-Test Checklist",id:"spec-build-test-checklist",level:2},{value:"APA Citations",id:"apa-citations",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-6-ros-2-pipeline-implementation",children:"Chapter 6: ROS 2 Pipeline Implementation"})}),"\n",(0,s.jsx)(n.h2,{id:"why-this-concept-matters-for-humanoids",children:"Why This Concept Matters for Humanoids"}),"\n",(0,s.jsx)(n.p,{children:"Creating a complete ROS 2 pipeline is essential for humanoid robotics as it establishes the communication framework that enables coordinated movement, sensory processing, and intelligent decision-making. A well-designed pipeline ensures that data flows efficiently between perception, planning, and actuation systems, which is crucial for the responsive and coordinated behavior required in humanoid robots. Understanding pipeline implementation allows robotics engineers to build robust systems that can handle the complex interplay of multiple subsystems required for humanoid locomotion and interaction."}),"\n",(0,s.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,s.jsx)(n.p,{children:"A ROS 2 pipeline consists of interconnected nodes that process data through topics, services, and actions. The pipeline typically follows a pattern where sensor data is collected, processed through various algorithms, and then used to control actuators or make decisions. In humanoid robotics, this might involve:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Ingestion"}),": Collecting sensor data from cameras, IMUs, joint encoders, and other sensors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Preprocessing"}),": Filtering, calibration, and initial processing of raw sensor data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fusion"}),": Combining data from multiple sensors to create a coherent understanding of the environment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Planning"}),": Generating motion plans, trajectories, or behavioral decisions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Execution"}),": Sending commands to actuators and monitoring their execution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feedback"}),": Monitoring system state and adjusting plans as needed"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The pipeline must handle real-time constraints, manage computational resources efficiently, and provide fault tolerance to ensure the safety and stability of humanoid robots."}),"\n",(0,s.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,s.jsx)(n.p,{children:"Let's implement a complete ROS 2 pipeline for humanoid robot control. This example will demonstrate a simple perception-action pipeline that processes camera input to generate joint commands."}),"\n",(0,s.jsx)(n.p,{children:"First, create the main pipeline launch file:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"\x3c!-- my-website/docs/module-1-ros2/examples/pipeline_launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Camera input node\n        Node(\n            package='image_publisher',\n            executable='image_publisher_node',\n            name='camera_driver',\n            parameters=[\n                {'image_path': '/path/to/camera'},\n                {'publish_rate': 30.0}\n            ]\n        ),\n\n        # Perception processing node\n        Node(\n            package='my_robot_perception',\n            executable='object_detector',\n            name='object_detector',\n            parameters=[\n                {'model_path': 'yolov8n.pt'},\n                {'confidence_threshold': 0.5}\n            ]\n        ),\n\n        # Motion planning node\n        Node(\n            package='my_robot_planning',\n            executable='motion_planner',\n            name='motion_planner',\n            parameters=[\n                {'planning_frequency': 10.0},\n                {'max_velocity': 0.5}\n            ]\n        ),\n\n        # Robot controller\n        Node(\n            package='my_robot_control',\n            executable='joint_controller',\n            name='joint_controller',\n            parameters=[\n                {'control_rate': 100.0},\n                {'max_effort': 100.0}\n            ]\n        )\n    ])\n"})}),"\n",(0,s.jsx)(n.p,{children:"Now, let's implement a perception node that processes camera data:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# my-website/docs/module-1-ros2/examples/perception_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass ObjectDetector(Node):\n    def __init__(self):\n        super().__init__('object_detector')\n\n        # Create publisher and subscriber\n        self.subscription = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.image_callback,\n            10)\n\n        self.publisher = self.create_publisher(\n            Detection2DArray,\n            'detections',\n            10)\n\n        # Initialize OpenCV bridge\n        self.bridge = CvBridge()\n\n        # Log initialization\n        self.get_logger().info('Object Detector Node Initialized')\n\n    def image_callback(self, msg):\n        try:\n            # Convert ROS Image message to OpenCV image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n            # Process image (simple example: detect circles)\n            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n            circles = cv2.HoughCircles(\n                gray,\n                cv2.HOUGH_GRADIENT,\n                1,\n                20,\n                param1=50,\n                param2=30,\n                minRadius=10,\n                maxRadius=100\n            )\n\n            # Create detection message\n            detection_msg = Detection2DArray()\n            detection_msg.header = msg.header\n\n            if circles is not None:\n                circles = np.round(circles[0, :]).astype(\"int\")\n                for (x, y, r) in circles:\n                    # Draw circle on image for visualization\n                    cv2.circle(cv_image, (x, y), r, (0, 255, 0), 4)\n\n            # Publish detections\n            self.publisher.publish(detection_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {str(e)}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    detector = ObjectDetector()\n\n    try:\n        rclpy.spin(detector)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        detector.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.p,{children:"Next, let's create a motion planning node that takes detections and generates movement commands:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# my-website/docs/module-1-ros2/examples/planning_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom vision_msgs.msg import Detection2DArray\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import Bool\n\nclass MotionPlanner(Node):\n    def __init__(self):\n        super().__init__('motion_planner')\n\n        # Subscribers\n        self.detection_sub = self.create_subscription(\n            Detection2DArray,\n            'detections',\n            self.detection_callback,\n            10)\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(\n            Twist,\n            'cmd_vel',\n            10)\n\n        self.emergency_stop_pub = self.create_publisher(\n            Bool,\n            'emergency_stop',\n            10)\n\n        # Parameters\n        self.target_distance = self.declare_parameter(\n            'target_distance', 1.0).value\n        self.max_linear_speed = self.declare_parameter(\n            'max_linear_speed', 0.5).value\n\n        # Internal state\n        self.detection_received = False\n        self.last_detection_time = self.get_clock().now()\n\n        # Create timer for safety checks\n        self.timer = self.create_timer(0.1, self.safety_timer_callback)\n\n        self.get_logger().info('Motion Planner Node Initialized')\n\n    def detection_callback(self, msg):\n        self.detection_received = True\n        self.last_detection_time = self.get_clock().now()\n\n        # Process detections to generate motion commands\n        if len(msg.detections) > 0:\n            # Example: Move towards detected object\n            cmd_vel = Twist()\n            cmd_vel.linear.x = self.max_linear_speed * 0.5  # Move forward slowly\n            cmd_vel.angular.z = 0.0  # No rotation for now\n\n            self.cmd_vel_pub.publish(cmd_vel)\n        else:\n            # Stop if no detections\n            cmd_vel = Twist()\n            cmd_vel.linear.x = 0.0\n            cmd_vel.angular.z = 0.0\n            self.cmd_vel_pub.publish(cmd_vel)\n\n    def safety_timer_callback(self):\n        # Check if we've received detections recently\n        time_since_last_detection = (\n            self.get_clock().now() - self.last_detection_time\n        ).nanoseconds / 1e9\n\n        if time_since_last_detection > 2.0:  # 2 seconds without detection\n            # Emergency stop\n            stop_msg = Bool()\n            stop_msg.data = True\n            self.emergency_stop_pub.publish(stop_msg)\n\n            # Stop movement\n            cmd_vel = Twist()\n            cmd_vel.linear.x = 0.0\n            cmd_vel.angular.z = 0.0\n            self.cmd_vel_pub.publish(cmd_vel)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    planner = MotionPlanner()\n\n    try:\n        rclpy.spin(planner)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        planner.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.p,{children:"Finally, let's create a complete pipeline configuration file:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# my-website/docs/module-1-ros2/examples/pipeline_config.yaml\n# Pipeline Configuration for Humanoid Robot\n\nrobot_name: "humanoid_robot"\npipeline_name: "perception_action_pipeline"\n\nnodes:\n  camera_driver:\n    package: "image_publisher"\n    executable: "image_publisher_node"\n    parameters:\n      image_path: "/dev/video0"\n      publish_rate: 30.0\n      camera_info_url: "package://my_robot_description/config/camera_info.yaml"\n\n  object_detector:\n    package: "my_robot_perception"\n    executable: "object_detector"\n    parameters:\n      model_path: "yolov8n.pt"\n      confidence_threshold: 0.5\n      max_detections: 10\n\n  motion_planner:\n    package: "my_robot_planning"\n    executable: "motion_planner"\n    parameters:\n      target_distance: 1.0\n      max_linear_speed: 0.5\n      max_angular_speed: 1.0\n\n  joint_controller:\n    package: "my_robot_control"\n    executable: "joint_controller"\n    parameters:\n      control_rate: 100.0\n      max_effort: 100.0\n      joint_names: ["hip_joint", "knee_joint", "ankle_joint", "shoulder_joint", "elbow_joint"]\n\ntopics:\n  camera/image_raw:\n    type: "sensor_msgs/Image"\n    qos:\n      history: "keep_last"\n      depth: 10\n      reliability: "reliable"\n      durability: "volatile"\n\n  detections:\n    type: "vision_msgs/Detection2DArray"\n    qos:\n      history: "keep_last"\n      depth: 10\n      reliability: "best_effort"\n\n  cmd_vel:\n    type: "geometry_msgs/Twist"\n    qos:\n      history: "keep_last"\n      depth: 1\n      reliability: "reliable"\n\nsafety:\n  emergency_stop_timeout: 2.0  # seconds\n  max_velocity_threshold: 1.0  # m/s\n  collision_distance_threshold: 0.5  # meters\n\nperformance:\n  pipeline_frequency: 30.0  # Hz\n  max_pipeline_latency: 0.1  # seconds\n'})}),"\n",(0,s.jsx)(n.h2,{id:"hardwaregpu-notes",children:"Hardware/GPU Notes"}),"\n",(0,s.jsx)(n.p,{children:"For implementing ROS 2 pipelines on humanoid robots, consider these hardware requirements:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CPU"}),": Multi-core processor (8+ cores recommended) for handling multiple concurrent nodes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RAM"}),": 16GB minimum, 32GB+ recommended for complex perception tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU"}),": For perception-intensive pipelines, NVIDIA GPU with CUDA support (RTX 4070 Ti minimum) for accelerated computer vision"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Network"}),": Gigabit Ethernet or high-bandwidth wireless for sensor data transmission"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Storage"}),": SSD with 500GB+ for logs, maps, and runtime data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time capability"}),": Consider using PREEMPT_RT kernel for deterministic timing"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"For humanoid robots specifically, joint control requires high-frequency communication (100Hz+) with low latency to maintain stability."}),"\n",(0,s.jsx)(n.h2,{id:"simulation-path",children:"Simulation Path"}),"\n",(0,s.jsx)(n.p,{children:"In simulation, you can test your pipeline using Gazebo or Isaac Sim:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Start the simulation environment"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 launch my_robot_gazebo humanoid_world.launch.py\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Launch your pipeline"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 launch my_robot_pipeline pipeline.launch.py\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Monitor the pipeline"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Check node connections\nros2 run rqt_graph rqt_graph\n\n# Monitor topics\nros2 topic echo /detections\n\n# Check performance\nros2 run topic_tools relay /camera/image_raw /monitor\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Visualize in RViz2"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 run rviz2 rviz2 -d my_robot_pipeline.rviz\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"real-world-path",children:"Real-World Path"}),"\n",(0,s.jsx)(n.p,{children:"For deployment on real humanoid hardware:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Validate safety systems"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Ensure emergency stop functionality works"}),"\n",(0,s.jsx)(n.li,{children:"Verify joint limits and effort constraints"}),"\n",(0,s.jsx)(n.li,{children:"Test communication timeouts"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Calibrate sensors"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Camera intrinsic/extrinsic calibration"}),"\n",(0,s.jsx)(n.li,{children:"IMU alignment and bias correction"}),"\n",(0,s.jsx)(n.li,{children:"Joint encoder zero positions"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Optimize for real-time performance"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Profile node execution times"}),"\n",(0,s.jsx)(n.li,{children:"Adjust QoS settings for reliability"}),"\n",(0,s.jsx)(n.li,{children:"Implement watchdog timers"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Test incrementally"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Start with simple single-node tests"}),"\n",(0,s.jsx)(n.li,{children:"Gradually add pipeline components"}),"\n",(0,s.jsx)(n.li,{children:"Test safety scenarios thoroughly"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"spec-build-test-checklist",children:"Spec-Build-Test Checklist"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Pipeline launch file correctly defines all required nodes"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Node parameters are configurable via YAML"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Topic connections match expected message types"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","QoS policies are appropriate for real-time requirements"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Safety mechanisms (emergency stop, timeouts) are implemented"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Error handling and logging are comprehensive"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Performance metrics are monitored and logged"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Pipeline can be started/stopped cleanly"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","All dependencies are properly declared in package.xml"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Pipeline works in both simulation and real hardware (when applicable)"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"apa-citations",children:"APA Citations"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Foote, T., Lalancette, C., & Quigley, J. (2016). ROS 2: Towards a robot platform for next generation robots. ",(0,s.jsx)(n.em,{children:"Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"}),", 4698-4704."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Quigley, M., Gerkey, B., & Smart, W. D. (2009). Programming robots with ROS: A practical introduction to the Robot Operating System. ",(0,s.jsx)(n.em,{children:"Communications of the ACM"}),", 57(9), 82-91."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Kammerl, J., Holzer, S., Rusu, R. B., & Konolige, K. (2012). Real-time automated parameter tuning for multi-dimensional point cloud processing. ",(0,s.jsx)(n.em,{children:"Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)"}),", 2459-2465."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Open Robotics. (2021). ROS 2 Documentation: Composable Nodes. Retrieved from ",(0,s.jsx)(n.a,{href:"https://docs.ros.org/en/rolling/Tutorials/Composable-Nodes.html",children:"https://docs.ros.org/en/rolling/Tutorials/Composable-Nodes.html"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["L\xfctkebohle, I., & Axer, P. (2018). Design patterns for ROS-based systems: Engineering software for robots. ",(0,s.jsx)(n.em,{children:"Proceedings of the Workshop on Software Engineering for Robotics"}),", 1-6."]}),"\n"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);