"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[5526],{8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var s=i(6540);const a={},t=s.createContext(a);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(t.Provider,{value:n},e.children)}},9437:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module-3-isaac/chapter-4","title":"Chapter 18: Computer Vision for Robotics","description":"Advanced computer vision techniques using NVIDIA Isaac for humanoid robotics applications","source":"@site/docs/module-3-isaac/chapter-4.md","sourceDirName":"module-3-isaac","slug":"/module-3-isaac/chapter-4","permalink":"/hackathon-book-robotics/fr/docs/module-3-isaac/chapter-4","draft":false,"unlisted":false,"editUrl":"https://github.com/noormusarrat/hackathon-book-robotics/edit/main/docs/module-3-isaac/chapter-4.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Chapter 18: Computer Vision for Robotics","description":"Advanced computer vision techniques using NVIDIA Isaac for humanoid robotics applications"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 17: Perception Pipelines with Isaac","permalink":"/hackathon-book-robotics/fr/docs/module-3-isaac/chapter-3"},"next":{"title":"Module 3: NVIDIA Isaac - Perception + Navigation","permalink":"/hackathon-book-robotics/fr/docs/module-3-isaac/"}}');var a=i(4848),t=i(8453);const r={sidebar_position:4,title:"Chapter 18: Computer Vision for Robotics",description:"Advanced computer vision techniques using NVIDIA Isaac for humanoid robotics applications"},o="Chapter 18: Computer Vision for Robotics",c={},l=[{value:"1. Why this concept matters for humanoids",id:"1-why-this-concept-matters-for-humanoids",level:2},{value:"2. Theory",id:"2-theory",level:2},{value:"Computer Vision in Robotics Context",id:"computer-vision-in-robotics-context",level:3},{value:"Isaac Computer Vision Architecture",id:"isaac-computer-vision-architecture",level:3},{value:"Isaac Computer Vision Pipeline Components",id:"isaac-computer-vision-pipeline-components",level:3},{value:"GPU-Accelerated Computer Vision",id:"gpu-accelerated-computer-vision",level:3},{value:"3. Implementation",id:"3-implementation",level:2},{value:"4. Hardware/GPU Notes",id:"4-hardwaregpu-notes",level:2},{value:"Isaac Computer Vision GPU Requirements",id:"isaac-computer-vision-gpu-requirements",level:3},{value:"Memory Management Strategies",id:"memory-management-strategies",level:3},{value:"Jetson Platform Considerations",id:"jetson-platform-considerations",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"5. Simulation Path",id:"5-simulation-path",level:2},{value:"6. Real-World Path",id:"6-real-world-path",level:2},{value:"7. Spec-Build-Test checklist",id:"7-spec-build-test-checklist",level:2},{value:"8. APA citations",id:"8-apa-citations",level:2}];function m(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-18-computer-vision-for-robotics",children:"Chapter 18: Computer Vision for Robotics"})}),"\n",(0,a.jsx)(n.h2,{id:"1-why-this-concept-matters-for-humanoids",children:"1. Why this concept matters for humanoids"}),"\n",(0,a.jsx)(n.p,{children:"Computer vision is the eyes of humanoid robots, enabling them to perceive and understand their environment in ways that are crucial for safe and effective operation in human spaces. For humanoid robots specifically, computer vision systems must handle complex tasks like human detection and tracking, gesture recognition, facial expression analysis, and object manipulation in cluttered environments. Isaac's computer vision capabilities provide hardware-accelerated processing that allows humanoid robots to perform these sophisticated visual tasks in real-time, which is essential for natural human-robot interaction. Without advanced computer vision, humanoid robots would be blind to their surroundings and unable to navigate safely, recognize objects for manipulation, or engage in meaningful social interactions with humans. Isaac's GPU-accelerated computer vision makes it possible to run complex algorithms like deep learning-based object detection, pose estimation, and scene understanding on humanoid robots while maintaining the real-time performance required for safe operation."}),"\n",(0,a.jsx)(n.h2,{id:"2-theory",children:"2. Theory"}),"\n",(0,a.jsx)(n.h3,{id:"computer-vision-in-robotics-context",children:"Computer Vision in Robotics Context"}),"\n",(0,a.jsx)(n.p,{children:"Computer vision for robotics differs significantly from traditional computer vision applications due to the real-time, safety-critical, and resource-constrained nature of robotic systems. In the context of humanoid robots, computer vision systems must:"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Handle Dynamic Environments"}),": Unlike static computer vision tasks, humanoid robots operate in constantly changing environments with moving objects, changing lighting conditions, and dynamic obstacles."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Provide Real-time Processing"}),": Robot control systems require visual information at high frame rates (typically 10-30 Hz) to enable safe navigation and interaction."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Maintain Robustness"}),": Computer vision systems must operate reliably under various conditions including poor lighting, occlusions, and sensor noise."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Enable Actionable Perception"}),": Visual information must be processed into actionable data that can guide robot behavior and decision-making."]}),"\n",(0,a.jsx)(n.h3,{id:"isaac-computer-vision-architecture",children:"Isaac Computer Vision Architecture"}),"\n",(0,a.jsx)(n.p,{children:"Isaac's computer vision architecture is designed specifically for robotics applications and consists of several key components:"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Sensor Interface Layer"}),": Handles camera data acquisition, calibration, and preprocessing. This layer manages multiple camera types (RGB, depth, thermal) and ensures proper synchronization."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Feature Processing Layer"}),": Extracts relevant visual features using classical computer vision algorithms (SIFT, ORB, Harris corners) and deep learning models. This layer is optimized for GPU acceleration."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Object Understanding Layer"}),": Performs object detection, recognition, and classification using Isaac's hardware-accelerated deep learning capabilities. This includes both pre-trained models and custom-trained models."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Scene Analysis Layer"}),": Analyzes the 3D structure of scenes using stereo vision, structure from motion, and RGB-D data. This layer generates maps and understands spatial relationships."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Human Interaction Layer"}),": Specialized algorithms for detecting, tracking, and understanding human behavior, gestures, and facial expressions."]}),"\n",(0,a.jsx)(n.h3,{id:"isaac-computer-vision-pipeline-components",children:"Isaac Computer Vision Pipeline Components"}),"\n",(0,a.jsx)(n.p,{children:"Isaac provides several specialized computer vision components:"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS DNN"}),": Hardware-accelerated deep learning inference for object detection, classification, and segmentation tasks."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS AprilTag"}),": High-precision fiducial marker detection for localization and calibration."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS Image Pipeline"}),": Optimized image processing operations including color conversion, rectification, and filtering."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS Stereo Disparity"}),": Stereo vision processing for depth estimation and 3D reconstruction."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS Optical Flow"}),": Motion estimation and tracking algorithms optimized for GPU processing."]}),"\n",(0,a.jsx)(n.h3,{id:"gpu-accelerated-computer-vision",children:"GPU-Accelerated Computer Vision"}),"\n",(0,a.jsx)(n.p,{children:"Isaac leverages GPU acceleration for several key computer vision tasks:"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Deep Learning Inference"}),": Tensor cores accelerate neural network inference for object detection and recognition."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Image Processing"}),": CUDA cores handle traditional computer vision operations like filtering, edge detection, and feature extraction."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Stereo Vision"}),": Parallel processing capabilities enable real-time stereo disparity computation."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Optical Flow"}),": GPU acceleration enables high-frame-rate motion estimation for tracking and navigation."]}),"\n",(0,a.jsx)(n.h2,{id:"3-implementation",children:"3. Implementation"}),"\n",(0,a.jsx)(n.p,{children:"Let's implement comprehensive Isaac computer vision systems for humanoid robotics:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# isaac_humanoid_vision/isaac_humanoid_vision/computer_vision_pipeline.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2\nfrom geometry_msgs.msg import PointStamped, PoseStamped\nfrom visualization_msgs.msg import MarkerArray\nfrom std_msgs.msg import Header, Bool, String\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport threading\nfrom typing import Dict, Any, Optional, List, Tuple\nimport time\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport math\n\nclass VisionTask(Enum):\n    """Computer vision tasks for humanoid robots"""\n    OBJECT_DETECTION = "object_detection"\n    HUMAN_DETECTION = "human_detection"\n    FACE_RECOGNITION = "face_recognition"\n    GESTURE_RECOGNITION = "gesture_recognition"\n    POSE_ESTIMATION = "pose_estimation"\n    DEPTH_ESTIMATION = "depth_estimation"\n    OPTICAL_FLOW = "optical_flow"\n\n@dataclass\nclass VisionResult:\n    """Data structure for computer vision results"""\n    timestamp: float\n    task: VisionTask\n    detections: List[Dict[str, Any]]\n    features: List[Tuple[float, float]]\n    pose: Optional[PoseStamped]\n    confidence: float\n    metadata: Dict[str, Any]\n\nclass IsaacComputerVisionPipeline(Node):\n    """\n    Isaac computer vision pipeline for humanoid robotics\n    """\n    def __init__(self):\n        super().__init__(\'isaac_computer_vision_pipeline\')\n\n        # Initialize components\n        self.bridge = CvBridge()\n        self.pipeline_lock = threading.Lock()\n        self.active_tasks = set([VisionTask.OBJECT_DETECTION, VisionTask.HUMAN_DETECTION])\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n\n        # Sensor data storage\n        self.latest_images = {}\n        self.previous_image = None\n        self.feature_points = []\n        self.tracked_objects = {}\n\n        # Configuration parameters\n        self.detection_threshold = 0.7\n        self.tracking_enabled = True\n        self.face_recognition_enabled = False\n\n        # Publishers for vision results\n        self.detection_pub = self.create_publisher(MarkerArray, \'/isaac/vision/detections\', 10)\n        self.human_pub = self.create_publisher(MarkerArray, \'/isaac/vision/humans\', 10)\n        self.face_pub = self.create_publisher(MarkerArray, \'/isaac/vision/faces\', 10)\n        self.pose_pub = self.create_publisher(PoseStamped, \'/isaac/vision/pose\', 10)\n        self.status_pub = self.create_publisher(Bool, \'/isaac/vision/ready\', 10)\n        self.debug_pub = self.create_publisher(Image, \'/isaac/vision/debug_image\', 10)\n\n        # Subscribers for sensor data\n        self.rgb_sub = self.create_subscription(\n            Image, \'/camera/rgb/image_raw\', self.rgb_callback, 10\n        )\n        self.depth_sub = self.create_subscription(\n            Image, \'/camera/depth/image_raw\', self.depth_callback, 10\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, \'/camera/rgb/camera_info\', self.camera_info_callback, 10\n        )\n\n        # Timer for vision processing\n        self.pipeline_timer = self.create_timer(0.05, self.process_vision_pipeline)  # 20 Hz\n\n        # Initialize vision components\n        self.initialize_vision_components()\n\n        self.get_logger().info(\'Isaac Computer Vision Pipeline initialized\')\n\n    def camera_info_callback(self, msg):\n        """Store camera calibration parameters"""\n        with self.pipeline_lock:\n            self.camera_matrix = np.array(msg.k).reshape(3, 3)\n            self.distortion_coeffs = np.array(msg.d)\n\n    def rgb_callback(self, msg):\n        """Process RGB camera data"""\n        with self.pipeline_lock:\n            try:\n                cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n                self.latest_images[\'rgb\'] = {\n                    \'image\': cv_image,\n                    \'timestamp\': msg.header.stamp,\n                    \'encoding\': msg.encoding\n                }\n            except Exception as e:\n                self.get_logger().error(f\'Error processing RGB image: {e}\')\n\n    def depth_callback(self, msg):\n        """Process depth camera data"""\n        with self.pipeline_lock:\n            try:\n                cv_depth = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'32FC1\')\n                self.latest_images[\'depth\'] = {\n                    \'image\': cv_depth,\n                    \'timestamp\': msg.header.stamp,\n                    \'encoding\': msg.encoding\n                }\n            except Exception as e:\n                self.get_logger().error(f\'Error processing depth image: {e}\')\n\n    def initialize_vision_components(self):\n        """Initialize computer vision components"""\n        self.get_logger().info(\'Initializing computer vision components...\')\n\n        # Initialize object detection model\n        self.initialize_object_detection()\n\n        # Initialize human detection\n        self.initialize_human_detection()\n\n        # Initialize face recognition\n        self.initialize_face_recognition()\n\n        # Initialize pose estimation\n        self.initialize_pose_estimation()\n\n        # Publish ready status\n        ready_msg = Bool()\n        ready_msg.data = True\n        self.status_pub.publish(ready_msg)\n\n        self.get_logger().info(\'Computer vision components initialized\')\n\n    def initialize_object_detection(self):\n        """Initialize object detection model"""\n        self.get_logger().info(\'Initializing object detection model...\')\n        # In a real implementation, this would load a pre-trained model\n        # For example: self.od_model = load_trt_model(\'coco_model.trt\')\n        time.sleep(0.2)  # Simulate model loading time\n\n    def initialize_human_detection(self):\n        """Initialize human detection model"""\n        self.get_logger().info(\'Initializing human detection model...\')\n        # In a real implementation, this would load a human-specific model\n        # For example: self.human_model = load_trt_model(\'human_model.trt\')\n        time.sleep(0.1)  # Simulate model loading time\n\n    def initialize_face_recognition(self):\n        """Initialize face recognition model"""\n        self.get_logger().info(\'Initializing face recognition model...\')\n        # In a real implementation, this would load face recognition models\n        # For example: self.face_model = load_face_recognition_model()\n        time.sleep(0.1)  # Simulate model loading time\n\n    def initialize_pose_estimation(self):\n        """Initialize pose estimation model"""\n        self.get_logger().info(\'Initializing pose estimation model...\')\n        # In a real implementation, this would load pose estimation models\n        # For example: self.pose_model = load_pose_model()\n        time.sleep(0.1)  # Simulate model loading time\n\n    def process_vision_pipeline(self):\n        """Main computer vision processing loop"""\n        with self.pipeline_lock:\n            if not self.camera_matrix or \'rgb\' not in self.latest_images:\n                return\n\n            # Get latest RGB image\n            rgb_data = self.latest_images[\'rgb\']\n            rgb_image = rgb_data[\'image\']\n\n            # Process active vision tasks\n            all_results = []\n\n            if VisionTask.OBJECT_DETECTION in self.active_tasks:\n                result = self.process_object_detection(rgb_image)\n                if result:\n                    all_results.append(result)\n\n            if VisionTask.HUMAN_DETECTION in self.active_tasks:\n                result = self.process_human_detection(rgb_image)\n                if result:\n                    all_results.append(result)\n\n            if VisionTask.FACE_RECOGNITION in self.active_tasks:\n                result = self.process_face_recognition(rgb_image)\n                if result:\n                    all_results.append(result)\n\n            if VisionTask.POSE_ESTIMATION in self.active_tasks:\n                result = self.process_pose_estimation(rgb_image)\n                if result:\n                    all_results.append(result)\n\n            # Publish results\n            if all_results:\n                self.publish_vision_results(all_results)\n\n            # Publish debug image if needed\n            if self.get_logger().level <= 10:  # DEBUG level\n                debug_image = self.create_debug_image(rgb_image, all_results)\n                debug_msg = self.bridge.cv2_to_imgmsg(debug_image, encoding=\'bgr8\')\n                debug_msg.header.stamp = self.get_clock().now().to_msg()\n                debug_msg.header.frame_id = \'camera_rgb_optical_frame\'\n                self.debug_pub.publish(debug_msg)\n\n    def process_object_detection(self, image):\n        """Process object detection using Isaac\'s optimized algorithms"""\n        # In a real implementation, this would use Isaac ROS DNN packages\n        # For this example, we\'ll implement a simple color-based detection\n        detections = []\n\n        # Convert to HSV for color-based detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define color ranges for common objects\n        color_ranges = [\n            (np.array([0, 50, 50]), np.array([10, 255, 255]), \'red_object\'),    # Red\n            (np.array([100, 50, 50]), np.array([130, 255, 255]), \'blue_object\'), # Blue\n            (np.array([30, 50, 50]), np.array([80, 255, 255]), \'green_object\'),  # Green\n        ]\n\n        for lower, upper, obj_type in color_ranges:\n            mask = cv2.inRange(hsv, lower, upper)\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            for contour in contours:\n                area = cv2.contourArea(contour)\n                if area > 1000:  # Filter small contours\n                    x, y, w, h = cv2.boundingRect(contour)\n                    detections.append({\n                        \'class\': obj_type,\n                        \'confidence\': 0.8,\n                        \'bbox\': (x, y, w, h),\n                        \'area\': area,\n                        \'center\': (x + w//2, y + h//2)\n                    })\n\n        return VisionResult(\n            timestamp=time.time(),\n            task=VisionTask.OBJECT_DETECTION,\n            detections=detections,\n            features=[],\n            pose=None,\n            confidence=0.8 if detections else 0.0,\n            metadata={\'object_count\': len(detections)}\n        )\n\n    def process_human_detection(self, image):\n        """Process human detection using Isaac\'s optimized algorithms"""\n        # In a real implementation, this would use Isaac ROS DNN with human detection model\n        # For this example, we\'ll implement a simple HOG-based detection\n        detections = []\n\n        # Use HOG descriptor for human detection (simplified)\n        hog = cv2.HOGDescriptor()\n        # Note: In real implementation, use Isaac\'s hardware-accelerated human detection\n\n        # For demonstration, detect humans using a simple method\n        # Convert to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Detect people using HOG\n        # This is a simplified approach - real implementation would use Isaac ROS\n        # hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n        # boxes, weights = hog.detectMultiScale(gray, winStride=(8,8), padding=(32,32), scale=1.05)\n\n        # For this example, we\'ll simulate human detection\n        # In a real implementation, this would use Isaac\'s optimized human detection\n        height, width = gray.shape\n        # Simulate detection of humans in specific areas\n        if width > 400 and height > 300:\n            # Simulate finding a human in the center\n            x, y, w, h = width//2 - 50, height//2 - 100, 100, 200\n            detections.append({\n                \'class\': \'person\',\n                \'confidence\': 0.9,\n                \'bbox\': (x, y, w, h),\n                \'area\': w * h,\n                \'center\': (x + w//2, y + h//2)\n            })\n\n        return VisionResult(\n            timestamp=time.time(),\n            task=VisionTask.HUMAN_DETECTION,\n            detections=detections,\n            features=[],\n            pose=None,\n            confidence=0.9 if detections else 0.0,\n            metadata={\'human_count\': len(detections)}\n        )\n\n    def process_face_recognition(self, image):\n        """Process face recognition (placeholder implementation)"""\n        # In a real implementation, this would use Isaac ROS face recognition\n        # For now, return a simple result\n        detections = []\n\n        # Use OpenCV\'s face detection as a placeholder\n        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \'haarcascade_frontalface_default.xml\')\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n        for (x, y, w, h) in faces:\n            detections.append({\n                \'class\': \'face\',\n                \'confidence\': 0.85,\n                \'bbox\': (x, y, w, h),\n                \'area\': w * h,\n                \'center\': (x + w//2, y + h//2)\n            })\n\n        return VisionResult(\n            timestamp=time.time(),\n            task=VisionTask.FACE_RECOGNITION,\n            detections=detections,\n            features=[],\n            pose=None,\n            confidence=0.85 if detections else 0.0,\n            metadata={\'face_count\': len(detections)}\n        )\n\n    def process_pose_estimation(self, image):\n        """Process human pose estimation (placeholder implementation)"""\n        # In a real implementation, this would use Isaac ROS pose estimation\n        # For now, return a simple result\n        detections = []\n\n        # Placeholder for pose estimation\n        # In real implementation, use Isaac\'s pose estimation models\n        return VisionResult(\n            timestamp=time.time(),\n            task=VisionTask.POSE_ESTIMATION,\n            detections=detections,\n            features=[],\n            pose=None,\n            confidence=0.9,\n            metadata={\'keypoints_detected\': 0}\n        )\n\n    def publish_vision_results(self, results: List[VisionResult]):\n        """Publish vision results to appropriate ROS topics"""\n        for result in results:\n            if result.task == VisionTask.OBJECT_DETECTION and result.detections:\n                marker_array = self.create_object_markers(result.detections)\n                self.detection_pub.publish(marker_array)\n\n            elif result.task == VisionTask.HUMAN_DETECTION and result.detections:\n                marker_array = self.create_human_markers(result.detections)\n                self.human_pub.publish(marker_array)\n\n            elif result.task == VisionTask.FACE_RECOGNITION and result.detections:\n                marker_array = self.create_face_markers(result.detections)\n                self.face_pub.publish(marker_array)\n\n            elif result.task == VisionTask.POSE_ESTIMATION and result.pose:\n                self.pose_pub.publish(result.pose)\n\n    def create_object_markers(self, objects):\n        """Create visualization markers for object detections"""\n        marker_array = MarkerArray()\n\n        for i, obj in enumerate(objects):\n            marker = self.create_object_marker(obj, i)\n            marker_array.markers.append(marker)\n\n        return marker_array\n\n    def create_object_marker(self, obj, id_num):\n        """Create a single object marker"""\n        from visualization_msgs.msg import Marker\n        from geometry_msgs.msg import Point\n\n        marker = Marker()\n        marker.header.frame_id = "camera_rgb_optical_frame"\n        marker.header.stamp = self.get_clock().now().to_msg()\n        marker.ns = "vision_objects"\n        marker.id = id_num\n        marker.type = Marker.CUBE\n        marker.action = Marker.ADD\n\n        # Set position based on bounding box center\n        x, y, w, h = obj[\'bbox\']\n        marker.pose.position.x = (x + w/2) / 100.0  # Scale down for visualization\n        marker.pose.position.y = (y + h/2) / 100.0\n        marker.pose.position.z = 1.0  # Fixed depth for visualization\n\n        # Set scale based on bounding box size\n        marker.scale.x = w / 100.0\n        marker.scale.y = h / 100.0\n        marker.scale.z = 0.2  # Fixed height\n\n        # Set color based on object class\n        if \'red\' in obj[\'class\']:\n            marker.color.r = 1.0\n            marker.color.g = 0.0\n            marker.color.b = 0.0\n        elif \'blue\' in obj[\'class\']:\n            marker.color.r = 0.0\n            marker.color.g = 0.0\n            marker.color.b = 1.0\n        elif \'green\' in obj[\'class\']:\n            marker.color.r = 0.0\n            marker.color.g = 1.0\n            marker.color.b = 0.0\n        elif \'person\' in obj[\'class\']:\n            marker.color.r = 0.0\n            marker.color.g = 1.0\n            marker.color.b = 1.0\n        else:\n            marker.color.r = 1.0\n            marker.color.g = 1.0\n            marker.color.b = 0.0\n\n        marker.color.a = 0.7\n\n        # Set object label\n        marker.text = f"{obj[\'class\']}: {obj[\'confidence\']:.2f}"\n\n        return marker\n\n    def create_human_markers(self, humans):\n        """Create visualization markers for human detections"""\n        marker_array = MarkerArray()\n\n        for i, human in enumerate(humans):\n            marker = self.create_human_marker(human, i)\n            marker_array.markers.append(marker)\n\n        return marker_array\n\n    def create_human_marker(self, human, id_num):\n        """Create a single human marker"""\n        from visualization_msgs.msg import Marker\n\n        marker = Marker()\n        marker.header.frame_id = "camera_rgb_optical_frame"\n        marker.header.stamp = self.get_clock().now().to_msg()\n        marker.ns = "vision_humans"\n        marker.id = id_num\n        marker.type = Marker.CYLINDER\n        marker.action = Marker.ADD\n\n        # Set position based on bounding box center\n        x, y, w, h = human[\'bbox\']\n        marker.pose.position.x = (x + w/2) / 100.0\n        marker.pose.position.y = (y + h/2) / 100.0\n        marker.pose.position.z = 1.5  # Human height for visualization\n\n        # Set scale\n        marker.scale.x = w / 100.0\n        marker.scale.y = w / 100.0\n        marker.scale.z = h / 100.0\n\n        # Set color\n        marker.color.r = 0.0\n        marker.color.g = 1.0\n        marker.color.b = 1.0\n        marker.color.a = 0.8\n\n        marker.text = f"Person: {human[\'confidence\']:.2f}"\n\n        return marker\n\n    def create_face_markers(self, faces):\n        """Create visualization markers for face detections"""\n        marker_array = MarkerArray()\n\n        for i, face in enumerate(faces):\n            marker = self.create_face_marker(face, i)\n            marker_array.markers.append(marker)\n\n        return marker_array\n\n    def create_face_marker(self, face, id_num):\n        """Create a single face marker"""\n        from visualization_msgs.msg import Marker\n\n        marker = Marker()\n        marker.header.frame_id = "camera_rgb_optical_frame"\n        marker.header.stamp = self.get_clock().now().to_msg()\n        marker.ns = "vision_faces"\n        marker.id = id_num\n        marker.type = Marker.SPHERE\n        marker.action = Marker.ADD\n\n        # Set position based on bounding box center\n        x, y, w, h = face[\'bbox\']\n        marker.pose.position.x = (x + w/2) / 100.0\n        marker.pose.position.y = (y + h/2) / 100.0\n        marker.pose.position.z = 1.2  # Face position for visualization\n\n        # Set scale\n        marker.scale.x = w / 100.0\n        marker.scale.y = h / 100.0\n        marker.scale.z = 0.1\n\n        # Set color\n        marker.color.r = 1.0\n        marker.color.g = 0.5\n        marker.color.b = 0.0\n        marker.color.a = 0.8\n\n        marker.text = f"Face: {face[\'confidence\']:.2f}"\n\n        return marker\n\n    def create_debug_image(self, original_image, results):\n        """Create debug visualization of vision processing"""\n        debug_image = original_image.copy()\n\n        # Draw all detection results\n        for result in results:\n            if result.detections:\n                for detection in result.detections:\n                    x, y, w, h = detection[\'bbox\']\n                    color = (0, 255, 0) if result.task == VisionTask.HUMAN_DETECTION else (255, 0, 0)\n                    cv2.rectangle(debug_image, (x, y), (x + w, y + h), color, 2)\n                    cv2.putText(debug_image, f"{detection[\'class\']}: {detection[\'confidence\']:.2f}",\n                               (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n        return debug_image\n\n    def add_vision_task(self, task: VisionTask):\n        """Add a vision task to the active processing list"""\n        with self.pipeline_lock:\n            self.active_tasks.add(task)\n            self.get_logger().info(f\'Added vision task: {task.value}\')\n\n    def remove_vision_task(self, task: VisionTask):\n        """Remove a vision task from the active processing list"""\n        with self.pipeline_lock:\n            self.active_tasks.discard(task)\n            self.get_logger().info(f\'Removed vision task: {task.value}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacComputerVisionPipeline()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Shutting down Isaac Computer Vision Pipeline\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.p,{children:"Create the computer vision configuration:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# isaac_humanoid_vision/config/computer_vision_config.yaml\nisaac_computer_vision_pipeline:\n  ros__parameters:\n    # Active vision tasks\n    active_tasks:\n      object_detection: true\n      human_detection: true\n      face_recognition: false\n      gesture_recognition: false\n      pose_estimation: false\n      depth_estimation: false\n      optical_flow: false\n\n    # Object detection parameters\n    object_detection:\n      model_type: "tensorrt_yolo"\n      model_path: "/path/to/coco_model.trt"\n      confidence_threshold: 0.7\n      nms_threshold: 0.4\n      max_objects: 50\n      tracking_enabled: true\n\n    # Human detection parameters\n    human_detection:\n      model_type: "tensorrt_human"\n      model_path: "/path/to/human_model.trt"\n      confidence_threshold: 0.8\n      max_humans: 10\n      tracking_enabled: true\n\n    # Face recognition parameters\n    face_recognition:\n      model_type: "tensorrt_face"\n      model_path: "/path/to/face_model.trt"\n      confidence_threshold: 0.85\n      max_faces: 5\n      recognition_enabled: true\n\n    # Pose estimation parameters\n    pose_estimation:\n      model_type: "tensorrt_pose"\n      model_path: "/path/to/pose_model.trt"\n      confidence_threshold: 0.8\n      max_poses: 10\n      keypoint_threshold: 0.1\n\n    # Processing parameters\n    processing:\n      frame_rate: 20.0  # Hz\n      queue_size: 10\n      max_queue_size: 100\n      enable_multithreading: true\n      synchronization_window: 0.05  # seconds\n\n    # GPU acceleration settings\n    gpu:\n      device_id: 0\n      memory_fraction: 0.8  # 80% of available GPU memory\n      enable_tensorrt: true\n      tensorrt_precision: "fp16"\n      use_cuda_graph: true\n\n    # Camera parameters\n    camera:\n      rectification_enabled: true\n      undistortion_enabled: true\n      resolution_scale: 1.0  # Scale factor for processing\n\n    # Feature tracking parameters\n    feature_tracking:\n      max_features: 1000\n      quality_level: 0.01\n      min_distance: 10\n      block_size: 3\n      use_harris_detector: false\n      k: 0.04\n\n    # Performance monitoring\n    performance:\n      enable_profiling: true\n      publish_statistics: true\n      statistics_topic: "/isaac/vision/performance"\n      warning_threshold: 0.8  # 80% of target frame rate\n'})}),"\n",(0,a.jsx)(n.p,{children:"Create the launch file for the computer vision pipeline:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:"\x3c!-- isaac_humanoid_vision/launch/isaac_computer_vision.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    namespace = LaunchConfiguration('namespace')\n    enable_face_recognition = LaunchConfiguration('enable_face_recognition')\n\n    return LaunchDescription([\n        # Declare launch arguments\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation (Gazebo) clock if true'\n        ),\n        DeclareLaunchArgument(\n            'namespace',\n            default_value='',\n            description='Robot namespace'\n        ),\n        DeclareLaunchArgument(\n            'enable_face_recognition',\n            default_value='false',\n            description='Enable face recognition (may require additional models)'\n        ),\n\n        # Isaac Computer Vision Pipeline\n        Node(\n            package='isaac_humanoid_vision',\n            executable='isaac_computer_vision_pipeline',\n            name='isaac_computer_vision_pipeline',\n            namespace=namespace,\n            parameters=[\n                os.path.join(\n                    get_package_share_directory('isaac_humanoid_vision'),\n                    'config',\n                    'computer_vision_config.yaml'\n                ),\n                {'use_sim_time': use_sim_time}\n            ],\n            output='screen',\n            respawn=True,\n            respawn_delay=2\n        ),\n\n        # Isaac ROS Image Pipeline (Color Conversion)\n        Node(\n            package='isaac_ros_image_pipeline',\n            executable='isaac_ros_color_convert',\n            name='color_convert_node',\n            namespace=namespace,\n            parameters=[\n                {\n                    'input_encoding': 'rgb8',\n                    'output_encoding': 'bgr8',\n                    'use_sim_time': use_sim_time\n                }\n            ],\n            remappings=[\n                ('image_raw', '/camera/rgb/image_raw'),\n                ('image_color_converted', '/camera/rgb/image_converted')\n            ],\n            output='screen'\n        ),\n\n        # Isaac ROS Depth Preprocessor\n        Node(\n            package='isaac_ros_depth_preprocessor',\n            executable='isaac_ros_depth_preprocessor',\n            name='depth_preprocessor_node',\n            namespace=namespace,\n            parameters=[\n                {\n                    'input_encoding': '16UC1',\n                    'output_encoding': '32FC1',\n                    'use_sim_time': use_sim_time\n                }\n            ],\n            remappings=[\n                ('depth/image', '/camera/depth/image_raw'),\n                ('depth/image_processed', '/camera/depth/image_processed')\n            ],\n            output='screen'\n        ),\n\n        # Isaac ROS AprilTag Detector (for pose estimation)\n        Node(\n            package='isaac_ros_apriltag',\n            executable='isaac_ros_apriltag',\n            name='apriltag_node',\n            namespace=namespace,\n            parameters=[\n                {\n                    'family': 'tag36h11',\n                    'size': 0.1,\n                    'max_tags': 10,\n                    'use_sim_time': use_sim_time\n                }\n            ],\n            remappings=[\n                ('image', '/camera/rgb/image_raw'),\n                ('camera_info', '/camera/rgb/camera_info'),\n                ('detections', '/isaac/vision/apriltag_detections')\n            ],\n            output='screen'\n        ),\n\n        # Isaac ROS DNN Image Encoder (for object detection)\n        Node(\n            package='isaac_ros_dnn_image_encoder',\n            executable='isaac_ros_image_preprocessor',\n            name='dnn_image_encoder',\n            namespace=namespace,\n            parameters=[\n                {\n                    'input_image_width': 960,\n                    'input_image_height': 540,\n                    'output_tensor_width': 640,\n                    'output_tensor_height': 640,\n                    'keep_aspect_ratio': True,\n                    'normalize_linear': False,\n                    'use_sim_time': use_sim_time\n                }\n            ],\n            remappings=[\n                ('image', '/camera/rgb/image_raw'),\n                ('image_encoded', '/isaac/vision/image_encoded')\n            ],\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,a.jsx)(n.p,{children:"Create a vision task manager for dynamic task control:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# isaac_humanoid_vision/isaac_humanoid_vision/vision_task_manager.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import Image\nimport threading\nfrom enum import Enum\n\nclass VisionTask(Enum):\n    """Computer vision tasks for humanoid robots"""\n    OBJECT_DETECTION = "object_detection"\n    HUMAN_DETECTION = "human_detection"\n    FACE_RECOGNITION = "face_recognition"\n    GESTURE_RECOGNITION = "gesture_recognition"\n    POSE_ESTIMATION = "pose_estimation"\n    DEPTH_ESTIMATION = "depth_estimation"\n    OPTICAL_FLOW = "optical_flow"\n\nclass VisionTaskManager(Node):\n    """\n    Manager for dynamic control of Isaac computer vision tasks\n    """\n    def __init__(self):\n        super().__init__(\'vision_task_manager\')\n\n        # Initialize task management\n        self.active_tasks = set([VisionTask.OBJECT_DETECTION, VisionTask.HUMAN_DETECTION])\n        self.task_lock = threading.Lock()\n\n        # Publishers\n        self.task_status_pub = self.create_publisher(Bool, \'/isaac/vision/task_status\', 10)\n\n        # Subscribers for task control\n        self.task_control_sub = self.create_subscription(\n            String, \'/isaac/vision/task_control\', self.task_control_callback, 10\n        )\n\n        # Timer for task monitoring\n        self.status_timer = self.create_timer(1.0, self.publish_task_status)\n\n        self.get_logger().info(\'Vision Task Manager initialized\')\n\n    def task_control_callback(self, msg):\n        """Handle task control commands"""\n        command = msg.data.strip().lower()\n        parts = command.split(\':\')\n\n        if len(parts) >= 2:\n            action = parts[0]\n            task_name = parts[1]\n\n            try:\n                task = VisionTask(task_name)\n\n                with self.task_lock:\n                    if action == \'enable\':\n                        self.active_tasks.add(task)\n                        self.get_logger().info(f\'Enabled vision task: {task.value}\')\n                    elif action == \'disable\':\n                        self.active_tasks.discard(task)\n                        self.get_logger().info(f\'Disabled vision task: {task.value}\')\n                    else:\n                        self.get_logger().warn(f\'Unknown action: {action}\')\n            except ValueError:\n                self.get_logger().error(f\'Unknown vision task: {task_name}\')\n        else:\n            self.get_logger().error(f\'Invalid command format: {command}. Expected: action:task\')\n\n    def publish_task_status(self):\n        """Publish current task status"""\n        status_msg = Bool()\n        status_msg.data = len(self.active_tasks) > 0\n        self.task_status_pub.publish(status_msg)\n\n    def get_active_tasks(self):\n        """Get list of currently active tasks"""\n        with self.task_lock:\n            return list(self.active_tasks)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisionTaskManager()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Shutting down Vision Task Manager\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"4-hardwaregpu-notes",children:"4. Hardware/GPU Notes"}),"\n",(0,a.jsx)(n.h3,{id:"isaac-computer-vision-gpu-requirements",children:"Isaac Computer Vision GPU Requirements"}),"\n",(0,a.jsx)(n.p,{children:"Isaac computer vision applications have varying GPU requirements based on the specific vision tasks:"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Object Detection"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Minimum"}),": RTX 4070 Ti (12GB VRAM) for basic models"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Recommended"}),": RTX 4080/4090 (16-24GB VRAM) for complex models"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory"}),": 2-8GB depending on model size and input resolution"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Compute"}),": Tensor cores recommended for INT8/FP16 inference"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Human Detection"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory"}),": 2-6GB VRAM for human-specific models"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Compute"}),": Optimized for real-time processing of human shapes"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Accuracy"}),": Higher accuracy models require more VRAM"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Face Recognition"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory"}),": 3-8GB VRAM for face detection and recognition"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Compute"}),": Specialized for facial feature extraction"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Latency"}),": Low latency critical for real-time interaction"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Pose Estimation"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory"}),": 4-10GB VRAM for keypoint detection"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Compute"}),": High computational requirements for keypoint estimation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Real-time"}),": Critical for gesture recognition and interaction"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"memory-management-strategies",children:"Memory Management Strategies"}),"\n",(0,a.jsx)(n.p,{children:"For optimal computer vision performance:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory Pooling"}),": Pre-allocate GPU memory pools for different vision tasks"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Zero-Copy Memory"}),": Use CUDA zero-copy for frequent CPU-GPU transfers"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Unified Memory"}),": Leverage CUDA unified memory for automatic management"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory Monitoring"}),": Continuously monitor GPU memory usage to prevent overflow"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"jetson-platform-considerations",children:"Jetson Platform Considerations"}),"\n",(0,a.jsx)(n.p,{children:"When running computer vision on Jetson platforms:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory Architecture"}),": Unified memory architecture simplifies memory management"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Power Efficiency"}),": Vision pipelines optimized for power-constrained environments"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Thermal Management"}),": Monitor temperature during intensive vision processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"I/O Bandwidth"}),": Maximize camera data bandwidth for real-time processing"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"TensorRT Integration"}),": Use TensorRT for optimized deep learning inference"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"CUDA Streams"}),": Use multiple CUDA streams for overlapping operations"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Async Processing"}),": Implement asynchronous processing to maximize GPU utilization"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Batch Processing"}),": Process multiple inputs simultaneously when possible"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Model Quantization"}),": Use INT8 quantization to reduce memory usage and increase speed"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resolution Scaling"}),": Adjust input resolution based on performance requirements"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"5-simulation-path",children:"5. Simulation Path"}),"\n",(0,a.jsx)(n.p,{children:"To implement Isaac computer vision in simulation:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Isaac Sim Setup"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Launch Isaac Sim with vision sensors\ncd ~/isaac-sim\npython3 -m omni.isaac.kit --summary-cache-path ./cache\n\n# Configure RGB-D cameras and other vision sensors in simulation\n# Set up human avatars and objects for vision testing\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Computer Vision Pipeline Testing"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Launch computer vision pipeline in simulation\nros2 launch isaac_humanoid_vision isaac_computer_vision_sim.launch.py\n\n# Test different vision tasks\nros2 topic pub /isaac/vision/task_control std_msgs/String \"data: 'enable:face_recognition'\"\nros2 topic pub /isaac/vision/task_control std_msgs/String \"data: 'disable:pose_estimation'\"\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Performance Validation"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Test vision accuracy in simulated environments"}),"\n",(0,a.jsx)(n.li,{children:"Validate processing frame rates and latency"}),"\n",(0,a.jsx)(n.li,{children:"Measure GPU memory usage and performance"}),"\n",(0,a.jsx)(n.li,{children:"Verify safety systems in simulation"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"6-real-world-path",children:"6. Real-World Path"}),"\n",(0,a.jsx)(n.p,{children:"For real-world deployment of Isaac computer vision:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Hardware Integration"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Integrate vision sensors with humanoid robot platform"}),"\n",(0,a.jsx)(n.li,{children:"Calibrate cameras and depth sensors"}),"\n",(0,a.jsx)(n.li,{children:"Configure vision processing pipeline"}),"\n",(0,a.jsx)(n.li,{children:"Validate sensor data quality and timing"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"System Integration"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Build Isaac vision workspace\ncd ~/isaac_vision_ws\ncolcon build --packages-select isaac_humanoid_vision\nsource install/setup.bash\n\n# Launch computer vision pipeline on robot\nros2 launch isaac_humanoid_vision isaac_computer_vision.launch.py\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Validation and Testing"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Test vision accuracy in real environments"}),"\n",(0,a.jsx)(n.li,{children:"Validate real-time performance requirements"}),"\n",(0,a.jsx)(n.li,{children:"Verify safety systems and emergency stops"}),"\n",(0,a.jsx)(n.li,{children:"Ensure system stability and reliability"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"7-spec-build-test-checklist",children:"7. Spec-Build-Test checklist"}),"\n",(0,a.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Isaac computer vision pipeline node implemented and functional"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Multi-task vision processing working correctly"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Object detection implementation functional"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Human detection implementation working"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Face recognition placeholder implemented"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Pose estimation placeholder implemented"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Vision task management system functional"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Visualization markers published correctly"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Configuration parameters properly set"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Launch files created and tested"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Performance monitoring implemented"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","GPU memory management implemented"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Dynamic task control functional"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Isaac computer vision pipeline validated in simulation"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"8-apa-citations",children:"8. APA citations"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["NVIDIA Corporation. (2023). ",(0,a.jsx)(n.em,{children:"Isaac ROS: Computer Vision Packages"}),". NVIDIA Developer Documentation. Retrieved from ",(0,a.jsx)(n.a,{href:"https://docs.nvidia.com/isaac/isaac_ros/",children:"https://docs.nvidia.com/isaac/isaac_ros/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Redmon, J., & Farhadi, A. (2018). YOLOv3: An incremental improvement. ",(0,a.jsx)(n.em,{children:"arXiv preprint arXiv:1804.02767"}),"."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: Towards real-time object detection with region proposal networks. ",(0,a.jsx)(n.em,{children:"Advances in Neural Information Processing Systems"}),", 28, 91-99."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Dollar, P., Wojek, C., Schiele, B., & Perona, P. (2012). Pedestrian detection: An evaluation of the state of the art. ",(0,a.jsx)(n.em,{children:"IEEE Transactions on Pattern Analysis and Machine Intelligence"}),", 34(4), 743-761."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. ",(0,a.jsx)(n.em,{children:"IEEE Computer Society Conference on Computer Vision and Pattern Recognition"}),", 1, 886-893."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Viola, P., & Jones, M. (2001). Rapid object detection using a boosted cascade of simple features. ",(0,a.jsx)(n.em,{children:"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition"}),", 1, 511-518."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Long, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic segmentation. ",(0,a.jsx)(n.em,{children:"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"}),", 3431-3440."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Ren, M., & Urtasun, R. (2017). End-to-end learning of driving models from large-scale video datasets. ",(0,a.jsx)(n.em,{children:"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"}),", 3530-3538."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Geiger, A., Lenz, P., & Urtasun, R. (2012). Are we ready for autonomous driving? The KITTI vision benchmark suite. ",(0,a.jsx)(n.em,{children:"Conference on Computer Vision and Pattern Recognition"}),", 3354-3361."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., & Berg, A. C. (2016). SSD: Single shot multibox detector. ",(0,a.jsx)(n.em,{children:"European Conference on Computer Vision"}),", 21-37."]}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}}}]);