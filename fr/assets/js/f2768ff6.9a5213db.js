"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[8862],{8453:(n,e,t)=>{t.d(e,{R:()=>i,x:()=>o});var s=t(6540);const a={},r=s.createContext(a);function i(n){const e=s.useContext(r);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:i(n.components),s.createElement(r.Provider,{value:e},n.children)}},9971:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module-4-vla/chapter-4","title":"Chapter 25: Natural Language Understanding","description":"Implementing semantic understanding and intent recognition for robotics applications","source":"@site/docs/module-4-vla/chapter-4.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-4","permalink":"/hackathon-book-robotics/fr/docs/module-4-vla/chapter-4","draft":false,"unlisted":false,"editUrl":"https://github.com/noormusarrat/hackathon-book-robotics/edit/main/docs/module-4-vla/chapter-4.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Chapter 25: Natural Language Understanding","description":"Implementing semantic understanding and intent recognition for robotics applications"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 24: Voice Input Processing","permalink":"/hackathon-book-robotics/fr/docs/module-4-vla/chapter-3"},"next":{"title":"Chapter 26: Action Planning from Language","permalink":"/hackathon-book-robotics/fr/docs/module-4-vla/chapter-5"}}');var a=t(4848),r=t(8453);const i={sidebar_position:4,title:"Chapter 25: Natural Language Understanding",description:"Implementing semantic understanding and intent recognition for robotics applications"},o="Chapter 25: Natural Language Understanding",c={},l=[{value:"1. Why this concept matters for humanoids",id:"1-why-this-concept-matters-for-humanoids",level:2},{value:"2. Theory",id:"2-theory",level:2},{value:"3. Implementation",id:"3-implementation",level:2},{value:"4. Hardware/GPU Notes",id:"4-hardwaregpu-notes",level:2},{value:"5. Simulation Path",id:"5-simulation-path",level:2},{value:"6. Real-World Path",id:"6-real-world-path",level:2},{value:"7. Spec-Build-Test checklist",id:"7-spec-build-test-checklist",level:2},{value:"8. APA citations",id:"8-apa-citations",level:2}];function d(n){const e={code:"code",em:"em",h1:"h1",h2:"h2",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"chapter-25-natural-language-understanding",children:"Chapter 25: Natural Language Understanding"})}),"\n",(0,a.jsx)(e.h2,{id:"1-why-this-concept-matters-for-humanoids",children:"1. Why this concept matters for humanoids"}),"\n",(0,a.jsx)(e.p,{children:"Natural Language Understanding (NLU) is crucial for humanoid robots to interpret human commands beyond simple keyword matching. While speech recognition converts audio to text, NLU enables robots to comprehend the meaning, intent, and context behind human language. This capability allows humanoid robots to understand complex, nuanced commands, handle ambiguous requests, and respond appropriately to variations in how humans express the same intention. For humanoid robotics, NLU bridges the gap between human communication and robot action, making interactions more natural and intuitive."}),"\n",(0,a.jsx)(e.h2,{id:"2-theory",children:"2. Theory"}),"\n",(0,a.jsx)(e.p,{children:"Natural Language Understanding in robotics involves several key components:"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Intent Recognition"}),': Identifying the underlying purpose or goal of a human command. For example, "Could you please move forward a bit?" and "Go forward" both have the same intent but different expressions.']}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Entity Extraction"}),': Identifying specific objects, locations, or parameters mentioned in a command. For example, in "Pick up the red cup", "red cup" is an entity of type "object" with color and name attributes.']}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Context Awareness"}),': Understanding commands in the context of the current situation, previous interactions, and environmental state. This allows robots to handle pronouns ("it", "that") and implicit references.']}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Semantic Parsing"}),": Converting natural language into structured representations that can be processed by robot control systems. This often involves mapping language to formal action descriptions."]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Ambiguity Resolution"}),": Handling cases where a command could have multiple interpretations by using context, asking clarifying questions, or making reasonable assumptions."]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Dialogue Management"}),": Maintaining coherent conversations and tracking the state of interaction over multiple exchanges."]}),"\n",(0,a.jsx)(e.h2,{id:"3-implementation",children:"3. Implementation"}),"\n",(0,a.jsx)(e.p,{children:"Let's implement a Natural Language Understanding system for our humanoid robot:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# vla_robot_control/vla_robot_control/nlu_processor.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom typing import Dict, List, Optional, Tuple\nimport re\nimport json\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass IntentType(Enum):\n    NAVIGATION = \"navigation\"\n    MANIPULATION = \"manipulation\"\n    INTERACTION = \"interaction\"\n    INFORMATION = \"information\"\n    NONE = \"none\"\n\n@dataclass\nclass Entity:\n    type: str\n    value: str\n    confidence: float = 1.0\n\n@dataclass\nclass IntentResult:\n    intent: IntentType\n    entities: List[Entity]\n    confidence: float\n    raw_command: str\n    structured_action: Dict\n\nclass NLUProcessor(Node):\n    def __init__(self, node):\n        super().__init__('nlu_processor')\n        self.node = node\n\n        # Initialize intent patterns and mappings\n        self.setup_intent_patterns()\n\n        # Setup publisher for structured commands\n        self.command_pub = self.node.create_publisher(\n            String, '/robot/structured_command', 10\n        )\n\n        self.get_logger().info('NLU Processor initialized')\n\n    def setup_intent_patterns(self):\n        \"\"\"\n        Setup patterns for different intents\n        \"\"\"\n        self.intent_patterns = {\n            IntentType.NAVIGATION: [\n                # Movement patterns\n                (r'go\\s+(?P<direction>forward|backward|left|right)\\s*(?P<distance>\\d+\\.?\\d*)?\\s*(meters?|steps?)?', self.parse_navigation),\n                (r'move\\s+(?P<direction>forward|backward|left|right)\\s*(?P<distance>\\d+\\.?\\d*)?\\s*(meters?|steps?)?', self.parse_navigation),\n                (r'come\\s+here', lambda m: {'action': 'move_to', 'target': 'user_location', 'distance': 1.0}),\n                (r'go\\s+to\\s+(?P<location>\\w+)', lambda m: {'action': 'move_to', 'target': m.group('location')}),\n                (r'turn\\s+(?P<direction>left|right)\\s*(?P<angle>\\d+\\.?\\d*)?\\s*degrees?', self.parse_turn),\n            ],\n            IntentType.MANIPULATION: [\n                # Manipulation patterns\n                (r'pick\\s+up\\s+(?P<object>.+)', lambda m: {'action': 'pick_up', 'object': m.group('object')}),\n                (r'grab\\s+(?P<object>.+)', lambda m: {'action': 'pick_up', 'object': m.group('object')}),\n                (r'put\\s+(?P<object>.+)\\s+(?P<action>down|on)', lambda m: {'action': 'place', 'object': m.group('object')}),\n                (r'place\\s+(?P<object>.+)', lambda m: {'action': 'place', 'object': m.group('object')}),\n                (r'open\\s+(?P<object>.+)', lambda m: {'action': 'open', 'object': m.group('object')}),\n                (r'close\\s+(?P<object>.+)', lambda m: {'action': 'close', 'object': m.group('object')}),\n            ],\n            IntentType.INTERACTION: [\n                # Interaction patterns\n                (r'say\\s+(?P<text>.+)', lambda m: {'action': 'speak', 'text': m.group('text')}),\n                (r'speak\\s+(?P<text>.+)', lambda m: {'action': 'speak', 'text': m.group('text')}),\n                (r'hello|hi|hey', lambda m: {'action': 'greet', 'text': 'Hello! How can I help you?'}),\n                (r'what.*name', lambda m: {'action': 'respond', 'text': 'I am a humanoid robot assistant.'}),\n            ],\n            IntentType.INFORMATION: [\n                # Information patterns\n                (r'what.*time', lambda m: {'action': 'tell_time', 'text': 'I can tell you the time'}),\n                (r'what.*weather', lambda m: {'action': 'weather_info', 'text': 'I can provide weather information'}),\n            ]\n        }\n\n        # Entity extraction patterns\n        self.entity_patterns = {\n            'object': r'(red|blue|green|yellow|large|small|big|little)?\\s*(cup|bottle|book|box|ball|toy|phone|tablet|glasses)',\n            'location': r'(kitchen|living room|bedroom|office|table|chair|door|window)',\n            'distance': r'(\\d+\\.?\\d*)\\s*(meters?|steps?|feet)',\n            'direction': r'(forward|backward|left|right|up|down)',\n            'color': r'(red|blue|green|yellow|orange|purple|pink|black|white|gray)',\n        }\n\n    def parse_command(self, command: str) -> IntentResult:\n        \"\"\"\n        Parse a natural language command and return structured intent\n        \"\"\"\n        # Normalize the command\n        normalized_command = self.normalize_command(command.lower())\n\n        # Extract entities first\n        entities = self.extract_entities(command)\n\n        # Match intents\n        best_match = self.match_intent(normalized_command)\n\n        if best_match:\n            intent, structured_action, confidence = best_match\n            return IntentResult(\n                intent=intent,\n                entities=entities,\n                confidence=confidence,\n                raw_command=command,\n                structured_action=structured_action\n            )\n        else:\n            # Use LLM fallback for complex understanding\n            return self.llm_fallback(command, entities)\n\n    def normalize_command(self, command: str) -> str:\n        \"\"\"\n        Normalize command for better pattern matching\n        \"\"\"\n        # Remove common fillers\n        command = re.sub(r'\\b(please|could you|would you|can you|kindly|just)\\b', '', command)\n        command = re.sub(r'\\s+', ' ', command).strip()\n        return command\n\n    def extract_entities(self, command: str) -> List[Entity]:\n        \"\"\"\n        Extract named entities from command\n        \"\"\"\n        entities = []\n\n        for entity_type, pattern in self.entity_patterns.items():\n            matches = re.finditer(pattern, command, re.IGNORECASE)\n            for match in matches:\n                entities.append(Entity(\n                    type=entity_type,\n                    value=match.group(),\n                    confidence=0.9  # High confidence for pattern matches\n                ))\n\n        return entities\n\n    def match_intent(self, command: str) -> Optional[Tuple[IntentType, Dict, float]]:\n        \"\"\"\n        Match command to intents using predefined patterns\n        \"\"\"\n        best_confidence = 0.0\n        best_result = None\n\n        for intent_type, patterns in self.intent_patterns.items():\n            for pattern, action_func in patterns:\n                match = re.search(pattern, command, re.IGNORECASE)\n                if match:\n                    try:\n                        structured_action = action_func(match)\n                        confidence = self.calculate_pattern_confidence(pattern, match)\n\n                        if confidence > best_confidence:\n                            best_confidence = confidence\n                            best_result = (intent_type, structured_action, confidence)\n                    except Exception as e:\n                        self.node.get_logger().warn(f'Error processing pattern: {e}')\n                        continue\n\n        return best_result\n\n    def calculate_pattern_confidence(self, pattern: str, match) -> float:\n        \"\"\"\n        Calculate confidence based on match quality\n        \"\"\"\n        # Simple confidence calculation based on match length\n        match_length = len(match.group(0))\n        pattern_complexity = len(pattern)\n        return min(0.9, 0.5 + (match_length / (match_length + 10)))\n\n    def llm_fallback(self, command: str, entities: List[Entity]) -> IntentResult:\n        \"\"\"\n        Use LLM for complex understanding when pattern matching fails\n        \"\"\"\n        # This would interface with the LLM service we created earlier\n        # For now, return a default response\n        return IntentResult(\n            intent=IntentType.NONE,\n            entities=entities,\n            confidence=0.3,\n            raw_command=command,\n            structured_action={'action': 'unknown', 'command': command}\n        )\n\n    def process_command(self, command_msg: String) -> Optional[IntentResult]:\n        \"\"\"\n        Process a command message and return structured result\n        \"\"\"\n        try:\n            result = self.parse_command(command_msg.data)\n\n            if result.confidence > 0.5:  # Confidence threshold\n                # Publish structured command\n                structured_msg = String()\n                structured_msg.data = json.dumps({\n                    'intent': result.intent.value,\n                    'action': result.structured_action,\n                    'entities': [{'type': e.type, 'value': e.value} for e in result.entities],\n                    'confidence': result.confidence\n                })\n                self.command_pub.publish(structured_msg)\n\n                self.node.get_logger().info(f'Processed command: {result.intent.value} - {result.structured_action}')\n                return result\n            else:\n                self.node.get_logger().warn(f'Low confidence parsing: {result.confidence} for \"{command_msg.data}\"')\n                return None\n\n        except Exception as e:\n            self.node.get_logger().error(f'Error processing command: {e}')\n            return None\n\n# Example of how to integrate with ROS 2\nclass NLUInterface(Node):\n    def __init__(self):\n        super().__init__('nlu_interface')\n\n        # Subscribe to voice commands\n        self.command_sub = self.create_subscription(\n            String, '/vla/command', self.command_callback, 10\n        )\n\n        # Initialize NLU processor\n        self.nlu_processor = NLUProcessor(self)\n\n        # Publisher for high-level commands\n        self.high_level_pub = self.create_publisher(String, '/robot/high_level_command', 10)\n\n        self.get_logger().info('NLU Interface initialized')\n\n    def command_callback(self, msg):\n        \"\"\"\n        Callback for incoming voice commands\n        \"\"\"\n        result = self.nlu_processor.process_command(msg)\n\n        if result and result.confidence > 0.5:\n            # Publish high-level command\n            high_level_msg = String()\n            high_level_msg.data = json.dumps({\n                'action': result.structured_action['action'],\n                'parameters': result.structured_action,\n                'entities': [{'type': e.type, 'value': e.value} for e in result.entities]\n            })\n            self.high_level_pub.publish(high_level_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    nlu_interface = NLUInterface()\n    rclpy.spin(nlu_interface)\n    nlu_interface.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.p,{children:"Create a semantic parser that works with the navigation stack:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# vla_robot_control/vla_robot_control/semantic_parser.py\nimport re\nfrom typing import Dict, Any, List\nfrom dataclasses import dataclass\n\n@dataclass\nclass SemanticFrame:\n    action: str\n    objects: List[Dict[str, Any]]\n    locations: List[Dict[str, Any]]\n    parameters: Dict[str, Any]\n\nclass SemanticParser:\n    def __init__(self):\n        self.spatial_rel_patterns = {\n            'on': ['on', 'atop', 'over'],\n            'in': ['in', 'inside', 'within'],\n            'near': ['near', 'by', 'beside', 'next to'],\n            'to': ['to', 'toward', 'towards'],\n            'from': ['from', 'away from']\n        }\n\n    def parse(self, command: str) -> SemanticFrame:\n        \"\"\"\n        Parse a command into semantic components\n        \"\"\"\n        # Extract action\n        action = self.extract_action(command)\n\n        # Extract objects with attributes\n        objects = self.extract_objects(command)\n\n        # Extract locations\n        locations = self.extract_locations(command)\n\n        # Extract parameters\n        parameters = self.extract_parameters(command)\n\n        return SemanticFrame(\n            action=action,\n            objects=objects,\n            locations=locations,\n            parameters=parameters\n        )\n\n    def extract_action(self, command: str) -> str:\n        \"\"\"\n        Extract the main action from the command\n        \"\"\"\n        action_patterns = [\n            (r'pick up|grab|take', 'pick_up'),\n            (r'put down|place|set', 'place'),\n            (r'go to|move to|navigate to', 'navigate'),\n            (r'go forward|move forward', 'move_forward'),\n            (r'turn (left|right)', 'turn'),\n            (r'open', 'open'),\n            (r'close', 'close'),\n            (r'say|speak', 'speak')\n        ]\n\n        for pattern, action in action_patterns:\n            if re.search(pattern, command, re.IGNORECASE):\n                return action\n\n        return 'unknown'\n\n    def extract_objects(self, command: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Extract objects with their attributes from the command\n        \"\"\"\n        objects = []\n\n        # Pattern for objects with attributes (e.g., \"red cup\", \"large box\")\n        object_pattern = r'((?:red|blue|green|yellow|large|small|big|little|wooden|metal|plastic)\\s+)?(\\w+)'\n\n        matches = re.finditer(object_pattern, command, re.IGNORECASE)\n        for match in matches:\n            attributes = match.group(1) or ''\n            obj_name = match.group(2)\n\n            obj = {\n                'name': obj_name,\n                'color': attributes.split()[0] if attributes and attributes.split()[0] in ['red', 'blue', 'green', 'yellow'] else None,\n                'size': attributes.split()[0] if attributes and attributes.split()[0] in ['large', 'small', 'big', 'little'] else None,\n                'material': attributes.split()[0] if attributes and attributes.split()[0] in ['wooden', 'metal', 'plastic'] else None\n            }\n\n            objects.append(obj)\n\n        return objects\n\n    def extract_locations(self, command: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Extract locations from the command\n        \"\"\"\n        locations = []\n\n        location_patterns = [\n            r'(kitchen|living room|bedroom|office|table|chair|door|window|couch|shelf|counter)',\n            r'to the (\\w+)',\n            r'at the (\\w+)'\n        ]\n\n        for pattern in location_patterns:\n            matches = re.finditer(pattern, command, re.IGNORECASE)\n            for match in matches:\n                locations.append({'name': match.group(1) if match.lastindex else match.group(0)})\n\n        return locations\n\n    def extract_parameters(self, command: str) -> Dict[str, Any]:\n        \"\"\"\n        Extract numerical and other parameters\n        \"\"\"\n        params = {}\n\n        # Distance extraction\n        distance_match = re.search(r'(\\d+\\.?\\d*)\\s*(meters?|steps?|feet)', command, re.IGNORECASE)\n        if distance_match:\n            params['distance'] = float(distance_match.group(1))\n            params['unit'] = distance_match.group(2)\n\n        # Angle extraction\n        angle_match = re.search(r'(\\d+)\\s*degrees?', command, re.IGNORECASE)\n        if angle_match:\n            params['angle'] = int(angle_match.group(1))\n\n        return params\n"})}),"\n",(0,a.jsx)(e.h2,{id:"4-hardwaregpu-notes",children:"4. Hardware/GPU Notes"}),"\n",(0,a.jsx)(e.p,{children:"Natural Language Understanding for robotics has specific hardware considerations:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"CPU Requirements"}),": Multi-core processor for real-time parsing (4+ cores recommended)"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Memory"}),": 2-4GB RAM for NLU models and pattern matching"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Storage"}),": Fast storage for pattern databases and semantic models"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Network"}),": Required for cloud-based NLU services (optional)"]}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Performance Optimization"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Use pattern-based matching for common commands (faster, more reliable)"}),"\n",(0,a.jsx)(e.li,{children:"Implement caching for frequently used command interpretations"}),"\n",(0,a.jsx)(e.li,{children:"Consider hybrid approach: local patterns + cloud LLM for complex commands"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Resource Constraints"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"On embedded systems, prioritize simple pattern matching"}),"\n",(0,a.jsx)(e.li,{children:"Implement progressive enhancement (basic \u2192 advanced understanding)"}),"\n",(0,a.jsx)(e.li,{children:"Use lightweight models for edge deployment"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"5-simulation-path",children:"5. Simulation Path"}),"\n",(0,a.jsx)(e.p,{children:"To implement NLU in simulation:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Command Simulation"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Test with various command formats\nros2 topic pub /vla/command std_msgs/String \"data: 'move forward 2 meters'\"\nros2 topic pub /vla/command std_msgs/String \"data: 'pick up the red cup'\"\n"})}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"NLU Testing Framework"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Test NLU parsing with various inputs\nimport unittest\nfrom nlu_processor import NLUProcessor\n\nclass TestNLUProcessor(unittest.TestCase):\n    def setUp(self):\n        self.nlu = NLUProcessor(None)  # Pass None for testing\n\n    def test_navigation_parsing(self):\n        result = self.nlu.parse_command("move forward 2 meters")\n        self.assertEqual(result.intent.value, "navigation")\n        self.assertIn("distance", result.structured_action)\n        self.assertEqual(result.structured_action["distance"], 2.0)\n\n    def test_manipulation_parsing(self):\n        result = self.nlu.parse_command("pick up the blue bottle")\n        self.assertEqual(result.intent.value, "manipulation")\n        self.assertEqual(result.structured_action["action"], "pick_up")\n'})}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Integration Testing"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Test command parsing with simulated environment"}),"\n",(0,a.jsx)(e.li,{children:"Validate entity extraction accuracy"}),"\n",(0,a.jsx)(e.li,{children:"Verify intent recognition rates"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"6-real-world-path",children:"6. Real-World Path"}),"\n",(0,a.jsx)(e.p,{children:"For real-world deployment:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Training and Calibration"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Train on domain-specific language patterns"}),"\n",(0,a.jsx)(e.li,{children:"Calibrate confidence thresholds based on accuracy"}),"\n",(0,a.jsx)(e.li,{children:"Collect real-world usage data for improvement"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Robustness Considerations"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Handle speech recognition errors gracefully"}),"\n",(0,a.jsx)(e.li,{children:"Implement confirmation for critical commands"}),"\n",(0,a.jsx)(e.li,{children:"Provide feedback when commands are ambiguous"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"User Experience"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Implement natural conversation flows"}),"\n",(0,a.jsx)(e.li,{children:"Add confirmation requests for complex commands"}),"\n",(0,a.jsx)(e.li,{children:"Provide error recovery mechanisms"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Privacy and Security"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Process sensitive commands locally when possible"}),"\n",(0,a.jsx)(e.li,{children:"Implement command filtering for security"}),"\n",(0,a.jsx)(e.li,{children:"Ensure compliance with privacy regulations"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"7-spec-build-test-checklist",children:"7. Spec-Build-Test checklist"}),"\n",(0,a.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","NLU processor implemented and integrated"]}),"\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Intent recognition working for common commands"]}),"\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Entity extraction functioning correctly"]}),"\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Semantic parsing converting commands to structured actions"]}),"\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Confidence scoring implemented and validated"]}),"\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Ambiguity handling with fallback mechanisms"]}),"\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Performance benchmarks established (100ms processing)"]}),"\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Accuracy tested with various command formulations"]}),"\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Error handling for unrecognized commands"]}),"\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Integration testing with voice input and robot control"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"8-apa-citations",children:"8. APA citations"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["Jurafsky, D., & Martin, J. H. (2020). ",(0,a.jsx)(e.em,{children:"Speech and language processing"})," (3rd ed.). Pearson."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["Young, S., Ga\u0161i\u0107, M., Thomson, B., & Williams, J. D. (2013). POMDP-based statistical spoken dialog systems: A review. ",(0,a.jsx)(e.em,{children:"Proceedings of the IEEE"}),", 101(5), 1160-1179."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["Henderson, M., Thomson, B., & Young, S. (2014). Word-based dialog state tracking with recurrent neural networks. ",(0,a.jsx)(e.em,{children:"Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue"}),", 292-301."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["Vlachos, A., & Rieser, V. (2014). Learning to map sentences with structured representations. ",(0,a.jsx)(e.em,{children:"Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)"}),", 1321-1331."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["Wen, T. H., Gasic, M., Mrk\u0161i\u0107, N., Su, P. H., Vandyke, D., & Young, S. (2016). Sequential neural networks as automata. ",(0,a.jsx)(e.em,{children:"Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers"}),", 65-70."]}),"\n"]}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}}}]);