"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[7954],{1484:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-3-isaac/chapter-6","title":"Chapter 20: Navigation with Isaac","description":"Advanced navigation systems using NVIDIA Isaac for humanoid robotics applications","source":"@site/docs/module-3-isaac/chapter-6.md","sourceDirName":"module-3-isaac","slug":"/module-3-isaac/chapter-6","permalink":"/hackathon-book-robotics/fr/docs/module-3-isaac/chapter-6","draft":false,"unlisted":false,"editUrl":"https://github.com/noormusarrat/hackathon-book-robotics/edit/main/docs/module-3-isaac/chapter-6.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"title":"Chapter 20: Navigation with Isaac","description":"Advanced navigation systems using NVIDIA Isaac for humanoid robotics applications"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 19: SLAM Systems with Isaac","permalink":"/hackathon-book-robotics/fr/docs/module-3-isaac/chapter-5"},"next":{"title":"Chapter 21: Advanced AI Control for Bipedal Robots","permalink":"/hackathon-book-robotics/fr/docs/module-3-isaac/chapter-7"}}');var s=a(4848),t=a(8453);const o={sidebar_position:6,title:"Chapter 20: Navigation with Isaac",description:"Advanced navigation systems using NVIDIA Isaac for humanoid robotics applications"},r="Chapter 20: Navigation with Isaac",l={},c=[{value:"1. Why this concept matters for humanoids",id:"1-why-this-concept-matters-for-humanoids",level:2},{value:"2. Theory",id:"2-theory",level:2},{value:"Isaac Navigation Architecture",id:"isaac-navigation-architecture",level:3},{value:"Navigation2 Integration with Isaac",id:"navigation2-integration-with-isaac",level:3},{value:"Path Planning Algorithms in Isaac",id:"path-planning-algorithms-in-isaac",level:3},{value:"Humanoid-Specific Navigation Considerations",id:"humanoid-specific-navigation-considerations",level:3},{value:"3. Implementation",id:"3-implementation",level:2},{value:"4. Hardware/GPU Notes",id:"4-hardwaregpu-notes",level:2},{value:"Isaac Navigation GPU Requirements",id:"isaac-navigation-gpu-requirements",level:3},{value:"Memory Management Strategies",id:"memory-management-strategies",level:3},{value:"Jetson Platform Considerations",id:"jetson-platform-considerations",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"5. Simulation Path",id:"5-simulation-path",level:2},{value:"6. Real-World Path",id:"6-real-world-path",level:2},{value:"7. Spec-Build-Test checklist",id:"7-spec-build-test-checklist",level:2},{value:"8. APA citations",id:"8-apa-citations",level:2}];function m(n){const e={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"chapter-20-navigation-with-isaac",children:"Chapter 20: Navigation with Isaac"})}),"\n",(0,s.jsx)(e.h2,{id:"1-why-this-concept-matters-for-humanoids",children:"1. Why this concept matters for humanoids"}),"\n",(0,s.jsx)(e.p,{children:"Navigation is the cornerstone of autonomous humanoid robotics, enabling robots to move safely and efficiently through complex human environments to perform useful tasks. For humanoid robots specifically, navigation systems must handle the unique challenges of bipedal locomotion, including complex dynamics, variable terrain, narrow passages, and the need to navigate in close proximity to humans. Isaac's navigation capabilities provide hardware-accelerated processing that allows humanoid robots to plan and execute safe paths through dynamic environments while considering their complex kinematic constraints. This capability is essential for humanoid robots to perform tasks like autonomous delivery in homes and offices, assistance for elderly or disabled individuals, and exploration of unknown environments. Without robust navigation systems, humanoid robots would be limited to static tasks or require constant human guidance, severely limiting their autonomy and utility in real-world applications."}),"\n",(0,s.jsx)(e.h2,{id:"2-theory",children:"2. Theory"}),"\n",(0,s.jsx)(e.h3,{id:"isaac-navigation-architecture",children:"Isaac Navigation Architecture"}),"\n",(0,s.jsx)(e.p,{children:"Isaac's navigation system is built on the Navigation2 framework but optimized for Isaac's hardware acceleration and humanoid-specific requirements. The architecture consists of several interconnected components:"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Perception Layer"}),": Processes sensor data to understand the environment, including obstacle detection, semantic mapping, and dynamic object tracking. This layer leverages Isaac's GPU-accelerated perception capabilities."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Mapping Layer"}),": Maintains representation of the environment including static obstacles, dynamic obstacles, and traversable areas. This layer integrates with Isaac's SLAM systems for real-time map updates."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Path Planning Layer"}),": Generates optimal paths from the current location to the goal while considering robot kinematics, obstacle avoidance, and safety constraints. This includes both global path planning and local trajectory optimization."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Control Layer"}),": Translates planned paths into low-level commands for the humanoid robot's locomotion system, accounting for bipedal dynamics and balance requirements."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Behavior Layer"}),": Manages navigation behaviors such as obstacle avoidance, recovery, and human-aware navigation patterns."]}),"\n",(0,s.jsx)(e.h3,{id:"navigation2-integration-with-isaac",children:"Navigation2 Integration with Isaac"}),"\n",(0,s.jsx)(e.p,{children:"Isaac extends Navigation2 with hardware acceleration and humanoid-specific capabilities:"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Isaac ROS Navigation"}),": Provides hardware-accelerated implementations of Navigation2 plugins, including costmap processing, path planners, and controllers."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Humanoid-Specific Plugins"}),": Custom plugins that account for humanoid robot kinematics, dynamics, and safety requirements."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"GPU-Accelerated Costmap"}),": Hardware-accelerated costmap processing for real-time obstacle avoidance and path planning."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Isaac Navigation Apps"}),": Pre-built navigation applications optimized for specific humanoid robot platforms."]}),"\n",(0,s.jsx)(e.h3,{id:"path-planning-algorithms-in-isaac",children:"Path Planning Algorithms in Isaac"}),"\n",(0,s.jsx)(e.p,{children:"Isaac implements several advanced path planning algorithms optimized for humanoid robots:"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Global Planners"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsxs)(e.em,{children:[(0,s.jsx)(e.em,{children:"A"})," and Dijkstra"]}),"*: Optimal path planning for static environments"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsxs)(e.em,{children:[(0,s.jsx)(e.em,{children:"Theta"})," and Any-angle"]}),"*: Path planning that allows for any-angle movements"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Human-aware Planning"}),": Path planning that considers human comfort zones and social navigation"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Local Planners"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Trajectory Rollout"}),": Local trajectory optimization for dynamic obstacle avoidance"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Dynamic Window Approach"}),": Velocity-based local planning for real-time obstacle avoidance"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"MPC (Model Predictive Control)"}),": Advanced control for complex humanoid dynamics"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"humanoid-specific-navigation-considerations",children:"Humanoid-Specific Navigation Considerations"}),"\n",(0,s.jsx)(e.p,{children:"Navigation for humanoid robots must account for unique constraints:"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Kinematic Constraints"}),": Bipedal robots have complex kinematic chains that affect turning radius, step size, and movement capabilities."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Dynamic Stability"}),": Humanoid robots must maintain balance during navigation, requiring careful consideration of center of mass and foot placement."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Step Planning"}),": For walking robots, navigation must include detailed footstep planning for stable locomotion."]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Social Navigation"}),": Humanoid robots must navigate in ways that are comfortable and safe for humans in shared spaces."]}),"\n",(0,s.jsx)(e.h2,{id:"3-implementation",children:"3. Implementation"}),"\n",(0,s.jsx)(e.p,{children:"Let's implement comprehensive Isaac navigation systems for humanoid robotics:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# isaac_humanoid_navigation/isaac_humanoid_navigation/navigation_manager.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, PointCloud2, Image, CameraInfo\nfrom nav_msgs.msg import Odometry, OccupancyGrid, Path, MapMetaData\nfrom geometry_msgs.msg import PoseStamped, PoseWithCovarianceStamped, Twist, Point\nfrom geometry_msgs.msg import PoseArray, PointStamped\nfrom visualization_msgs.msg import MarkerArray\nfrom std_msgs.msg import Header, Bool, String, Float32\nfrom builtin_interfaces.msg import Duration\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport threading\nfrom typing import Dict, Any, Optional, List, Tuple\nimport time\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport math\nimport tf2_ros\nfrom tf2_ros import TransformListener\nfrom tf2_geometry_msgs import do_transform_pose\nimport tf_transformations\n\nclass NavigationState(Enum):\n    """Navigation states for humanoid robots"""\n    IDLE = "idle"\n    PLANNING = "planning"\n    EXECUTING = "executing"\n    RECOVERY = "recovery"\n    PAUSED = "paused"\n    STOPPED = "stopped"\n    SUCCEEDED = "succeeded"\n    FAILED = "failed"\n\nclass NavigationMode(Enum):\n    """Navigation modes for different scenarios"""\n    BASIC = "basic"\n    HUMAN_AWARE = "human_aware"\n    DYNAMIC = "dynamic"\n    SOCIAL = "social"\n\n@dataclass\nclass NavigationGoal:\n    """Data structure for navigation goals"""\n    pose: PoseStamped\n    behavior: str  # "normal", "urgent", "cautious"\n    priority: int  # 1-10 scale\n    timeout: float  # seconds\n    constraints: Dict[str, Any]  # kinematic constraints\n\n@dataclass\nclass NavigationResult:\n    """Data structure for navigation results"""\n    success: bool\n    final_pose: PoseStamped\n    path_executed: Path\n    execution_time: float\n    distance_traveled: float\n    collisions: int\n    replanning_count: int\n\nclass IsaacNavigationManager(Node):\n    """\n    Isaac navigation manager for humanoid robotics\n    """\n    def __init__(self):\n        super().__init__(\'isaac_navigation_manager\')\n\n        # Initialize components\n        self.bridge = CvBridge()\n        self.nav_lock = threading.Lock()\n        self.navigation_state = NavigationState.IDLE\n        self.navigation_mode = NavigationMode.HUMAN_AWARE\n        self.tf_buffer = tf2_ros.Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Navigation parameters\n        self.max_linear_speed = 0.5  # m/s\n        self.max_angular_speed = 0.5  # rad/s\n        self.min_distance_to_obstacle = 0.5  # m\n        self.planning_frequency = 10.0  # Hz\n        self.controller_frequency = 20.0  # Hz\n        self.recovery_enabled = True\n        self.human_detection_enabled = True\n\n        # Navigation state\n        self.current_goal: Optional[NavigationGoal] = None\n        self.current_path: Optional[Path] = None\n        self.current_pose: Optional[PoseStamped] = None\n        self.velocity_cmd: Optional[Twist] = None\n        self.path_index = 0\n\n        # Sensor data storage\n        self.latest_scan = None\n        self.latest_odom = None\n        self.latest_map = None\n        self.humans_detected = []\n\n        # Publishers for navigation\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.global_plan_pub = self.create_publisher(Path, \'/navigation/global_plan\', 10)\n        self.local_plan_pub = self.create_publisher(Path, \'/navigation/local_plan\', 10)\n        self.velocity_pub = self.create_publisher(Twist, \'/navigation/velocity\', 10)\n        self.status_pub = self.create_publisher(String, \'/navigation/status\', 10)\n        self.feedback_pub = self.create_publisher(String, \'/navigation/feedback\', 10)\n        self.collision_pub = self.create_publisher(Bool, \'/navigation/collision_warning\', 10)\n\n        # Subscribers for navigation\n        self.odom_sub = self.create_subscription(\n            Odometry, \'/odom\', self.odom_callback, 10\n        )\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10\n        )\n        self.map_sub = self.create_subscription(\n            OccupancyGrid, \'/map\', self.map_callback, 10\n        )\n        self.initial_pose_sub = self.create_subscription(\n            PoseWithCovarianceStamped, \'/initialpose\', self.initial_pose_callback, 10\n        )\n\n        # Action server for navigation goals\n        self.goal_sub = self.create_subscription(\n            PoseStamped, \'/navigation/goal\', self.goal_callback, 10\n        )\n\n        # Timer for navigation execution\n        self.navigation_timer = self.create_timer(1.0/self.controller_frequency, self.execute_navigation)\n\n        # Initialize navigation components\n        self.initialize_navigation_components()\n\n        self.get_logger().info(\'Isaac Navigation Manager initialized\')\n\n    def initialize_navigation_components(self):\n        """Initialize navigation components"""\n        self.get_logger().info(\'Initializing navigation components...\')\n\n        # Initialize path planner\n        self.initialize_path_planner()\n\n        # Initialize local controller\n        self.initialize_local_controller()\n\n        # Initialize collision detection\n        self.initialize_collision_detection()\n\n        # Initialize recovery behaviors\n        self.initialize_recovery_behaviors()\n\n        self.get_logger().info(\'Navigation components initialized\')\n\n    def initialize_path_planner(self):\n        """Initialize path planning components"""\n        self.get_logger().info(\'Initializing path planner...\')\n        # In a real implementation, this would initialize Isaac\'s path planner\n        # For example: self.path_planner = IsaacPathPlanner()\n\n    def initialize_local_controller(self):\n        """Initialize local trajectory controller"""\n        self.get_logger().info(\'Initializing local controller...\')\n        # In a real implementation, this would initialize local controller\n        # For example: self.local_controller = IsaacLocalController()\n\n    def initialize_collision_detection(self):\n        """Initialize collision detection system"""\n        self.get_logger().info(\'Initializing collision detection...\')\n        # In a real implementation, this would initialize collision detection\n        # For example: self.collision_detector = IsaacCollisionDetector()\n\n    def initialize_recovery_behaviors(self):\n        """Initialize recovery behaviors"""\n        self.get_logger().info(\'Initializing recovery behaviors...\')\n        # In a real implementation, this would initialize recovery behaviors\n        # For example: self.recovery_behaviors = IsaacRecoveryBehaviors()\n\n    def odom_callback(self, msg):\n        """Process odometry data"""\n        with self.nav_lock:\n            self.latest_odom = msg\n\n            # Update current pose\n            pose_stamped = PoseStamped()\n            pose_stamped.header = msg.header\n            pose_stamped.pose = msg.pose.pose\n            self.current_pose = pose_stamped\n\n    def scan_callback(self, msg):\n        """Process laser scan data"""\n        with self.nav_lock:\n            self.latest_scan = msg\n\n            # Process scan for obstacles and humans if enabled\n            if self.human_detection_enabled:\n                self.detect_humans_in_scan(msg)\n\n    def map_callback(self, msg):\n        """Process map data"""\n        with self.nav_lock:\n            self.latest_map = msg\n\n    def initial_pose_callback(self, msg):\n        """Process initial pose estimate"""\n        with self.nav_lock:\n            # Update initial pose\n            pose_stamped = PoseStamped()\n            pose_stamped.header = msg.header\n            pose_stamped.pose = msg.pose.pose\n            self.current_pose = pose_stamped\n\n    def goal_callback(self, msg):\n        """Process navigation goal"""\n        with self.nav_lock:\n            goal = NavigationGoal(\n                pose=msg,\n                behavior="normal",\n                priority=5,\n                timeout=60.0,\n                constraints={}\n            )\n\n            self.current_goal = goal\n            self.navigation_state = NavigationState.PLANNING\n\n            self.get_logger().info(f\'New navigation goal received: {msg.pose.position.x}, {msg.pose.position.y}\')\n\n            # Plan path to goal\n            success = self.plan_path_to_goal(goal)\n\n            if success:\n                self.navigation_state = NavigationState.EXECUTING\n                self.publish_status("EXECUTING")\n            else:\n                self.navigation_state = NavigationState.FAILED\n                self.publish_status("FAILED")\n\n    def detect_humans_in_scan(self, scan_msg):\n        """Detect humans in laser scan data (simplified approach)"""\n        # In a real implementation, this would use Isaac\'s human detection\n        # For this example, we\'ll use a simple clustering approach\n        ranges = scan_msg.ranges\n        angles = [scan_msg.angle_min + i * scan_msg.angle_increment for i in range(len(ranges))]\n\n        clusters = []\n        current_cluster = []\n\n        for i, r in enumerate(ranges):\n            if not np.isnan(r) and scan_msg.range_min < r < scan_msg.range_max * 0.8:\n                x = r * math.cos(angles[i])\n                y = r * math.sin(angles[i])\n\n                if current_cluster and self.distance_to_point(x, y, current_cluster[-1][0], current_cluster[-1][1]) > 0.5:\n                    if len(current_cluster) >= 3:  # Minimum cluster size\n                        clusters.append(current_cluster)\n                    current_cluster = [(x, y)]\n                else:\n                    current_cluster.append((x, y))\n\n        if current_cluster and len(current_cluster) >= 3:\n            clusters.append(current_cluster)\n\n        # Process clusters (simplified human detection)\n        humans = []\n        for cluster in clusters:\n            if len(cluster) >= 5:  # Likely a human-sized object\n                center_x = sum(p[0] for p in cluster) / len(cluster)\n                center_y = sum(p[1] for p in cluster) / len(cluster)\n                humans.append((center_x, center_y))\n\n        self.humans_detected = humans\n\n    def distance_to_point(self, x1, y1, x2, y2):\n        """Calculate Euclidean distance between two points"""\n        return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n\n    def plan_path_to_goal(self, goal: NavigationGoal):\n        """Plan path to navigation goal using Isaac\'s optimized algorithms"""\n        # In a real implementation, this would use Isaac ROS navigation planners\n        # For this example, we\'ll implement a simplified path planner\n\n        if not self.current_pose:\n            self.get_logger().warn(\'No current pose available for path planning\')\n            return False\n\n        # Create a simple path (in a real implementation, this would use A*, Dijkstra, etc.)\n        path = Path()\n        path.header.frame_id = "map"\n        path.header.stamp = self.get_clock().now().to_msg()\n\n        # Calculate path points (simplified - straight line with intermediate points)\n        start_x = self.current_pose.pose.position.x\n        start_y = self.current_pose.pose.position.y\n        goal_x = goal.pose.pose.position.x\n        goal_y = goal.pose.position.y\n\n        # Calculate distance and intermediate points\n        distance = math.sqrt((goal_x - start_x)**2 + (goal_y - start_y)**2)\n        num_points = max(2, int(distance / 0.5))  # 0.5m spacing\n\n        for i in range(num_points + 1):\n            t = i / num_points if num_points > 0 else 0\n            x = start_x + t * (goal_x - start_x)\n            y = start_y + t * (goal_y - start_y)\n\n            pose_stamped = PoseStamped()\n            pose_stamped.header.frame_id = "map"\n            pose_stamped.header.stamp = self.get_clock().now().to_msg()\n            pose_stamped.pose.position.x = x\n            pose_stamped.pose.position.y = y\n            pose_stamped.pose.position.z = 0.0\n\n            # Simple orientation toward goal\n            angle = math.atan2(goal_y - y, goal_x - x)\n            quat = tf_transformations.quaternion_from_euler(0, 0, angle)\n            pose_stamped.pose.orientation.x = quat[0]\n            pose_stamped.pose.orientation.y = quat[1]\n            pose_stamped.pose.orientation.z = quat[2]\n            pose_stamped.pose.orientation.w = quat[3]\n\n            path.poses.append(pose_stamped)\n\n        self.current_path = path\n        self.path_index = 0\n\n        # Publish global plan\n        self.global_plan_pub.publish(path)\n\n        return True\n\n    def execute_navigation(self):\n        """Main navigation execution loop"""\n        with self.nav_lock:\n            if self.navigation_state != NavigationState.EXECUTING:\n                # Publish zero velocity when not executing\n                if self.navigation_state in [NavigationState.IDLE, NavigationState.STOPPED]:\n                    zero_twist = Twist()\n                    self.cmd_vel_pub.publish(zero_twist)\n                return\n\n            if not self.current_pose or not self.current_path:\n                self.get_logger().warn(\'No pose or path for navigation execution\')\n                return\n\n            # Check for collisions\n            collision_imminent = self.check_collision_risk()\n            if collision_imminent:\n                self.handle_collision_risk()\n                return\n\n            # Calculate next velocity command\n            velocity_cmd = self.calculate_velocity_command()\n            if velocity_cmd:\n                self.cmd_vel_pub.publish(velocity_cmd)\n                self.velocity_pub.publish(velocity_cmd)\n\n            # Check if goal reached\n            if self.check_goal_reached():\n                self.navigation_state = NavigationState.SUCCEEDED\n                self.publish_status("SUCCEEDED")\n                self.publish_feedback("Goal reached successfully")\n\n    def check_collision_risk(self):\n        """Check for imminent collision based on scan data"""\n        if not self.latest_scan:\n            return False\n\n        # Check if any scan points are within minimum safe distance\n        for r in self.latest_scan.ranges:\n            if not np.isnan(r) and self.latest_scan.range_min < r < self.min_distance_to_obstacle:\n                return True\n\n        return False\n\n    def handle_collision_risk(self):\n        """Handle imminent collision situation"""\n        self.get_logger().warn(\'Collision risk detected, stopping robot\')\n\n        # Publish stop command\n        stop_twist = Twist()\n        self.cmd_vel_pub.publish(stop_twist)\n\n        # Publish collision warning\n        collision_msg = Bool()\n        collision_msg.data = True\n        self.collision_pub.publish(collision_msg)\n\n        # Change state to recovery if enabled\n        if self.recovery_enabled:\n            self.navigation_state = NavigationState.RECOVERY\n            self.publish_status("RECOVERY")\n            self.attempt_recovery()\n        else:\n            self.navigation_state = NavigationState.STOPPED\n            self.publish_status("STOPPED")\n\n    def attempt_recovery(self):\n        """Attempt navigation recovery behaviors"""\n        self.get_logger().info(\'Attempting recovery behavior\')\n\n        # For this example, we\'ll try a simple backup and turn\n        recovery_twist = Twist()\n        recovery_twist.linear.x = -0.2  # Backup\n        recovery_twist.angular.z = 0.5  # Turn while backing up\n\n        # Publish recovery command for 2 seconds\n        for _ in range(int(2.0 * self.controller_frequency)):\n            self.cmd_vel_pub.publish(recovery_twist)\n            time.sleep(1.0/self.controller_frequency)\n\n        # Stop after recovery\n        stop_twist = Twist()\n        self.cmd_vel_pub.publish(stop_twist)\n\n        # Try replanning\n        if self.current_goal:\n            success = self.plan_path_to_goal(self.current_goal)\n            if success:\n                self.navigation_state = NavigationState.EXECUTING\n                self.publish_status("EXECUTING")\n\n    def calculate_velocity_command(self):\n        """Calculate velocity command based on current path"""\n        if not self.current_path or self.path_index >= len(self.current_path.poses):\n            return None\n\n        # Get current target pose from path\n        target_pose = self.current_path.poses[self.path_index]\n\n        # Transform target to robot frame\n        try:\n            transform = self.tf_buffer.lookup_transform(\n                "base_link", "map",\n                Duration(seconds=0, nanoseconds=0)\n            )\n            transformed_pose = do_transform_pose(target_pose, transform)\n        except Exception as e:\n            self.get_logger().warn(f\'Could not transform pose: {e}\')\n            return None\n\n        # Simple proportional controller\n        dx = transformed_pose.pose.position.x\n        dy = transformed_pose.pose.position.y\n        target_angle = math.atan2(dy, dx)\n\n        # Calculate linear and angular velocities\n        linear_vel = min(self.max_linear_speed, math.sqrt(dx**2 + dy**2) * 0.5)\n        angular_vel = min(self.max_angular_speed, target_angle * 2.0)\n\n        # Check if we\'ve reached this waypoint\n        distance_to_waypoint = math.sqrt(dx**2 + dy**2)\n        if distance_to_waypoint < 0.3:  # Within 30cm of waypoint\n            self.path_index += 1\n            if self.path_index >= len(self.current_path.poses):\n                # Reached end of path\n                self.publish_feedback("Reached goal")\n                return None\n\n        # Create velocity command\n        twist = Twist()\n        twist.linear.x = linear_vel\n        twist.angular.z = angular_vel\n\n        return twist\n\n    def check_goal_reached(self):\n        """Check if navigation goal has been reached"""\n        if not self.current_pose or not self.current_goal:\n            return False\n\n        # Calculate distance to goal\n        dx = self.current_pose.pose.position.x - self.current_goal.pose.pose.position.x\n        dy = self.current_pose.pose.position.y - self.current_goal.pose.pose.position.y\n        distance = math.sqrt(dx**2 + dy**2)\n\n        # Check if within goal tolerance\n        goal_tolerance = 0.5  # meters\n        return distance < goal_tolerance\n\n    def publish_status(self, status_str):\n        """Publish navigation status"""\n        status_msg = String()\n        status_msg.data = status_str\n        self.status_pub.publish(status_msg)\n\n    def publish_feedback(self, feedback_str):\n        """Publish navigation feedback"""\n        feedback_msg = String()\n        feedback_msg.data = feedback_str\n        self.feedback_pub.publish(feedback_msg)\n\n    def set_navigation_mode(self, mode: NavigationMode):\n        """Set the current navigation mode"""\n        with self.nav_lock:\n            self.navigation_mode = mode\n            self.get_logger().info(f\'Switched to navigation mode: {mode.value}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacNavigationManager()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Shutting down Isaac Navigation Manager\')\n    finally:\n        # Stop the robot before shutting down\n        stop_twist = Twist()\n        node.cmd_vel_pub.publish(stop_twist)\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(e.p,{children:"Create the navigation configuration:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-yaml",children:'# isaac_humanoid_navigation/config/navigation_config.yaml\nisaac_navigation_manager:\n  ros__parameters:\n    # Navigation mode\n    navigation_mode: "human_aware"\n\n    # Robot parameters\n    robot:\n      base_frame: "base_link"\n      odom_frame: "odom"\n      map_frame: "map"\n      footprint_radius: 0.3  # meters\n      max_linear_speed: 0.5\n      max_angular_speed: 0.5\n      min_linear_speed: 0.1\n      min_angular_speed: 0.1\n\n    # Global planner parameters\n    global_planner:\n      planner_frequency: 1.0  # Hz\n      plan_resolution: 0.05  # meters per cell\n      costmap_topic: "/global_costmap/costmap"\n      use_dijkstra: true\n      use_grid_path: false\n      allow_unknown: false\n\n    # Local planner parameters\n    local_planner:\n      controller_frequency: 20.0  # Hz\n      max_vel_x: 0.5\n      min_vel_x: 0.1\n      max_vel_theta: 0.5\n      min_vel_theta: 0.1\n      acc_lim_x: 2.5\n      acc_lim_theta: 3.2\n      xy_goal_tolerance: 0.3\n      yaw_goal_tolerance: 0.1\n      holonomic_robot: false\n\n    # Costmap parameters\n    costmap:\n      obstacle_range: 3.0\n      raytrace_range: 4.0\n      inflation_radius: 0.55\n      cost_scaling_factor: 10.0\n      lethal_cost_threshold: 100\n      unknown_cost_value: -1\n      transform_tolerance: 0.3\n\n    # Human-aware navigation parameters\n    human_aware:\n      enable_human_detection: true\n      personal_space_radius: 1.0  # meters\n      social_space_radius: 2.0    # meters\n      public_space_radius: 4.0    # meters\n      human_following_enabled: false\n      human_avoidance_gain: 2.0\n\n    # Recovery behaviors\n    recovery:\n      enable_recovery: true\n      recovery_behavior_enabled: true\n      clearing_rotation_allowed: true\n      shutdown_costmaps: false\n      conservative_reset_dist: 3.0\n\n    # Safety parameters\n    safety:\n      min_obstacle_dist: 0.5  # meters\n      collision_check_frequency: 10.0  # Hz\n      emergency_stop_distance: 0.3  # meters\n      max_retries: 3\n\n    # Processing parameters\n    processing:\n      queue_size: 10\n      max_queue_size: 100\n      enable_multithreading: true\n      synchronization_window: 0.1  # seconds\n\n    # GPU acceleration settings (for Isaac-specific components)\n    gpu:\n      device_id: 0\n      memory_fraction: 0.6  # 60% of available GPU memory for navigation\n\n    # Performance monitoring\n    performance:\n      enable_profiling: true\n      publish_statistics: true\n      statistics_topic: "/isaac/navigation/performance"\n      warning_threshold: 0.8  # 80% of target frame rate\n'})}),"\n",(0,s.jsx)(e.p,{children:"Create the launch file for the navigation system:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:"\x3c!-- isaac_humanoid_navigation/launch/isaac_navigation.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    namespace = LaunchConfiguration('namespace')\n    navigation_mode = LaunchConfiguration('navigation_mode')\n\n    return LaunchDescription([\n        # Declare launch arguments\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation (Gazebo) clock if true'\n        ),\n        DeclareLaunchArgument(\n            'namespace',\n            default_value='',\n            description='Robot namespace'\n        ),\n        DeclareLaunchArgument(\n            'navigation_mode',\n            default_value='human_aware',\n            description='Navigation mode: basic, human_aware, dynamic, social'\n        ),\n\n        # Isaac Navigation Manager\n        Node(\n            package='isaac_humanoid_navigation',\n            executable='isaac_navigation_manager',\n            name='isaac_navigation_manager',\n            namespace=namespace,\n            parameters=[\n                os.path.join(\n                    get_package_share_directory('isaac_humanoid_navigation'),\n                    'config',\n                    'navigation_config.yaml'\n                ),\n                {'use_sim_time': use_sim_time}\n            ],\n            output='screen',\n            respawn=True,\n            respawn_delay=2\n        ),\n\n        # Isaac ROS Navigation (Global Planner)\n        Node(\n            package='isaac_ros_navigation',\n            executable='isaac_ros_global_planner',\n            name='global_planner',\n            namespace=namespace,\n            parameters=[\n                {\n                    'use_sim_time': use_sim_time,\n                    'planner_frequency': 1.0,\n                    'plan_resolution': 0.05,\n                    'allow_unknown': False\n                }\n            ],\n            remappings=[\n                ('/global_planner/costmap', '/global_costmap/costmap'),\n                ('/global_planner/plan', '/navigation/global_plan'),\n                ('/global_planner/start', '/initialpose'),\n                ('/global_planner/goal', '/navigation/goal')\n            ],\n            output='screen'\n        ),\n\n        # Isaac ROS Navigation (Local Planner)\n        Node(\n            package='isaac_ros_navigation',\n            executable='isaac_ros_local_planner',\n            name='local_planner',\n            namespace=namespace,\n            parameters=[\n                {\n                    'use_sim_time': use_sim_time,\n                    'controller_frequency': 20.0,\n                    'max_vel_x': 0.5,\n                    'min_vel_x': 0.1,\n                    'max_vel_theta': 0.5,\n                    'min_vel_theta': 0.1\n                }\n            ],\n            remappings=[\n                ('/local_planner/cmd_vel', '/cmd_vel'),\n                ('/local_planner/local_plan', '/navigation/local_plan'),\n                ('/local_planner/global_plan', '/navigation/global_plan'),\n                ('/local_planner/odom', '/odom'),\n                ('/local_planner/scan', '/scan')\n            ],\n            output='screen'\n        ),\n\n        # Isaac ROS Costmap (Global)\n        Node(\n            package='isaac_ros_navigation',\n            executable='isaac_ros_global_costmap',\n            name='global_costmap',\n            namespace=namespace,\n            parameters=[\n                {\n                    'use_sim_time': use_sim_time,\n                    'global_frame': 'map',\n                    'robot_base_frame': 'base_link',\n                    'update_frequency': 5.0,\n                    'publish_frequency': 2.0,\n                    'resolution': 0.05,\n                    'width': 40,\n                    'height': 40,\n                    'origin_x': -20,\n                    'origin_y': -20\n                }\n            ],\n            remappings=[\n                ('/global_costmap/scan', '/scan'),\n                ('/global_costmap/costmap', '/global_costmap/costmap'),\n                ('/global_costmap/costmap_updates', '/global_costmap/costmap_updates')\n            ],\n            output='screen'\n        ),\n\n        # Isaac ROS Costmap (Local)\n        Node(\n            package='isaac_ros_navigation',\n            executable='isaac_ros_local_costmap',\n            name='local_costmap',\n            namespace=namespace,\n            parameters=[\n                {\n                    'use_sim_time': use_sim_time,\n                    'global_frame': 'odom',\n                    'robot_base_frame': 'base_link',\n                    'update_frequency': 10.0,\n                    'publish_frequency': 5.0,\n                    'resolution': 0.05,\n                    'width': 10,\n                    'height': 10,\n                    'origin_x': -5,\n                    'origin_y': -5\n                }\n            ],\n            remappings=[\n                ('/local_costmap/scan', '/scan'),\n                ('/local_costmap/costmap', '/local_costmap/costmap'),\n                ('/local_costmap/costmap_updates', '/local_costmap/costmap_updates')\n            ],\n            output='screen'\n        ),\n\n        # Isaac ROS Human Detection (for human-aware navigation)\n        Node(\n            package='isaac_ros_apriltag',\n            executable='isaac_ros_apriltag',\n            name='human_detector',\n            namespace=namespace,\n            parameters=[\n                {\n                    'family': 'tag36h11',\n                    'size': 0.3,  # 30cm tags for human detection simulation\n                    'max_tags': 20,\n                    'use_sim_time': use_sim_time\n                }\n            ],\n            remappings=[\n                ('image', '/camera/rgb/image_raw'),\n                ('camera_info', '/camera/rgb/camera_info'),\n                ('detections', '/navigation/human_detections')\n            ],\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,s.jsx)(e.p,{children:"Create a navigation safety monitor:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# isaac_humanoid_navigation/isaac_humanoid_navigation/navigation_safety_monitor.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, PointCloud2\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom std_msgs.msg import Bool, Float32\nfrom builtin_interfaces.msg import Duration\nimport numpy as np\nimport threading\nimport math\nfrom typing import List, Tuple\n\nclass IsaacNavigationSafetyMonitor(Node):\n    """\n    Safety monitor for Isaac navigation system\n    """\n    def __init__(self):\n        super().__init__(\'isaac_navigation_safety_monitor\')\n\n        # Initialize safety parameters\n        self.safety_lock = threading.Lock()\n        self.emergency_stop_active = False\n        self.safety_distance = 0.5  # meters\n        self.collision_threshold = 0.3  # meters\n        self.scan_data = None\n        self.velocity_cmd = None\n\n        # Publishers for safety\n        self.emergency_stop_pub = self.create_publisher(Bool, \'/navigation/emergency_stop\', 10)\n        self.safe_distance_pub = self.create_publisher(Float32, \'/navigation/safe_distance\', 10)\n        self.collision_warning_pub = self.create_publisher(Bool, \'/navigation/collision_warning\', 10)\n\n        # Subscribers for safety monitoring\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10\n        )\n        self.cmd_vel_sub = self.create_subscription(\n            Twist, \'/cmd_vel\', self.cmd_vel_callback, 10\n        )\n\n        # Timer for safety monitoring\n        self.safety_timer = self.create_timer(0.1, self.check_safety)\n\n        self.get_logger().info(\'Isaac Navigation Safety Monitor initialized\')\n\n    def scan_callback(self, msg):\n        """Process laser scan for safety monitoring"""\n        with self.safety_lock:\n            self.scan_data = msg\n\n    def cmd_vel_callback(self, msg):\n        """Monitor velocity commands"""\n        with self.safety_lock:\n            self.velocity_cmd = msg\n\n    def check_safety(self):\n        """Main safety monitoring loop"""\n        with self.safety_lock:\n            if not self.scan_data:\n                return\n\n            # Check for obstacles in path\n            min_distance = self.get_min_distance_in_front()\n\n            # Publish safe distance\n            safe_dist_msg = Float32()\n            safe_dist_msg.data = min_distance\n            self.safe_distance_pub.publish(safe_dist_msg)\n\n            # Check if collision is imminent\n            if min_distance < self.collision_threshold:\n                # Issue collision warning\n                warning_msg = Bool()\n                warning_msg.data = True\n                self.collision_warning_pub.publish(warning_msg)\n\n                # If moving forward and obstacle is too close, trigger emergency stop\n                if self.velocity_cmd and self.velocity_cmd.linear.x > 0:\n                    self.trigger_emergency_stop()\n            else:\n                # Clear emergency stop if it was active\n                if self.emergency_stop_active:\n                    self.clear_emergency_stop()\n\n    def get_min_distance_in_front(self):\n        """Get minimum distance to obstacles in the front 90-degree sector"""\n        if not self.scan_data:\n            return float(\'inf\')\n\n        ranges = self.scan_data.ranges\n        angle_min = self.scan_data.angle_min\n        angle_increment = self.scan_data.angle_increment\n\n        # Define front sector (\xb145 degrees from forward)\n        front_ranges = []\n        for i, r in enumerate(ranges):\n            angle = angle_min + i * angle_increment\n            if -math.pi/4 <= angle <= math.pi/4:  # Front 90 degrees\n                if not np.isnan(r) and r < self.scan_data.range_max:\n                    front_ranges.append(r)\n\n        return min(front_ranges) if front_ranges else float(\'inf\')\n\n    def trigger_emergency_stop(self):\n        """Trigger emergency stop"""\n        if not self.emergency_stop_active:\n            self.emergency_stop_active = True\n            self.get_logger().warn(\'EMERGENCY STOP TRIGGERED - Collision imminent!\')\n\n            # Publish emergency stop command\n            stop_msg = Bool()\n            stop_msg.data = True\n            self.emergency_stop_pub.publish(stop_msg)\n\n    def clear_emergency_stop(self):\n        """Clear emergency stop"""\n        if self.emergency_stop_active:\n            self.emergency_stop_active = False\n            self.get_logger().info(\'Emergency stop cleared - Safe to proceed\')\n\n            # Publish clear emergency stop\n            clear_msg = Bool()\n            clear_msg.data = False\n            self.emergency_stop_pub.publish(clear_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacNavigationSafetyMonitor()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Shutting down Isaac Navigation Safety Monitor\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"4-hardwaregpu-notes",children:"4. Hardware/GPU Notes"}),"\n",(0,s.jsx)(e.h3,{id:"isaac-navigation-gpu-requirements",children:"Isaac Navigation GPU Requirements"}),"\n",(0,s.jsx)(e.p,{children:"Isaac navigation applications have specific hardware requirements based on the navigation complexity:"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Basic Navigation"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Minimum"}),": RTX 4070 Ti (12GB VRAM) for basic path planning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Memory"}),": 2-4GB for costmap processing and path planning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Compute"}),": CPU-intensive for basic navigation, minimal GPU usage"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Human-Aware Navigation"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Memory"}),": 4-8GB VRAM for human detection and social navigation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Compute"}),": GPU acceleration for human detection algorithms"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Real-time"}),": Requires sustained performance for human interaction"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Dynamic Navigation"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Memory"}),": 6-10GB VRAM for dynamic obstacle tracking and avoidance"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Compute"}),": High computational requirements for real-time obstacle prediction"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Latency"}),": Critical for collision avoidance in dynamic environments"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Social Navigation"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Memory"}),": 8-12GB VRAM for complex social behavior modeling"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Compute"}),": Advanced algorithms for human-aware path planning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sensors"}),": Multiple sensors for comprehensive environment understanding"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"memory-management-strategies",children:"Memory Management Strategies"}),"\n",(0,s.jsx)(e.p,{children:"For optimal navigation performance:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Costmap Memory Pooling"}),": Pre-allocate memory for costmap representation and updates"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Path Planning Memory"}),": Efficient storage for planned paths and waypoints"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Dynamic Obstacle Tracking"}),": Memory-efficient storage for moving obstacle predictions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Behavior Memory"}),": Adaptive memory management for different navigation behaviors"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"jetson-platform-considerations",children:"Jetson Platform Considerations"}),"\n",(0,s.jsx)(e.p,{children:"When running navigation on Jetson platforms:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Memory Architecture"}),": Unified memory architecture for efficient navigation processing"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Power Efficiency"}),": Navigation algorithms optimized for power-constrained environments"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Thermal Management"}),": Monitor temperature during intensive navigation operations"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"I/O Bandwidth"}),": Maximize sensor data bandwidth for real-time navigation"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multi-threading"}),": Separate threads for perception, planning, and control"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Predictive Planning"}),": Anticipate future states for smoother navigation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Hierarchical Planning"}),": Multi-level path planning for efficiency"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Adaptive Control"}),": Adjust control parameters based on environment complexity"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sensor Fusion"}),": Efficient integration of multiple sensor modalities"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"5-simulation-path",children:"5. Simulation Path"}),"\n",(0,s.jsx)(e.p,{children:"To implement Isaac navigation in simulation:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Isaac Sim Setup"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Launch Isaac Sim with navigation environments\ncd ~/isaac-sim\npython3 -m omni.isaac.kit --summary-cache-path ./cache\n\n# Configure navigation sensors and environments\n# Set up dynamic obstacles and human avatars\n"})}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Navigation Pipeline Testing"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Launch navigation pipeline in simulation\nros2 launch isaac_humanoid_navigation isaac_navigation_sim.launch.py\n\n# Test navigation with goals\nros2 topic pub /navigation/goal geometry_msgs/PoseStamped \"header: {frame_id: 'map'}; pose: {position: {x: 5.0, y: 5.0, z: 0.0}; orientation: {w: 1.0}}\"\n"})}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Performance Validation"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Test navigation accuracy in simulated environments"}),"\n",(0,s.jsx)(e.li,{children:"Validate path planning and obstacle avoidance"}),"\n",(0,s.jsx)(e.li,{children:"Measure computational performance and memory usage"}),"\n",(0,s.jsx)(e.li,{children:"Verify safety systems and emergency stops"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"6-real-world-path",children:"6. Real-World Path"}),"\n",(0,s.jsx)(e.p,{children:"For real-world deployment of Isaac navigation:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Hardware Integration"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Integrate navigation sensors with humanoid robot platform"}),"\n",(0,s.jsx)(e.li,{children:"Calibrate LIDAR, cameras, and other navigation sensors"}),"\n",(0,s.jsx)(e.li,{children:"Configure navigation processing pipeline"}),"\n",(0,s.jsx)(e.li,{children:"Validate sensor data quality and timing"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"System Integration"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Build Isaac navigation workspace\ncd ~/isaac_navigation_ws\ncolcon build --packages-select isaac_humanoid_navigation\nsource install/setup.bash\n\n# Launch navigation pipeline on robot\nros2 launch isaac_humanoid_navigation isaac_navigation.launch.py\n"})}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Validation and Testing"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Test navigation accuracy in real environments"}),"\n",(0,s.jsx)(e.li,{children:"Validate path planning and obstacle avoidance"}),"\n",(0,s.jsx)(e.li,{children:"Verify safety systems and emergency stops"}),"\n",(0,s.jsx)(e.li,{children:"Ensure system stability and reliability"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"7-spec-build-test-checklist",children:"7. Spec-Build-Test checklist"}),"\n",(0,s.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Isaac navigation manager node implemented and functional"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Multi-mode navigation processing working correctly"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Path planning implementation functional"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Local trajectory control working"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Collision detection and avoidance implemented"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Human-aware navigation features functional"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Safety monitoring system implemented"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Recovery behaviors implemented"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Configuration parameters properly set"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Launch files created and tested"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Performance monitoring implemented"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Navigation state management functional"]}),"\n",(0,s.jsxs)(e.li,{className:"task-list-item",children:[(0,s.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Isaac navigation pipeline validated in simulation"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"8-apa-citations",children:"8. APA citations"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:["NVIDIA Corporation. (2023). ",(0,s.jsx)(e.em,{children:"Isaac ROS: Navigation and Path Planning"}),". NVIDIA Developer Documentation. Retrieved from ",(0,s.jsx)(e.a,{href:"https://docs.nvidia.com/isaac/isaac_ros/",children:"https://docs.nvidia.com/isaac/isaac_ros/"})]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:["Fox, D., Burgard, W., & Thrun, S. (1997). The dynamic window approach to collision avoidance. ",(0,s.jsx)(e.em,{children:"IEEE Robotics & Automation Magazine"}),", 4(1), 23-33."]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:["Khatib, O. (1986). Real-time obstacle avoidance for manipulators and mobile robots. ",(0,s.jsx)(e.em,{children:"International Journal of Robotics Research"}),", 5(1), 90-98."]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:["LaValle, S. M. (2006). ",(0,s.jsx)(e.em,{children:"Planning algorithms"}),". Cambridge University Press."]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:["Karaman, S., & Frazzoli, E. (2011). Sampling-based algorithms for optimal motion planning. ",(0,s.jsx)(e.em,{children:"International Journal of Robotics Research"}),", 30(7), 846-894."]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:["Siciliano, B., & Khatib, O. (2016). ",(0,s.jsx)(e.em,{children:"Springer handbook of robotics"}),". Springer Publishing Company."]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:["Thrun, S., Burgard, W., & Fox, D. (2005). ",(0,s.jsx)(e.em,{children:"Probabilistic robotics"}),". MIT Press."]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:["Fox, D., Burgard, W., Kruppa, H., & Thrun, S. (2001). A probabilistic approach to collaborative multi-robot localization. ",(0,s.jsx)(e.em,{children:"Autonomous Robots"}),", 8(3), 325-344."]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:["Sisbot, E. A., Marquez-Chico, J. J., Simeon, T., & Alami, R. (2007). Human-aware navigation planner. ",(0,s.jsx)(e.em,{children:"IEEE/RSJ International Conference on Intelligent Robots and Systems"}),", 3814-3819."]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsxs)(e.p,{children:["Trautman, P., & Krause, A. (2010). Unfreezing the robot: Navigation in dense crowd groups. ",(0,s.jsx)(e.em,{children:"IEEE/RSJ International Conference on Intelligent Robots and Systems"}),", 797-803."]}),"\n"]}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(m,{...n})}):m(n)}},8453:(n,e,a)=>{a.d(e,{R:()=>o,x:()=>r});var i=a(6540);const s={},t=i.createContext(s);function o(n){const e=i.useContext(t);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),i.createElement(t.Provider,{value:e},n.children)}}}]);