"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[7080],{8453:(e,n,a)=>{a.d(n,{R:()=>i,x:()=>r});var t=a(6540);const s={},o=t.createContext(s);function i(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(o.Provider,{value:n},e.children)}},9502:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-3-isaac/examples/index","title":"Module 3 Examples: Isaac Perception and Navigation Code Samples","description":"Example code implementations for NVIDIA Isaac perception and navigation systems","source":"@site/docs/module-3-isaac/examples/index.md","sourceDirName":"module-3-isaac/examples","slug":"/module-3-isaac/examples/","permalink":"/hackathon-book-robotics/fr/docs/module-3-isaac/examples/","draft":false,"unlisted":false,"editUrl":"https://github.com/noormusarrat/hackathon-book-robotics/edit/main/docs/module-3-isaac/examples/index.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9,"title":"Module 3 Examples: Isaac Perception and Navigation Code Samples","description":"Example code implementations for NVIDIA Isaac perception and navigation systems"},"sidebar":"tutorialSidebar","previous":{"title":"Module 3 Exercises: NVIDIA Isaac - Perception + Navigation","permalink":"/hackathon-book-robotics/fr/docs/module-3-isaac/exercises/"},"next":{"title":"Module 3 Citations and References","permalink":"/hackathon-book-robotics/fr/docs/module-3-isaac/citations"}}');var s=a(4848),o=a(8453);const i={sidebar_position:9,title:"Module 3 Examples: Isaac Perception and Navigation Code Samples",description:"Example code implementations for NVIDIA Isaac perception and navigation systems"},r="Module 3 Examples: Isaac Perception and Navigation Code Samples",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Example 1: Isaac Perception Pipeline",id:"example-1-isaac-perception-pipeline",level:2},{value:"File: <code>isaac_perception_pipeline.py</code>",id:"file-isaac_perception_pipelinepy",level:3},{value:"Setup and Running",id:"setup-and-running",level:3},{value:"Example 2: Isaac SLAM Integration",id:"example-2-isaac-slam-integration",level:2},{value:"File: <code>isaac_slam_integration.py</code>",id:"file-isaac_slam_integrationpy",level:3},{value:"Example 3: Isaac Navigation System",id:"example-3-isaac-navigation-system",level:2},{value:"File: <code>isaac_navigation_system.py</code>",id:"file-isaac_navigation_systempy",level:3},{value:"Example 4: Isaac Bipedal Control System",id:"example-4-isaac-bipedal-control-system",level:2},{value:"File: <code>isaac_bipedal_control.py</code>",id:"file-isaac_bipedal_controlpy",level:3},{value:"Example 5: Isaac Perception-Navigation Integration",id:"example-5-isaac-perception-navigation-integration",level:2},{value:"File: <code>isaac_perception_navigation_integration.py</code>",id:"file-isaac_perception_navigation_integrationpy",level:3},{value:"Example 6: Complete Isaac System Launch File",id:"example-6-complete-isaac-system-launch-file",level:2},{value:"File: <code>isaac_complete_system.launch.py</code>",id:"file-isaac_complete_systemlaunchpy",level:3},{value:"Running the Examples",id:"running-the-examples",level:2},{value:"Important Notes",id:"important-notes",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"module-3-examples-isaac-perception-and-navigation-code-samples",children:"Module 3 Examples: Isaac Perception and Navigation Code Samples"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This section contains complete, runnable code examples that demonstrate the implementation of NVIDIA Isaac perception and navigation systems. Each example builds upon the concepts covered in the module chapters and provides practical implementations that can be used as starting points for your own projects."}),"\n",(0,s.jsx)(n.h2,{id:"example-1-isaac-perception-pipeline",children:"Example 1: Isaac Perception Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"This example demonstrates a complete Isaac perception pipeline that processes RGB-D data and performs object detection and feature tracking."}),"\n",(0,s.jsxs)(n.h3,{id:"file-isaac_perception_pipelinepy",children:["File: ",(0,s.jsx)(n.code,{children:"isaac_perception_pipeline.py"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PointStamped\nfrom visualization_msgs.msg import MarkerArray\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nfrom typing import Dict, Any, Optional\n\nclass IsaacPerceptionPipeline(Node):\n    def __init__(self):\n        super().__init__(\'isaac_perception_pipeline\')\n\n        # Initialize components\n        self.bridge = CvBridge()\n        self.camera_matrix = None\n\n        # Create subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/rgb/image_raw\', self.image_callback, 10\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, \'/camera/rgb/camera_info\', self.camera_info_callback, 10\n        )\n\n        # Create publishers\n        self.detection_pub = self.create_publisher(MarkerArray, \'/isaac/perception/detections\', 10)\n        self.feature_pub = self.create_publisher(MarkerArray, \'/isaac/perception/features\', 10)\n\n        self.get_logger().info(\'Isaac Perception Pipeline initialized\')\n\n    def camera_info_callback(self, msg):\n        """Store camera calibration parameters"""\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n\n    def image_callback(self, msg):\n        """Process incoming images"""\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n\n            # Perform object detection\n            detections = self.perform_object_detection(cv_image)\n\n            # Perform feature tracking\n            features = self.perform_feature_tracking(cv_image)\n\n            # Publish results\n            if detections:\n                detection_markers = self.create_detection_markers(detections)\n                self.detection_pub.publish(detection_markers)\n\n            if features:\n                feature_markers = self.create_feature_markers(features)\n                self.feature_pub.publish(feature_markers)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def perform_object_detection(self, image):\n        """Perform object detection using Isaac-compatible methods"""\n        # For demonstration, use a simple color-based detection\n        # In practice, integrate with Isaac ROS DNN packages\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define color ranges for common objects\n        color_ranges = [\n            (np.array([0, 50, 50]), np.array([10, 255, 255]), \'red_object\'),\n            (np.array([100, 50, 50]), np.array([130, 255, 255]), \'blue_object\'),\n        ]\n\n        detections = []\n        for lower, upper, obj_type in color_ranges:\n            mask = cv2.inRange(hsv, lower, upper)\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            for contour in contours:\n                area = cv2.contourArea(contour)\n                if area > 1000:  # Filter small contours\n                    x, y, w, h = cv2.boundingRect(contour)\n                    detections.append({\n                        \'class\': obj_type,\n                        \'bbox\': (x, y, w, h),\n                        \'confidence\': 0.8,\n                        \'center\': (x + w//2, y + h//2)\n                    })\n\n        return detections\n\n    def perform_feature_tracking(self, image):\n        """Perform feature tracking using Isaac-compatible methods"""\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Detect good features to track\n        features = cv2.goodFeaturesToTrack(\n            gray,\n            maxCorners=100,\n            qualityLevel=0.01,\n            minDistance=10,\n            blockSize=3\n        )\n\n        if features is not None:\n            return [(int(x), int(y)) for x, y in features.reshape(-1, 2)]\n        else:\n            return []\n\n    def create_detection_markers(self, detections):\n        """Create visualization markers for detections"""\n        marker_array = MarkerArray()\n\n        for i, det in enumerate(detections):\n            marker = self.create_detection_marker(det, i)\n            marker_array.markers.append(marker)\n\n        return marker_array\n\n    def create_detection_marker(self, detection, id_num):\n        """Create a single detection marker"""\n        from visualization_msgs.msg import Marker\n\n        marker = Marker()\n        marker.header.frame_id = "camera_rgb_optical_frame"\n        marker.header.stamp = self.get_clock().now().to_msg()\n        marker.ns = "perception_detections"\n        marker.id = id_num\n        marker.type = Marker.CUBE\n        marker.action = Marker.ADD\n\n        # Set position based on detection\n        x, y, w, h = detection[\'bbox\']\n        marker.pose.position.x = x / 100.0  # Scale for visualization\n        marker.pose.position.y = y / 100.0\n        marker.pose.position.z = 1.0\n\n        # Set scale\n        marker.scale.x = w / 100.0\n        marker.scale.y = h / 100.0\n        marker.scale.z = 0.1\n\n        # Set color based on class\n        if \'red\' in detection[\'class\']:\n            marker.color.r = 1.0\n            marker.color.g = 0.0\n            marker.color.b = 0.0\n        else:\n            marker.color.r = 0.0\n            marker.color.g = 1.0\n            marker.color.b = 0.0\n\n        marker.color.a = 0.7\n\n        return marker\n\n    def create_feature_markers(self, features):\n        """Create visualization markers for features"""\n        marker_array = MarkerArray()\n\n        for i, (x, y) in enumerate(features):\n            marker = self.create_feature_marker(x, y, i)\n            marker_array.markers.append(marker)\n\n        return marker_array\n\n    def create_feature_marker(self, x, y, id_num):\n        """Create a single feature marker"""\n        from visualization_msgs.msg import Marker\n\n        marker = Marker()\n        marker.header.frame_id = "camera_rgb_optical_frame"\n        marker.header.stamp = self.get_clock().now().to_msg()\n        marker.ns = "perception_features"\n        marker.id = id_num\n        marker.type = Marker.SPHERE\n        marker.action = Marker.ADD\n\n        marker.pose.position.x = x / 100.0\n        marker.pose.position.y = y / 100.0\n        marker.pose.position.z = 0.5\n\n        marker.scale.x = 0.02\n        marker.scale.y = 0.02\n        marker.scale.z = 0.02\n\n        marker.color.r = 1.0\n        marker.color.g = 1.0\n        marker.color.b = 0.0\n        marker.color.a = 0.8\n\n        return marker\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacPerceptionPipeline()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Shutting down Isaac Perception Pipeline\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"setup-and-running",children:"Setup and Running"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Install required packages:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip3 install opencv-python numpy\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:"Run the node:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 run your_package isaac_perception_pipeline.py\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"example-2-isaac-slam-integration",children:"Example 2: Isaac SLAM Integration"}),"\n",(0,s.jsx)(n.p,{children:"This example demonstrates how to integrate Isaac's SLAM capabilities into your robotics application."}),"\n",(0,s.jsxs)(n.h3,{id:"file-isaac_slam_integrationpy",children:["File: ",(0,s.jsx)(n.code,{children:"isaac_slam_integration.py"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu\nfrom nav_msgs.msg import Odometry, OccupancyGrid\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import Header\nimport tf2_ros\nfrom tf2_ros import TransformBroadcaster\nimport numpy as np\nfrom typing import Dict, Any, Optional\n\nclass IsaacSLAMIntegration(Node):\n    def __init__(self):\n        super().__init__(\'isaac_slam_integration\')\n\n        # Initialize TF broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # Create subscribers for sensor data\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/rgb/image_raw\', self.image_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, \'/imu/data\', self.imu_callback, 10\n        )\n\n        # Create publishers for SLAM results\n        self.odom_pub = self.create_publisher(Odometry, \'/isaac_slam/odometry\', 10)\n        self.map_pub = self.create_publisher(OccupancyGrid, \'/isaac_slam/map\', 10)\n        self.pose_pub = self.create_publisher(PoseStamped, \'/isaac_slam/pose\', 10)\n\n        # SLAM state variables\n        self.current_pose = np.array([0.0, 0.0, 0.0])  # x, y, theta\n        self.map_data = None\n\n        self.get_logger().info(\'Isaac SLAM Integration initialized\')\n\n    def image_callback(self, msg):\n        """Process image data for visual SLAM"""\n        # In a real implementation, this would interface with Isaac Visual SLAM\n        # For demonstration, we\'ll simulate pose estimation\n        self.update_pose_estimate()\n\n    def imu_callback(self, msg):\n        """Process IMU data for inertial integration"""\n        # Integrate IMU data to improve pose estimate\n        # In a real implementation, this would be handled by Isaac Visual-Inertial SLAM\n        pass\n\n    def update_pose_estimate(self):\n        """Update robot pose estimate (simulated)"""\n        # Simulate pose update (in real implementation, this comes from Isaac SLAM)\n        dt = 0.1  # 10Hz update\n        v = 0.1   # 0.1 m/s forward velocity\n        omega = 0.05  # 0.05 rad/s angular velocity\n\n        # Update pose using simple kinematic model\n        self.current_pose[0] += v * np.cos(self.current_pose[2]) * dt\n        self.current_pose[1] += v * np.sin(self.current_pose[2]) * dt\n        self.current_pose[2] += omega * dt\n\n        # Publish updated pose\n        self.publish_slam_results()\n\n    def publish_slam_results(self):\n        """Publish SLAM results"""\n        # Publish odometry\n        odom_msg = Odometry()\n        odom_msg.header.stamp = self.get_clock().now().to_msg()\n        odom_msg.header.frame_id = "map"\n        odom_msg.child_frame_id = "base_link"\n\n        odom_msg.pose.pose.position.x = self.current_pose[0]\n        odom_msg.pose.pose.position.y = self.current_pose[1]\n        odom_msg.pose.pose.position.z = 0.0\n\n        # Convert angle to quaternion\n        from tf_transformations import quaternion_from_euler\n        quat = quaternion_from_euler(0, 0, self.current_pose[2])\n        odom_msg.pose.pose.orientation.x = quat[0]\n        odom_msg.pose.pose.orientation.y = quat[1]\n        odom_msg.pose.pose.orientation.z = quat[2]\n        odom_msg.pose.pose.orientation.w = quat[3]\n\n        self.odom_pub.publish(odom_msg)\n\n        # Publish pose\n        pose_msg = PoseStamped()\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\n        pose_msg.header.frame_id = "map"\n        pose_msg.pose.position.x = self.current_pose[0]\n        pose_msg.pose.position.y = self.current_pose[1]\n        pose_msg.pose.position.z = 0.0\n        pose_msg.pose.orientation.x = quat[0]\n        pose_msg.pose.orientation.y = quat[1]\n        pose_msg.pose.orientation.z = quat[2]\n        pose_msg.pose.orientation.w = quat[3]\n\n        self.pose_pub.publish(pose_msg)\n\n        # Broadcast transform\n        self.broadcast_transform()\n\n    def broadcast_transform(self):\n        """Broadcast TF transform"""\n        from geometry_msgs.msg import TransformStamped\n\n        t = TransformStamped()\n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = "map"\n        t.child_frame_id = "odom"\n\n        t.transform.translation.x = self.current_pose[0]\n        t.transform.translation.y = self.current_pose[1]\n        t.transform.translation.z = 0.0\n\n        quat = [0, 0, 0, 1]  # Default quaternion\n        t.transform.rotation.x = quat[0]\n        t.transform.rotation.y = quat[1]\n        t.transform.rotation.z = quat[2]\n        t.transform.rotation.w = quat[3]\n\n        self.tf_broadcaster.sendTransform(t)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacSLAMIntegration()\n\n    # Create timer for periodic updates\n    node.create_timer(0.1, node.update_pose_estimate)  # 10Hz\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Shutting down Isaac SLAM Integration\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"example-3-isaac-navigation-system",children:"Example 3: Isaac Navigation System"}),"\n",(0,s.jsx)(n.p,{children:"This example demonstrates a complete Isaac navigation system with path planning and obstacle avoidance."}),"\n",(0,s.jsxs)(n.h3,{id:"file-isaac_navigation_systempy",children:["File: ",(0,s.jsx)(n.code,{children:"isaac_navigation_system.py"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom nav_msgs.msg import Path, OccupancyGrid\nfrom std_msgs.msg import String, Bool\nfrom geometry_msgs.msg import Point\nimport numpy as np\nfrom typing import List, Tuple\nimport math\n\nclass IsaacNavigationSystem(Node):\n    def __init__(self):\n        super().__init__(\'isaac_navigation_system\')\n\n        # Create subscribers\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10\n        )\n\n        # Create publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.path_pub = self.create_publisher(Path, \'/navigation/global_plan\', 10)\n        self.status_pub = self.create_publisher(String, \'/navigation/status\', 10)\n\n        # Navigation state\n        self.current_goal = None\n        self.current_path = []\n        self.path_index = 0\n        self.robot_pose = (0.0, 0.0, 0.0)  # x, y, theta\n        self.scan_data = None\n        self.navigation_active = False\n\n        # Navigation parameters\n        self.linear_speed = 0.3  # m/s\n        self.angular_speed = 0.5  # rad/s\n        self.min_distance_to_obstacle = 0.5  # meters\n        self.arrival_threshold = 0.3  # meters\n\n        self.get_logger().info(\'Isaac Navigation System initialized\')\n\n    def scan_callback(self, msg):\n        """Process laser scan data"""\n        self.scan_data = msg\n\n    def set_goal(self, goal_x, goal_y):\n        """Set navigation goal"""\n        self.current_goal = (goal_x, goal_y)\n        self.navigation_active = True\n\n        # Plan path to goal (simplified)\n        self.plan_path_to_goal()\n\n        status_msg = String()\n        status_msg.data = "GOAL_SET"\n        self.status_pub.publish(status_msg)\n\n    def plan_path_to_goal(self):\n        """Plan path to goal (simplified implementation)"""\n        if self.current_goal is None:\n            return\n\n        # In a real implementation, this would use proper path planning algorithms\n        # For this example, we\'ll create a straight-line path\n        start_x, start_y = self.robot_pose[0], self.robot_pose[1]\n        goal_x, goal_y = self.current_goal\n\n        # Calculate distance and intermediate points\n        distance = math.sqrt((goal_x - start_x)**2 + (goal_y - start_y)**2)\n        num_points = max(2, int(distance / 0.5))  # 0.5m spacing\n\n        path = []\n        for i in range(num_points + 1):\n            t = i / num_points if num_points > 0 else 0\n            x = start_x + t * (goal_x - start_x)\n            y = start_y + t * (goal_y - start_y)\n            path.append((x, y))\n\n        self.current_path = path\n        self.path_index = 0\n\n        # Publish path\n        self.publish_path()\n\n    def publish_path(self):\n        """Publish the planned path"""\n        path_msg = Path()\n        path_msg.header.stamp = self.get_clock().now().to_msg()\n        path_msg.header.frame_id = "map"\n\n        for x, y in self.current_path:\n            pose = PoseStamped()\n            pose.header.frame_id = "map"\n            pose.pose.position.x = x\n            pose.pose.position.y = y\n            pose.pose.position.z = 0.0\n            path_msg.poses.append(pose)\n\n        self.path_pub.publish(path_msg)\n\n    def navigate(self):\n        """Main navigation logic"""\n        if not self.navigation_active or self.current_goal is None:\n            return\n\n        if not self.scan_data:\n            # Stop robot if no scan data\n            self.stop_robot()\n            return\n\n        # Check if goal reached\n        goal_x, goal_y = self.current_goal\n        current_x, current_y = self.robot_pose[0], self.robot_pose[1]\n        distance_to_goal = math.sqrt((goal_x - current_x)**2 + (goal_y - current_y)**2)\n\n        if distance_to_goal < self.arrival_threshold:\n            self.navigation_active = False\n            self.stop_robot()\n            status_msg = String()\n            status_msg.data = "GOAL_REACHED"\n            self.status_pub.publish(status_msg)\n            return\n\n        # Check for obstacles\n        min_distance = float(\'inf\')\n        for r in self.scan_data.ranges:\n            if not np.isnan(r) and r < min_distance:\n                min_distance = r\n\n        if min_distance < self.min_distance_to_obstacle:\n            # Stop or slow down due to obstacles\n            self.avoid_obstacles(min_distance)\n            return\n\n        # Calculate velocity command to follow path\n        cmd_vel = self.follow_path()\n        self.cmd_vel_pub.publish(cmd_vel)\n\n    def follow_path(self):\n        """Calculate velocity command to follow the planned path"""\n        if not self.current_path or self.path_index >= len(self.current_path):\n            cmd = Twist()\n            return cmd\n\n        # Get current target point\n        target_x, target_y = self.current_path[self.path_index]\n\n        # Calculate relative position\n        dx = target_x - self.robot_pose[0]\n        dy = target_y - self.robot_pose[1]\n\n        # Calculate distance to target point\n        distance = math.sqrt(dx*dx + dy*dy)\n\n        # Check if reached current target point\n        if distance < 0.3:  # Within 30cm of target\n            if self.path_index < len(self.current_path) - 1:\n                self.path_index += 1\n                target_x, target_y = self.current_path[self.path_index]\n                dx = target_x - self.robot_pose[0]\n                dy = target_y - self.robot_pose[1]\n\n        # Calculate angle to target\n        target_angle = math.atan2(dy, dx)\n        angle_diff = target_angle - self.robot_pose[2]\n\n        # Normalize angle to [-pi, pi]\n        while angle_diff > math.pi:\n            angle_diff -= 2 * math.pi\n        while angle_diff < -math.pi:\n            angle_diff += 2 * math.pi\n\n        # Create velocity command\n        cmd = Twist()\n        if abs(angle_diff) > 0.1:  # If need to turn\n            cmd.angular.z = max(-self.angular_speed, min(self.angular_speed, angle_diff * 1.0))\n        else:\n            cmd.linear.x = self.linear_speed\n\n        return cmd\n\n    def avoid_obstacles(self, min_distance):\n        """Handle obstacle avoidance"""\n        cmd = Twist()\n\n        # Simple obstacle avoidance: turn away from obstacles\n        if min_distance < self.min_distance_to_obstacle * 0.7:\n            # Emergency turn\n            cmd.angular.z = self.angular_speed\n        else:\n            # Slow down\n            cmd.linear.x = self.linear_speed * 0.3\n\n        self.cmd_vel_pub.publish(cmd)\n\n    def stop_robot(self):\n        """Stop the robot"""\n        cmd = Twist()\n        self.cmd_vel_pub.publish(cmd)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacNavigationSystem()\n\n    # Set a test goal\n    node.set_goal(5.0, 5.0)\n\n    # Create timer for navigation\n    node.create_timer(0.1, node.navigate)  # 10Hz navigation update\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Shutting down Isaac Navigation System\')\n        node.stop_robot()\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"example-4-isaac-bipedal-control-system",children:"Example 4: Isaac Bipedal Control System"}),"\n",(0,s.jsx)(n.p,{children:"This example demonstrates advanced AI control for bipedal humanoid robots using Isaac."}),"\n",(0,s.jsxs)(n.h3,{id:"file-isaac_bipedal_controlpy",children:["File: ",(0,s.jsx)(n.code,{children:"isaac_bipedal_control.py"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState, Imu\nfrom geometry_msgs.msg import Twist, Vector3Stamped\nfrom std_msgs.msg import Float32, String\nimport numpy as np\nfrom typing import Dict, List\nimport math\n\nclass IsaacBipedalControl(Node):\n    def __init__(self):\n        super().__init__('isaac_bipedal_control')\n\n        # Create subscribers\n        self.joint_state_sub = self.create_subscription(\n            JointState, '/joint_states', self.joint_state_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10\n        )\n\n        # Create publishers\n        self.joint_cmd_pub = self.create_publisher(JointState, '/joint_commands', 10)\n        self.com_pub = self.create_publisher(Vector3Stamped, '/control/com', 10)\n        self.status_pub = self.create_publisher(String, '/control/status', 10)\n\n        # Robot state\n        self.joint_positions = {}\n        self.joint_velocities = {}\n        self.imu_orientation = None\n        self.imu_angular_velocity = None\n        self.imu_linear_acceleration = None\n\n        # Control parameters\n        self.com_height = 0.8  # Desired center of mass height\n        self.balance_kp = 50.0\n        self.balance_kd = 10.0\n        self.control_frequency = 200.0  # Hz\n\n        # Walking parameters\n        self.step_length = 0.3\n        self.step_width = 0.2\n        self.step_height = 0.05\n        self.walking_frequency = 1.0  # Hz\n\n        self.get_logger().info('Isaac Bipedal Control initialized')\n\n    def joint_state_callback(self, msg):\n        \"\"\"Process joint state data\"\"\"\n        for i, name in enumerate(msg.name):\n            if i < len(msg.position):\n                self.joint_positions[name] = msg.position[i]\n            if i < len(msg.velocity):\n                self.joint_velocities[name] = msg.velocity[i]\n\n    def imu_callback(self, msg):\n        \"\"\"Process IMU data\"\"\"\n        self.imu_orientation = (\n            msg.orientation.x,\n            msg.orientation.y,\n            msg.orientation.z,\n            msg.orientation.w\n        )\n        self.imu_angular_velocity = (\n            msg.angular_velocity.x,\n            msg.angular_velocity.y,\n            msg.angular_velocity.z\n        )\n        self.imu_linear_acceleration = (\n            msg.linear_acceleration.x,\n            msg.linear_acceleration.y,\n            msg.linear_acceleration.z\n        )\n\n    def estimate_center_of_mass(self):\n        \"\"\"Estimate center of mass position (simplified)\"\"\"\n        # This is a simplified estimation\n        # In a real implementation, use the robot's kinematic model\n        com_x = 0.0  # Simplified - assume centered\n        com_y = 0.0  # Simplified - assume centered\n        com_z = self.com_height  # Fixed height assumption\n\n        # Publish CoM for visualization\n        com_msg = Vector3Stamped()\n        com_msg.header.stamp = self.get_clock().now().to_msg()\n        com_msg.header.frame_id = \"base_link\"\n        com_msg.vector.x = com_x\n        com_msg.vector.y = com_y\n        com_msg.vector.z = com_z\n        self.com_pub.publish(com_msg)\n\n        return (com_x, com_y, com_z)\n\n    def calculate_balance_control(self, desired_com, actual_com):\n        \"\"\"Calculate balance control torques\"\"\"\n        # Calculate error\n        com_error = (\n            desired_com[0] - actual_com[0],\n            desired_com[1] - actual_com[1],\n            desired_com[2] - actual_com[2]\n        )\n\n        # Simple PD control for balance\n        balance_torques = {\n            'left_hip_roll': self.balance_kp * com_error[1] - self.balance_kd * 0,  # Simplified\n            'right_hip_roll': -self.balance_kp * com_error[1] + self.balance_kd * 0,\n            'left_hip_pitch': self.balance_kp * com_error[0] - self.balance_kd * 0,\n            'right_hip_pitch': self.balance_kp * com_error[0] - self.balance_kd * 0,\n        }\n\n        return balance_torques\n\n    def generate_walking_pattern(self, time_in_cycle):\n        \"\"\"Generate walking pattern based on gait cycle\"\"\"\n        # Simplified walking pattern generation\n        phase = (time_in_cycle * self.walking_frequency) % 1.0\n\n        # Determine which foot is swing foot based on phase\n        if phase < 0.5:\n            # Left foot swing, right foot stance\n            left_foot_z = self.step_height * math.sin(phase * 2 * math.pi)  # Swing motion\n            right_foot_z = 0.0  # Stance foot\n        else:\n            # Right foot swing, left foot stance\n            left_foot_z = 0.0  # Stance foot\n            right_foot_z = self.step_height * math.sin((phase - 0.5) * 2 * math.pi)  # Swing motion\n\n        return {\n            'left_foot_z': left_foot_z,\n            'right_foot_z': right_foot_z,\n            'step_phase': phase\n        }\n\n    def generate_joint_commands(self):\n        \"\"\"Generate joint commands for bipedal locomotion\"\"\"\n        # Estimate current state\n        current_com = self.estimate_center_of_mass()\n\n        # Calculate desired CoM position\n        desired_com = (0.0, 0.0, self.com_height)  # Keep centered and at desired height\n\n        # Calculate balance control\n        balance_torques = self.calculate_balance_control(desired_com, current_com)\n\n        # Generate walking pattern\n        walking_pattern = self.generate_walking_pattern(self.get_clock().now().nanoseconds / 1e9)\n\n        # Create joint command message\n        cmd = JointState()\n        cmd.header.stamp = self.get_clock().now().to_msg()\n        cmd.header.frame_id = \"base_link\"\n\n        # Define joint names for a typical humanoid robot\n        joint_names = [\n            'left_hip_roll', 'left_hip_pitch', 'left_knee', 'left_ankle_pitch', 'left_ankle_roll',\n            'right_hip_roll', 'right_hip_pitch', 'right_knee', 'right_ankle_pitch', 'right_ankle_roll'\n        ]\n\n        cmd.name = joint_names\n        cmd.position = [0.0] * len(joint_names)  # Placeholder positions\n        cmd.velocity = [0.0] * len(joint_names)  # Placeholder velocities\n        cmd.effort = [0.0] * len(joint_names)    # Placeholder efforts\n\n        # Apply balance corrections to joint positions\n        # This is a simplified example - real implementation would be more complex\n        cmd.effort[0] = balance_torques.get('left_hip_roll', 0.0)  # Left hip roll\n        cmd.effort[5] = balance_torques.get('right_hip_roll', 0.0)  # Right hip roll\n        cmd.effort[1] = balance_torques.get('left_hip_pitch', 0.0)  # Left hip pitch\n        cmd.effort[6] = balance_torques.get('right_hip_pitch', 0.0)  # Right hip pitch\n\n        # Add walking pattern influences (simplified)\n        cmd.position[2] += walking_pattern['left_foot_z'] * 0.1  # Left knee for stepping\n        cmd.position[7] += walking_pattern['right_foot_z'] * 0.1  # Right knee for stepping\n\n        return cmd\n\n    def control_loop(self):\n        \"\"\"Main control loop\"\"\"\n        # Generate joint commands\n        joint_commands = self.generate_joint_commands()\n\n        # Publish joint commands\n        self.joint_cmd_pub.publish(joint_commands)\n\n        # Publish status\n        status_msg = String()\n        status_msg.data = \"BALANCED\"\n        self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacBipedalControl()\n\n    # Create timer for control loop\n    node.create_timer(1.0/node.control_frequency, node.control_loop)\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down Isaac Bipedal Control')\n        # Send zero commands to stop robot\n        zero_cmd = JointState()\n        zero_cmd.header.stamp = node.get_clock().now().to_msg()\n        zero_cmd.header.frame_id = \"base_link\"\n        node.joint_cmd_pub.publish(zero_cmd)\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"example-5-isaac-perception-navigation-integration",children:"Example 5: Isaac Perception-Navigation Integration"}),"\n",(0,s.jsx)(n.p,{children:"This example demonstrates how to integrate perception and navigation systems using Isaac."}),"\n",(0,s.jsxs)(n.h3,{id:"file-isaac_perception_navigation_integrationpy",children:["File: ",(0,s.jsx)(n.code,{children:"isaac_perception_navigation_integration.py"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom nav_msgs.msg import Path, OccupancyGrid\nfrom visualization_msgs.msg import MarkerArray\nfrom std_msgs.msg import String\nimport numpy as np\nimport math\nfrom typing import List, Tuple\n\nclass IsaacPerceptionNavigationIntegration(Node):\n    def __init__(self):\n        super().__init__('isaac_perception_navigation_integration')\n\n        # Create subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/rgb/image_raw', self.image_callback, 10\n        )\n        self.scan_sub = self.create_subscription(\n            LaserScan, '/scan', self.scan_callback, 10\n        )\n\n        # Create publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.goal_pub = self.create_publisher(PoseStamped, '/move_base_simple/goal', 10)\n        self.semantic_map_pub = self.create_publisher(OccupancyGrid, '/semantic_map', 10)\n        self.navigation_status_pub = self.create_publisher(String, '/navigation/status', 10)\n\n        # System state\n        self.objects_detected = []\n        self.obstacles = []\n        self.current_goal = None\n        self.navigation_active = False\n\n        # Navigation parameters\n        self.safe_distance = 0.8  # meters\n        self.object_approach_distance = 2.0  # meters\n\n        self.get_logger().info('Isaac Perception-Navigation Integration initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process image for object detection\"\"\"\n        # In a real implementation, this would use Isaac ROS DNN\n        # For this example, we'll simulate object detection\n        self.detect_objects()\n\n    def scan_callback(self, msg):\n        \"\"\"Process laser scan for obstacle detection\"\"\"\n        # Process scan data to detect obstacles\n        ranges = msg.ranges\n        angles = [msg.angle_min + i * msg.angle_increment for i in range(len(ranges))]\n\n        obstacles = []\n        for i, r in enumerate(ranges):\n            if not np.isnan(r) and msg.range_min < r < msg.range_max * 0.9:\n                x = r * math.cos(angles[i])\n                y = r * math.sin(angles[i])\n                obstacles.append((x, y))\n\n        self.obstacles = obstacles\n\n    def detect_objects(self):\n        \"\"\"Simulate object detection (in real implementation, use Isaac ROS DNN)\"\"\"\n        # For demonstration, simulate detecting objects at fixed positions\n        # In real implementation, use Isaac ROS DNN packages\n        self.objects_detected = [\n            {'class': 'person', 'position': (3.0, 1.5), 'confidence': 0.9},\n            {'class': 'chair', 'position': (4.2, -0.8), 'confidence': 0.85},\n            {'class': 'table', 'position': (2.1, 2.3), 'confidence': 0.92}\n        ]\n\n    def find_object_of_interest(self, object_class):\n        \"\"\"Find objects of a specific class\"\"\"\n        for obj in self.objects_detected:\n            if obj['class'] == object_class:\n                return obj\n        return None\n\n    def navigate_to_object(self, object_class):\n        \"\"\"Navigate to an object of interest\"\"\"\n        obj = self.find_object_of_interest(object_class)\n        if obj:\n            # Set goal near the object\n            goal_x = obj['position'][0]\n            goal_y = obj['position'][1]\n\n            # Approach from a safe distance\n            approach_distance = self.object_approach_distance\n            current_x, current_y = 0.0, 0.0  # Current position (simplified)\n\n            # Calculate approach point\n            dx = goal_x - current_x\n            dy = goal_y - current_y\n            distance = math.sqrt(dx*dx + dy*dy)\n\n            if distance > approach_distance:\n                scale = approach_distance / distance\n                target_x = current_x + dx * scale\n                target_y = current_y + dy * scale\n            else:\n                target_x = goal_x\n                target_y = goal_y\n\n            # Create and publish goal\n            goal_msg = PoseStamped()\n            goal_msg.header.stamp = self.get_clock().now().to_msg()\n            goal_msg.header.frame_id = \"map\"\n            goal_msg.pose.position.x = target_x\n            goal_msg.pose.position.y = target_y\n            goal_msg.pose.position.z = 0.0\n            goal_msg.pose.orientation.w = 1.0\n\n            self.current_goal = (target_x, target_y)\n            self.navigation_active = True\n\n            self.get_logger().info(f'Navigating to {object_class} at ({target_x:.2f}, {target_y:.2f})')\n\n    def avoid_obstacles(self):\n        \"\"\"Generate velocity commands to avoid obstacles\"\"\"\n        cmd = Twist()\n\n        # Check if path is clear to goal\n        if self.current_goal and self.obstacles:\n            goal_x, goal_y = self.current_goal\n            current_x, current_y = 0.0, 0.0  # Current position (simplified)\n\n            # Check for obstacles along path to goal\n            path_clear = True\n            for obs_x, obs_y in self.obstacles:\n                # Calculate distance from line between current position and goal\n                # Simplified: check distance to each obstacle\n                dist_to_obs = math.sqrt((obs_x - current_x)**2 + (obs_y - current_y)**2)\n                dist_to_goal = math.sqrt((goal_x - current_x)**2 + (goal_y - current_y)**2)\n\n                # Check if obstacle is on path to goal\n                if dist_to_obs < self.safe_distance and dist_to_obs < dist_to_goal:\n                    path_clear = False\n                    break\n\n            if not path_clear:\n                # Implement obstacle avoidance behavior\n                cmd.angular.z = 0.5  # Turn to avoid\n                cmd.linear.x = 0.0  # Stop forward motion\n            else:\n                # Path is clear, move toward goal\n                angle_to_goal = math.atan2(goal_y - current_y, goal_x - current_x)\n                cmd.angular.z = angle_to_goal * 0.5  # Proportional control\n                cmd.linear.x = 0.3  # Move forward\n\n        return cmd\n\n    def navigation_loop(self):\n        \"\"\"Main navigation loop\"\"\"\n        if not self.navigation_active:\n            return\n\n        # Generate obstacle avoidance commands\n        cmd = self.avoid_obstacles()\n\n        # Check if reached goal\n        if self.current_goal:\n            current_x, current_y = 0.0, 0.0  # Current position (simplified)\n            goal_x, goal_y = self.current_goal\n            distance = math.sqrt((goal_x - current_x)**2 + (goal_y - current_y)**2)\n\n            if distance < 0.3:  # Within 30cm of goal\n                self.navigation_active = False\n                cmd = Twist()  # Stop robot\n                self.get_logger().info('Reached goal!')\n\n                status_msg = String()\n                status_msg.data = \"GOAL_REACHED\"\n                self.navigation_status_pub.publish(status_msg)\n\n        # Publish velocity command\n        self.cmd_vel_pub.publish(cmd)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacPerceptionNavigationIntegration()\n\n    # Example: Navigate to a person\n    node.create_timer(5.0, lambda: node.navigate_to_object('person'))\n\n    # Navigation loop timer\n    node.create_timer(0.1, node.navigation_loop)\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down Isaac Perception-Navigation Integration')\n        # Stop robot\n        stop_cmd = Twist()\n        node.cmd_vel_pub.publish(stop_cmd)\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"example-6-complete-isaac-system-launch-file",children:"Example 6: Complete Isaac System Launch File"}),"\n",(0,s.jsxs)(n.h3,{id:"file-isaac_complete_systemlaunchpy",children:["File: ",(0,s.jsx)(n.code,{children:"isaac_complete_system.launch.py"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    namespace = LaunchConfiguration('namespace')\n\n    return LaunchDescription([\n        # Declare launch arguments\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation (Gazebo) clock if true'\n        ),\n        DeclareLaunchArgument(\n            'namespace',\n            default_value='',\n            description='Robot namespace'\n        ),\n\n        # Isaac Perception Pipeline\n        Node(\n            package='your_robot_package',\n            executable='isaac_perception_pipeline',\n            name='isaac_perception_pipeline',\n            namespace=namespace,\n            parameters=[\n                {'use_sim_time': use_sim_time}\n            ],\n            output='screen'\n        ),\n\n        # Isaac SLAM Integration\n        Node(\n            package='your_robot_package',\n            executable='isaac_slam_integration',\n            name='isaac_slam_integration',\n            namespace=namespace,\n            parameters=[\n                {'use_sim_time': use_sim_time}\n            ],\n            output='screen'\n        ),\n\n        # Isaac Navigation System\n        Node(\n            package='your_robot_package',\n            executable='isaac_navigation_system',\n            name='isaac_navigation_system',\n            namespace=namespace,\n            parameters=[\n                {'use_sim_time': use_sim_time}\n            ],\n            output='screen'\n        ),\n\n        # Isaac Bipedal Control\n        Node(\n            package='your_robot_package',\n            executable='isaac_bipedal_control',\n            name='isaac_bipedal_control',\n            namespace=namespace,\n            parameters=[\n                {'use_sim_time': use_sim_time}\n            ],\n            output='screen'\n        ),\n\n        # Isaac Perception-Navigation Integration\n        Node(\n            package='your_robot_package',\n            executable='isaac_perception_navigation_integration',\n            name='isaac_perception_navigation_integration',\n            namespace=namespace,\n            parameters=[\n                {'use_sim_time': use_sim_time}\n            ],\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"running-the-examples",children:"Running the Examples"}),"\n",(0,s.jsx)(n.p,{children:"To run these examples:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Set up your ROS 2 workspace:"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/isaac_examples_ws/src\ncd ~/isaac_examples_ws/src\n# Copy your robot package with the example files\ncd ..\ncolcon build\nsource install/setup.bash\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Launch the complete system:"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 launch your_robot_package isaac_complete_system.launch.py\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Test individual components:"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Test perception pipeline\nros2 run your_robot_package isaac_perception_pipeline.py\n\n# Test navigation\nros2 run your_robot_package isaac_navigation_system.py\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"4",children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Visualize in RViz:"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 run rviz2 rviz2\n# Add displays for topics like /isaac/perception/detections, /navigation/global_plan, etc.\n"})}),"\n",(0,s.jsx)(n.h2,{id:"important-notes",children:"Important Notes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"These examples are simplified for educational purposes"}),"\n",(0,s.jsx)(n.li,{children:"In production systems, add proper error handling and safety checks"}),"\n",(0,s.jsx)(n.li,{children:"Consider real-time constraints when implementing control systems"}),"\n",(0,s.jsx)(n.li,{children:"Adapt the code to your specific robot's joint names and kinematics"}),"\n",(0,s.jsx)(n.li,{children:"Test thoroughly in simulation before deploying on physical robots"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"For more advanced implementations, consider integrating with Isaac's specialized packages like Isaac ROS Visual SLAM, Isaac ROS DNN, and Isaac Sim for comprehensive testing."})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}}}]);