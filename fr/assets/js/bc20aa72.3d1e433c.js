"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[7673],{1504:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-2-simulation/chapter-3","title":"Chapter 10 - Physics and Sensors in Gazebo","description":"Why This Concept Matters for Humanoids","source":"@site/docs/module-2-simulation/chapter-3.md","sourceDirName":"module-2-simulation","slug":"/module-2-simulation/chapter-3","permalink":"/hackathon-book-robotics/fr/docs/module-2-simulation/chapter-3","draft":false,"unlisted":false,"editUrl":"https://github.com/noormusarrat/hackathon-book-robotics/edit/main/docs/module-2-simulation/chapter-3.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"title":"Chapter 10 - Physics and Sensors in Gazebo","sidebar_position":10},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 9 - Gazebo for ROS 2 Integration","permalink":"/hackathon-book-robotics/fr/docs/module-2-simulation/chapter-2"},"next":{"title":"Chapter 11 - Unity for Robotics Visualization","permalink":"/hackathon-book-robotics/fr/docs/module-2-simulation/chapter-4"}}');var a=i(4848),r=i(8453);const t={title:"Chapter 10 - Physics and Sensors in Gazebo",sidebar_position:10},o="Chapter 10: Physics and Sensors in Gazebo",l={},c=[{value:"Why This Concept Matters for Humanoids",id:"why-this-concept-matters-for-humanoids",level:2},{value:"Theory",id:"theory",level:2},{value:"Physics Engine Fundamentals",id:"physics-engine-fundamentals",level:3},{value:"Multi-Body Dynamics",id:"multi-body-dynamics",level:3},{value:"Sensor Simulation Theory",id:"sensor-simulation-theory",level:3},{value:"Realism vs. Performance Trade-offs",id:"realism-vs-performance-trade-offs",level:3},{value:"Implementation",id:"implementation",level:2},{value:"Advanced URDF with Physics Properties",id:"advanced-urdf-with-physics-properties",level:3},{value:"Physics Configuration File",id:"physics-configuration-file",level:3},{value:"Sensor Processing Node",id:"sensor-processing-node",level:3},{value:"Hardware/GPU Notes",id:"hardwaregpu-notes",level:2},{value:"Physics Simulation Requirements",id:"physics-simulation-requirements",level:3},{value:"Sensor Simulation Requirements",id:"sensor-simulation-requirements",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Simulation Path",id:"simulation-path",level:2},{value:"Physics Setup",id:"physics-setup",level:3},{value:"Sensor Configuration",id:"sensor-configuration",level:3},{value:"Validation Process",id:"validation-process",level:3},{value:"Real-World Path",id:"real-world-path",level:2},{value:"Physics Validation",id:"physics-validation",level:3},{value:"Sensor Calibration",id:"sensor-calibration",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Spec-Build-Test Checklist",id:"spec-build-test-checklist",level:2},{value:"APA Citations",id:"apa-citations",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-10-physics-and-sensors-in-gazebo",children:"Chapter 10: Physics and Sensors in Gazebo"})}),"\n",(0,a.jsx)(n.h2,{id:"why-this-concept-matters-for-humanoids",children:"Why This Concept Matters for Humanoids"}),"\n",(0,a.jsx)(n.p,{children:"Accurate physics simulation and realistic sensor modeling are crucial for humanoid robotics, as these robots must interact with the physical world in complex ways. Humanoid robots rely on precise balance, coordinated movement, and environmental interaction, all of which depend heavily on accurate physics simulation. For safe and effective humanoid operation, the simulation must accurately model contact forces, friction, and multi-body dynamics. Similarly, sensor simulation must closely match real hardware to ensure successful simulation-to-reality transfer. Without accurate physics and sensor simulation, control algorithms developed in simulation may fail when deployed on real hardware, potentially causing damage or safety issues."}),"\n",(0,a.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,a.jsx)(n.p,{children:"Physics simulation in Gazebo involves modeling the fundamental laws of physics to create realistic robot-environment interactions:"}),"\n",(0,a.jsx)(n.h3,{id:"physics-engine-fundamentals",children:"Physics Engine Fundamentals"}),"\n",(0,a.jsx)(n.p,{children:"Gazebo uses Open Dynamics Engine (ODE), Bullet, or DART physics engines to simulate:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Rigid body dynamics"}),": Motion of solid objects under applied forces"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Collision detection"}),": Identifying when objects make contact"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Contact response"}),": Calculating forces when objects touch"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Friction modeling"}),": Simulating surface interactions and grip"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"multi-body-dynamics",children:"Multi-Body Dynamics"}),"\n",(0,a.jsx)(n.p,{children:"For humanoid robots with multiple interconnected links:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Forward dynamics"}),": Computing motion from applied forces"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Inverse dynamics"}),": Computing forces needed for desired motion"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Constraint solving"}),": Maintaining joint relationships"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Stability"}),": Maintaining numerical stability in complex systems"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"sensor-simulation-theory",children:"Sensor Simulation Theory"}),"\n",(0,a.jsx)(n.p,{children:"Accurate sensor simulation requires modeling:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Physical principles"}),": How sensors actually work (optics, magnetism, etc.)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise characteristics"}),": Realistic sensor noise and uncertainty"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Latency"}),": Communication and processing delays"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Bandwidth limitations"}),": Data rate constraints"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"realism-vs-performance-trade-offs",children:"Realism vs. Performance Trade-offs"}),"\n",(0,a.jsx)(n.p,{children:"Simulation design involves balancing:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Physical accuracy vs. computational performance"}),"\n",(0,a.jsx)(n.li,{children:"Sensor realism vs. simulation speed"}),"\n",(0,a.jsx)(n.li,{children:"Environmental complexity vs. stability"}),"\n",(0,a.jsx)(n.li,{children:"Model detail vs. real-time constraints"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,a.jsx)(n.p,{children:"Let's implement realistic physics and sensor configurations for humanoid robotics in Gazebo:"}),"\n",(0,a.jsx)(n.h3,{id:"advanced-urdf-with-physics-properties",children:"Advanced URDF with Physics Properties"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0" ?>\n<robot name="advanced_humanoid" xmlns:xacro="http://www.ros.org/wiki/xacro">\n\n  \x3c!-- Materials --\x3e\n  <material name="black">\n    <color rgba="0.0 0.0 0.0 1.0"/>\n  </material>\n  <material name="blue">\n    <color rgba="0.0 0.0 0.8 1.0"/>\n  </material>\n  <material name="green">\n    <color rgba="0.0 0.8 0.0 1.0"/>\n  </material>\n  <material name="grey">\n    <color rgba="0.2 0.2 0.2 1.0"/>\n  </material>\n  <material name="orange">\n    <color rgba="1.0 0.423529411765 0.0392156862745 1.0"/>\n  </material>\n  <material name="brown">\n    <color rgba="0.870588235294 0.811764705882 0.764705882353 1.0"/>\n  </material>\n  <material name="red">\n    <color rgba="0.8 0.0 0.0 1.0"/>\n  </material>\n  <material name="white">\n    <color rgba="1.0 1.0 1.0 1.0"/>\n  </material>\n\n  \x3c!-- Base link with detailed physics --\x3e\n  <link name="base_link">\n    <inertial>\n      <mass value="15.0"/>\n      <origin xyz="0 0 0.15"/>\n      <inertia ixx="0.2" ixy="0.0" ixz="0.0" iyy="0.3" iyz="0.0" izz="0.1"/>\n    </inertial>\n    <visual>\n      <origin xyz="0 0 0.15"/>\n      <geometry>\n        <mesh filename="package://my_humanoid_description/meshes/base_link.dae"/>\n      </geometry>\n      <material name="grey"/>\n    </visual>\n    <collision>\n      <origin xyz="0 0 0.15"/>\n      <geometry>\n        <box size="0.3 0.25 0.3"/>\n      </geometry>\n    </collision>\n  </link>\n\n  \x3c!-- Torso with COM adjustment --\x3e\n  <link name="torso">\n    <inertial>\n      <mass value="8.0"/>\n      <origin xyz="0 0 0.25"/>\n      <inertia ixx="0.1" ixy="0.0" ixz="0.0" iyy="0.15" iyz="0.0" izz="0.08"/>\n    </inertial>\n    <visual>\n      <origin xyz="0 0 0.25"/>\n      <geometry>\n        <mesh filename="package://my_humanoid_description/meshes/torso.dae"/>\n      </geometry>\n      <material name="blue"/>\n    </visual>\n    <collision>\n      <origin xyz="0 0 0.25"/>\n      <geometry>\n        <box size="0.2 0.15 0.5"/>\n      </geometry>\n    </collision>\n  </link>\n\n  <joint name="base_torso_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="torso"/>\n    <origin xyz="0 0 0.3"/>\n  </joint>\n\n  \x3c!-- Head with realistic properties --\x3e\n  <link name="head">\n    <inertial>\n      <mass value="2.0"/>\n      <origin xyz="0 0 0.08"/>\n      <inertia ixx="0.005" ixy="0.0" ixz="0.0" iyy="0.005" iyz="0.0" izz="0.005"/>\n    </inertial>\n    <visual>\n      <origin xyz="0 0 0.08"/>\n      <geometry>\n        <mesh filename="package://my_humanoid_description/meshes/head.dae"/>\n      </geometry>\n      <material name="white"/>\n    </visual>\n    <collision>\n      <origin xyz="0 0 0.08"/>\n      <geometry>\n        <sphere radius="0.1"/>\n      </geometry>\n    </collision>\n  </link>\n\n  <joint name="neck_joint" type="revolute">\n    <parent link="torso"/>\n    <child link="head"/>\n    <origin xyz="0 0 0.5"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="-0.5" upper="0.5" effort="15.0" velocity="3.0"/>\n    <dynamics damping="0.5" friction="0.1"/>\n  </joint>\n\n  \x3c!-- Left leg with detailed physics --\x3e\n  <link name="left_hip">\n    <inertial>\n      <mass value="3.0"/>\n      <origin xyz="0 0 -0.1"/>\n      <inertia ixx="0.01" ixy="0.0" ixz="0.0" iyy="0.01" iyz="0.0" izz="0.005"/>\n    </inertial>\n    <visual>\n      <origin xyz="0 0 -0.1"/>\n      <geometry>\n        <mesh filename="package://my_humanoid_description/meshes/hip.dae"/>\n      </geometry>\n      <material name="red"/>\n    </visual>\n    <collision>\n      <origin xyz="0 0 -0.1"/>\n      <geometry>\n        <cylinder length="0.2" radius="0.05"/>\n      </geometry>\n    </collision>\n  </link>\n\n  <joint name="left_hip_joint" type="revolute">\n    <parent link="base_link"/>\n    <child link="left_hip"/>\n    <origin xyz="-0.08 0.0 0.15"/>\n    <axis xyz="1 0 0"/>\n    <limit lower="-1.57" upper="1.57" effort="100.0" velocity="5.0"/>\n    <dynamics damping="1.0" friction="0.5"/>\n  </joint>\n\n  \x3c!-- Left knee --\x3e\n  <link name="left_knee">\n    <inertial>\n      <mass value="2.5"/>\n      <origin xyz="0 0 -0.15"/>\n      <inertia ixx="0.008" ixy="0.0" ixz="0.0" iyy="0.008" iyz="0.0" izz="0.004"/>\n    </inertial>\n    <visual>\n      <origin xyz="0 0 -0.15"/>\n      <geometry>\n        <mesh filename="package://my_humanoid_description/meshes/knee.dae"/>\n      </geometry>\n      <material name="red"/>\n    </visual>\n    <collision>\n      <origin xyz="0 0 -0.15"/>\n      <geometry>\n        <cylinder length="0.3" radius="0.04"/>\n      </geometry>\n    </collision>\n  </link>\n\n  <joint name="left_knee_joint" type="revolute">\n    <parent link="left_hip"/>\n    <child link="left_knee"/>\n    <origin xyz="0 0 -0.2"/>\n    <axis xyz="1 0 0"/>\n    <limit lower="0" upper="2.35" effort="100.0" velocity="5.0"/>\n    <dynamics damping="1.0" friction="0.5"/>\n  </joint>\n\n  \x3c!-- Left ankle --\x3e\n  <link name="left_ankle">\n    <inertial>\n      <mass value="1.5"/>\n      <origin xyz="0 0 -0.05"/>\n      <inertia ixx="0.003" ixy="0.0" ixz="0.0" iyy="0.003" iyz="0.0" izz="0.002"/>\n    </inertial>\n    <visual>\n      <origin xyz="0 0 -0.05"/>\n      <geometry>\n        <mesh filename="package://my_humanoid_description/meshes/ankle.dae"/>\n      </geometry>\n      <material name="red"/>\n    </visual>\n    <collision>\n      <origin xyz="0 0 -0.05"/>\n      <geometry>\n        <box size="0.1 0.08 0.1"/>\n      </geometry>\n    </collision>\n  </link>\n\n  <joint name="left_ankle_joint" type="revolute">\n    <parent link="left_knee"/>\n    <child link="left_ankle"/>\n    <origin xyz="0 0 -0.3"/>\n    <axis xyz="1 0 0"/>\n    <limit lower="-0.5" upper="0.5" effort="50.0" velocity="3.0"/>\n    <dynamics damping="0.5" friction="0.2"/>\n  </joint>\n\n  \x3c!-- Left foot --\x3e\n  <link name="left_foot">\n    <inertial>\n      <mass value="1.0"/>\n      <origin xyz="0.05 0 -0.02"/>\n      <inertia ixx="0.001" ixy="0.0" ixz="0.0" iyy="0.002" iyz="0.0" izz="0.002"/>\n    </inertial>\n    <visual>\n      <origin xyz="0.05 0 -0.02"/>\n      <geometry>\n        <mesh filename="package://my_humanoid_description/meshes/foot.dae"/>\n      </geometry>\n      <material name="red"/>\n    </visual>\n    <collision>\n      <origin xyz="0.05 0 -0.02"/>\n      <geometry>\n        <box size="0.2 0.1 0.04"/>\n      </geometry>\n    </collision>\n  </link>\n\n  <joint name="left_foot_joint" type="fixed">\n    <parent link="left_ankle"/>\n    <child link="left_foot"/>\n    <origin xyz="0 0 -0.1"/>\n  </joint>\n\n  \x3c!-- Gazebo plugins for physics and sensors --\x3e\n  <gazebo>\n    <plugin name="gazebo_ros2_control" filename="libgazebo_ros2_control.so">\n      <parameters>$(find my_humanoid_description)/config/humanoid_controllers.yaml</parameters>\n    </plugin>\n  </gazebo>\n\n  \x3c!-- Gazebo material properties --\x3e\n  <gazebo reference="base_link">\n    <material>Gazebo/Grey</material>\n    <mu1>0.8</mu1>\n    <mu2>0.8</mu2>\n    <kp>1000000.0</kp>\n    <kd>100.0</kd>\n    <fdir1>0 0 1</fdir1>\n    <maxVel>100.0</maxVel>\n    <minDepth>0.001</minDepth>\n  </gazebo>\n\n  <gazebo reference="torso">\n    <material>Gazebo/Blue</material>\n    <mu1>0.8</mu1>\n    <mu2>0.8</mu2>\n    <kp>1000000.0</kp>\n    <kd>100.0</kd>\n  </gazebo>\n\n  <gazebo reference="head">\n    <material>Gazebo/White</material>\n    <mu1>0.8</mu1>\n    <mu2>0.8</mu2>\n    <kp>1000000.0</kp>\n    <kd>100.0</kd>\n  </gazebo>\n\n  \x3c!-- Left leg physics properties --\x3e\n  <gazebo reference="left_hip">\n    <mu1>0.8</mu1>\n    <mu2>0.8</mu2>\n    <kp>1000000.0</kp>\n    <kd>100.0</kd>\n  </gazebo>\n\n  <gazebo reference="left_knee">\n    <mu1>0.8</mu1>\n    <mu2>0.8</mu2>\n    <kp>1000000.0</kp>\n    <kd>100.0</kd>\n  </gazebo>\n\n  <gazebo reference="left_ankle">\n    <mu1>0.8</mu1>\n    <mu2>0.8</mu2>\n    <kp>1000000.0</kp>\n    <kd>100.0</kd>\n  </gazebo>\n\n  <gazebo reference="left_foot">\n    <mu1>0.8</mu1>\n    <mu2>0.8</mu2>\n    <kp>1000000.0</kp>\n    <kd>100.0</kd>\n    <fdir1>1 0 0</fdir1> \x3c!-- Friction direction for foot contact --\x3e\n  </gazebo>\n\n  \x3c!-- Camera sensor with realistic parameters --\x3e\n  <gazebo reference="head">\n    <sensor name="head_camera" type="camera">\n      <always_on>true</always_on>\n      <update_rate>30</update_rate>\n      <camera name="head_camera">\n        <horizontal_fov>1.047</horizontal_fov> \x3c!-- 60 degrees --\x3e\n        <image>\n          <width>640</width>\n          <height>480</height>\n          <format>R8G8B8</format>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>10.0</far>\n        </clip>\n        <noise>\n          <type>gaussian</type>\n          <mean>0.0</mean>\n          <stddev>0.007</stddev>\n        </noise>\n      </camera>\n      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n        <frame_name>head_camera_optical_frame</frame_name>\n        <min_depth>0.1</min_depth>\n        <max_depth>10.0</max_depth>\n        <update_rate>30.0</update_rate>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- IMU sensor with realistic noise --\x3e\n  <gazebo reference="torso">\n    <sensor name="torso_imu" type="imu">\n      <always_on>true</always_on>\n      <update_rate>100</update_rate>\n      <imu>\n        <angular_velocity>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.0017</stddev> \x3c!-- ~0.1 deg/s --\x3e\n            </noise>\n          </x>\n          <y>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.0017</stddev>\n            </noise>\n          </y>\n          <z>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.0017</stddev>\n            </noise>\n          </z>\n        </angular_velocity>\n        <linear_acceleration>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.017</stddev> \x3c!-- ~0.017 m/s^2 --\x3e\n            </noise>\n          </x>\n          <y>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.017</stddev>\n            </noise>\n          </y>\n          <z>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.017</stddev>\n            </noise>\n          </z>\n        </linear_acceleration>\n      </imu>\n      <plugin name="imu_controller" filename="libgazebo_ros_imu.so">\n        <frame_name>torso_imu_frame</frame_name>\n        <body_name>torso</body_name>\n        <topic>__default_topic__</topic>\n        <serviceName>__default_service_name__</serviceName>\n        <gaussianNoise>0.01</gaussianNoise>\n        <updateRateHZ>100.0</updateRateHZ>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- Force/Torque sensor for foot contact --\x3e\n  <gazebo reference="left_foot">\n    <sensor name="left_foot_ft_sensor" type="force_torque">\n      <always_on>true</always_on>\n      <update_rate>100</update_rate>\n      <force_torque>\n        <frame>child</frame>\n        <measure_direction>child_to_parent</measure_direction>\n      </force_torque>\n      <plugin name="left_foot_ft_plugin" filename="libgazebo_ros_ft_sensor.so">\n        <frame_name>left_foot</frame_name>\n        <topic>left_foot/ft_sensor</topic>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- LiDAR sensor for environment perception --\x3e\n  <gazebo reference="head">\n    <sensor name="head_lidar" type="ray">\n      <always_on>true</always_on>\n      <visualize>false</visualize>\n      <update_rate>10</update_rate>\n      <ray>\n        <scan>\n          <horizontal>\n            <samples>720</samples>\n            <resolution>1</resolution>\n            <min_angle>-1.570796</min_angle> \x3c!-- -90 degrees --\x3e\n            <max_angle>1.570796</max_angle>   \x3c!-- 90 degrees --\x3e\n          </horizontal>\n        </scan>\n        <range>\n          <min>0.1</min>\n          <max>10.0</max>\n          <resolution>0.01</resolution>\n        </range>\n      </ray>\n      <plugin name="head_lidar_controller" filename="libgazebo_ros_laser.so">\n        <frame_name>head_lidar_frame</frame_name>\n        <topic>head_lidar/scan</topic>\n        <gaussianNoise>0.01</gaussianNoise>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n</robot>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"physics-configuration-file",children:"Physics Configuration File"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- config/physics_config.xml --\x3e\n<gazebo>\n  <physics type="ode">\n    <max_step_size>0.001</max_step_size>\n    <real_time_factor>1.0</real_time_factor>\n    <real_time_update_rate>1000.0</real_time_update_rate>\n    <gravity>0 0 -9.8</gravity>\n    <ode>\n      <solver>\n        <type>quick</type>\n        <iters>10</iters>\n        <sor>1.3</sor>\n      </solver>\n      <constraints>\n        <cfm>0.0</cfm>\n        <erp>0.2</erp>\n        <contact_max_correcting_vel>100.0</contact_max_correcting_vel>\n        <contact_surface_layer>0.001</contact_surface_layer>\n      </constraints>\n    </ode>\n  </physics>\n</gazebo>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"sensor-processing-node",children:"Sensor Processing Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# scripts/sensor_processing_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu, JointState, LaserScan\nfrom geometry_msgs.msg import Vector3\nfrom std_msgs.msg import Float64MultiArray\nfrom builtin_interfaces.msg import Time\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\nfrom tf2_ros import TransformBroadcaster\nimport tf_transformations\n\n\nclass SensorProcessor(Node):\n    def __init__(self):\n        super().__init__(\'sensor_processor\')\n\n        # Declare parameters\n        self.declare_parameter(\'processing_frequency\', 100.0)\n        self.declare_parameter(\'image_processing_enabled\', True)\n        self.declare_parameter(\'imu_calibration_enabled\', True)\n\n        # Get parameters\n        self.processing_frequency = self.get_parameter(\'processing_frequency\').value\n        self.image_processing_enabled = self.get_parameter(\'image_processing_enabled\').value\n        self.imu_calibration_enabled = self.get_parameter(\'imu_calibration_enabled\').value\n\n        # Publishers\n        self.processed_sensor_pub = self.create_publisher(\n            Float64MultiArray,\n            \'processed_sensors\',\n            10\n        )\n\n        # Subscribers\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        self.joint_state_sub = self.create_subscription(\n            JointState,\n            \'joint_states\',\n            self.joint_state_callback,\n            10\n        )\n\n        if self.image_processing_enabled:\n            self.image_sub = self.create_subscription(\n                Image,\n                \'camera/image_raw\',\n                self.image_callback,\n                10\n            )\n\n        self.lidar_sub = self.create_subscription(\n            LaserScan,\n            \'scan\',\n            self.lidar_callback,\n            10\n        )\n\n        # Timer for processing loop\n        self.processing_timer = self.create_timer(\n            1.0/self.processing_frequency,\n            self.processing_loop\n        )\n\n        # Internal state\n        self.current_imu = Imu()\n        self.current_joint_states = JointState()\n        self.current_lidar = LaserScan()\n        self.cv_bridge = CvBridge()\n        self.last_image_time = Time()\n        self.balance_estimator = BalanceEstimator()\n\n        self.get_logger().info(\'Sensor Processor initialized\')\n\n    def imu_callback(self, msg):\n        """Process IMU data"""\n        self.current_imu = msg\n\n        # Apply calibration if enabled\n        if self.imu_calibration_enabled:\n            self.current_imu = self.calibrate_imu(msg)\n\n    def joint_state_callback(self, msg):\n        """Process joint state data"""\n        self.current_joint_states = msg\n\n    def image_callback(self, msg):\n        """Process camera image data"""\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Perform basic image processing\n            processed_data = self.process_image(cv_image)\n\n            self.get_logger().debug(f\'Processed image: {processed_data}\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {str(e)}\')\n\n    def lidar_callback(self, msg):\n        """Process LiDAR scan data"""\n        self.current_lidar = msg\n\n        # Process scan for obstacles\n        obstacles = self.detect_obstacles(msg)\n        self.get_logger().debug(f\'Detected {len(obstacles)} obstacles\')\n\n    def processing_loop(self):\n        """Main processing loop"""\n        # Estimate balance state from sensor data\n        balance_state = self.balance_estimator.estimate_balance(\n            self.current_imu,\n            self.current_joint_states\n        )\n\n        # Process sensor fusion\n        fused_data = self.fuse_sensors(\n            self.current_imu,\n            self.current_joint_states,\n            self.current_lidar\n        )\n\n        # Publish processed data\n        processed_msg = Float64MultiArray()\n        processed_msg.data = fused_data\n        self.processed_sensor_pub.publish(processed_msg)\n\n        self.get_logger().debug(f\'Published processed sensor data: {len(fused_data)} values\')\n\n    def calibrate_imu(self, raw_imu):\n        """Apply IMU calibration"""\n        calibrated_imu = Imu()\n        calibrated_imu.header = raw_imu.header\n\n        # Apply bias corrections (these would be calibrated values)\n        bias_offset = Vector3(x=0.01, y=-0.02, z=0.005)\n\n        calibrated_imu.linear_acceleration.x = raw_imu.linear_acceleration.x + bias_offset.x\n        calibrated_imu.linear_acceleration.y = raw_imu.linear_acceleration.y + bias_offset.y\n        calibrated_imu.linear_acceleration.z = raw_imu.linear_acceleration.z + bias_offset.z\n\n        calibrated_imu.angular_velocity.x = raw_imu.angular_velocity.x\n        calibrated_imu.angular_velocity.y = raw_imu.angular_velocity.y\n        calibrated_imu.angular_velocity.z = raw_imu.angular_velocity.z\n\n        calibrated_imu.orientation = raw_imu.orientation\n\n        return calibrated_imu\n\n    def process_image(self, cv_image):\n        """Perform basic image processing"""\n        # Convert to grayscale\n        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n\n        # Apply Gaussian blur\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # Perform edge detection\n        edges = cv2.Canny(blurred, 50, 150)\n\n        # Find contours\n        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Return count of contours as simple processed data\n        return [len(contours)]\n\n    def detect_obstacles(self, scan_msg):\n        """Detect obstacles from LiDAR scan"""\n        obstacles = []\n        for i, range_val in enumerate(scan_msg.ranges):\n            if scan_msg.range_min <= range_val <= scan_msg.range_max:\n                angle = scan_msg.angle_min + i * scan_msg.angle_increment\n                # Check if obstacle is close enough to be significant\n                if range_val < 1.0:  # 1 meter threshold\n                    obstacles.append((angle, range_val))\n        return obstacles\n\n    def fuse_sensors(self, imu_data, joint_data, lidar_data):\n        """Simple sensor fusion"""\n        fused_data = []\n\n        # IMU data\n        fused_data.append(imu_data.linear_acceleration.x)\n        fused_data.append(imu_data.linear_acceleration.y)\n        fused_data.append(imu_data.linear_acceleration.z)\n\n        # Joint positions\n        for pos in joint_data.position:\n            fused_data.append(pos)\n\n        # LiDAR data (first few ranges as example)\n        if len(lidar_data.ranges) > 0:\n            for i in range(min(10, len(lidar_data.ranges))):\n                fused_data.append(lidar_data.ranges[i])\n\n        return fused_data\n\n\nclass BalanceEstimator:\n    """Estimate robot balance state from sensor data"""\n\n    def __init__(self):\n        self.com_estimator = CenterOfMassEstimator()\n        self.stability_threshold = 0.1  # meters\n\n    def estimate_balance(self, imu_data, joint_data):\n        """Estimate balance based on IMU and joint data"""\n        # Calculate center of mass position\n        com_pos = self.com_estimator.calculate_com(joint_data.position)\n\n        # Get orientation from IMU\n        orientation = imu_data.orientation\n        euler = tf_transformations.euler_from_quaternion([\n            orientation.x,\n            orientation.y,\n            orientation.z,\n            orientation.w\n        ])\n\n        # Calculate stability metrics\n        roll, pitch, yaw = euler\n        stability = np.sqrt(roll**2 + pitch**2)\n\n        return {\n            \'com_x\': com_pos[0],\n            \'com_y\': com_pos[1],\n            \'com_z\': com_pos[2],\n            \'roll\': roll,\n            \'pitch\': pitch,\n            \'yaw\': yaw,\n            \'stability\': stability,\n            \'is_stable\': stability < self.stability_threshold\n        }\n\n\nclass CenterOfMassEstimator:\n    """Calculate center of mass based on joint positions"""\n\n    def __init__(self):\n        # Mass properties for each link (simplified)\n        self.link_masses = {\n            \'base_link\': 15.0,\n            \'torso\': 8.0,\n            \'head\': 2.0,\n            \'left_hip\': 3.0,\n            \'left_knee\': 2.5,\n            \'left_ankle\': 1.5,\n            \'left_foot\': 1.0\n        }\n\n    def calculate_com(self, joint_positions):\n        """Calculate center of mass (simplified)"""\n        # This is a simplified calculation - in reality, you\'d need\n        # the full kinematic chain and link positions\n        total_mass = sum(self.link_masses.values())\n\n        # For demonstration, return a simple calculation\n        # In practice, you\'d use forward kinematics\n        com_x = 0.0\n        com_y = 0.0\n        com_z = 0.8  # Approximate CoM height for humanoid\n\n        return [com_x, com_y, com_z]\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    processor = SensorProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        processor.get_logger().info(\'Shutting down sensor processor...\')\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"hardwaregpu-notes",children:"Hardware/GPU Notes"}),"\n",(0,a.jsx)(n.p,{children:"Physics simulation and sensor modeling for humanoid robotics have specific hardware requirements:"}),"\n",(0,a.jsx)(n.h3,{id:"physics-simulation-requirements",children:"Physics Simulation Requirements"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"CPU"}),": Multi-core processor (8+ cores recommended) for real-time physics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory"}),": 16GB+ for complex humanoid models with multiple contacts"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Physics Engine"}),": ODE, Bullet, or DART with appropriate tuning"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Real-time Kernel"}),": Consider PREEMPT_RT for deterministic physics"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"sensor-simulation-requirements",children:"Sensor Simulation Requirements"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"GPU"}),": Modern graphics card with CUDA support for camera simulation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"VRAM"}),": 8GB+ for detailed camera and LiDAR simulation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory Bandwidth"}),": High bandwidth for sensor data processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Processing Power"}),": Real-time sensor data generation and processing"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Use simplified collision meshes for physics (keep detailed visuals separate)"}),"\n",(0,a.jsx)(n.li,{children:"Limit sensor update rates to realistic values"}),"\n",(0,a.jsx)(n.li,{children:"Optimize contact parameters for stability"}),"\n",(0,a.jsx)(n.li,{children:"Use appropriate solver parameters for humanoid dynamics"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"simulation-path",children:"Simulation Path"}),"\n",(0,a.jsx)(n.p,{children:"For developing realistic physics and sensor simulation for humanoid robotics:"}),"\n",(0,a.jsx)(n.h3,{id:"physics-setup",children:"Physics Setup"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Configure physics engine parameters for humanoid dynamics"}),"\n",(0,a.jsx)(n.li,{children:"Set realistic mass, inertia, and friction properties"}),"\n",(0,a.jsx)(n.li,{children:"Optimize solver parameters for stability"}),"\n",(0,a.jsx)(n.li,{children:"Test single-joint physics before complex models"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"sensor-configuration",children:"Sensor Configuration"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Model real sensor characteristics and noise"}),"\n",(0,a.jsx)(n.li,{children:"Configure realistic update rates and ranges"}),"\n",(0,a.jsx)(n.li,{children:"Validate sensor data against hardware specifications"}),"\n",(0,a.jsx)(n.li,{children:"Test sensor fusion algorithms in simulation"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"validation-process",children:"Validation Process"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Test physics stability with complex humanoid models"}),"\n",(0,a.jsx)(n.li,{children:"Validate sensor data realism and noise characteristics"}),"\n",(0,a.jsx)(n.li,{children:"Compare simulation vs. real-world sensor data"}),"\n",(0,a.jsx)(n.li,{children:"Adjust parameters for better fidelity"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"real-world-path",children:"Real-World Path"}),"\n",(0,a.jsx)(n.p,{children:"Transitioning from simulation to real hardware:"}),"\n",(0,a.jsx)(n.h3,{id:"physics-validation",children:"Physics Validation"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Compare simulation and real-world dynamics"}),"\n",(0,a.jsx)(n.li,{children:"Validate contact forces and friction models"}),"\n",(0,a.jsx)(n.li,{children:"Adjust parameters based on real-world behavior"}),"\n",(0,a.jsx)(n.li,{children:"Document differences for compensation strategies"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"sensor-calibration",children:"Sensor Calibration"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Calibrate simulated sensors to match hardware"}),"\n",(0,a.jsx)(n.li,{children:"Validate noise models against real sensors"}),"\n",(0,a.jsx)(n.li,{children:"Adjust simulation parameters for better match"}),"\n",(0,a.jsx)(n.li,{children:"Test sensor fusion algorithms with both datasets"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Implement safety limits based on simulation results"}),"\n",(0,a.jsx)(n.li,{children:"Validate emergency stop procedures with physics"}),"\n",(0,a.jsx)(n.li,{children:"Test sensor failure scenarios"}),"\n",(0,a.jsx)(n.li,{children:"Ensure safe operation boundaries are respected"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"spec-build-test-checklist",children:"Spec-Build-Test Checklist"}),"\n",(0,a.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Physics parameters are realistic for humanoid dynamics"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Mass and inertia properties match real hardware"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Friction and contact parameters are properly configured"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Sensor noise models match real hardware characteristics"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Update rates are realistic for each sensor type"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Physics simulation runs at real-time speed"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Collision detection works properly for all links"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Sensor data is published at expected rates"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Joint limits and constraints are properly enforced"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Solver parameters provide stable simulation"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Multi-body dynamics behave realistically"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Sensor fusion algorithms work with simulated data"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Emergency stop procedures work with physics simulation"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Performance metrics are monitored during simulation"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"apa-citations",children:"APA Citations"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Koenig, N., & Howard, A. (2004). Design and use paradigms for Gazebo, an open-source multi-robot simulator. ",(0,a.jsx)(n.em,{children:"IEEE/RSJ International Conference on Intelligent Robots and Systems"}),", 2350-2354."]}),"\n",(0,a.jsxs)(n.li,{children:["Tedrake, R., Jackowski, Z., Miller, R., Murphey, J., & Erez, T. (2010). Using system identification to obtain reliable models for legged robots. ",(0,a.jsx)(n.em,{children:"IEEE/RSJ International Conference on Intelligent Robots and Systems"}),", 1417-1423."]}),"\n",(0,a.jsxs)(n.li,{children:["Coumans, E., & Bai, Y. (2016). Mujoco: A physics engine for model-based control. ",(0,a.jsx)(n.em,{children:"IEEE International Conference on Robotics and Automation"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["Murai, R., & Kyrki, V. (2021). Simulation to reality transfer in robotics: A survey. ",(0,a.jsx)(n.em,{children:"IEEE Access"}),", 9, 142395-142418."]}),"\n",(0,a.jsxs)(n.li,{children:["Sadeghi, F., & Levine, S. (2017). CAD2RL: Real single-image flight without a single real image. ",(0,a.jsx)(n.em,{children:"Proceedings of the International Conference on Robotics and Automation"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["Xie, W., Tan, J., & Turk, G. (2020). Learning dexterous manipulation from random grasps. ",(0,a.jsx)(n.em,{children:"IEEE Robotics and Automation Letters"}),", 5(2), 2810-2817."]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(6540);const a={},r=s.createContext(a);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);