"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[8439],{3136:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vla/examples/index","title":"Module 4 Examples: VLA Robotics Code Samples","description":"Example code implementations for Vision-Language-Action robotics systems","source":"@site/docs/module-4-vla/examples/index.md","sourceDirName":"module-4-vla/examples","slug":"/module-4-vla/examples/","permalink":"/hackathon-book-robotics/fr/docs/module-4-vla/examples/","draft":false,"unlisted":false,"editUrl":"https://github.com/noormusarrat/hackathon-book-robotics/edit/main/docs/module-4-vla/examples/index.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9,"title":"Module 4 Examples: VLA Robotics Code Samples","description":"Example code implementations for Vision-Language-Action robotics systems"},"sidebar":"tutorialSidebar","previous":{"title":"Module 4 Exercises: Vision-Language-Action Robotics","permalink":"/hackathon-book-robotics/fr/docs/module-4-vla/exercises/"}}');var a=s(4848),i=s(8453);const r={sidebar_position:9,title:"Module 4 Examples: VLA Robotics Code Samples",description:"Example code implementations for Vision-Language-Action robotics systems"},o="Module 4 Examples: VLA Robotics Code Samples",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Example 1: Basic Voice Command Node",id:"example-1-basic-voice-command-node",level:2},{value:"File: <code>voice_command_node.py</code>",id:"file-voice_command_nodepy",level:3},{value:"Setup and Running",id:"setup-and-running",level:3},{value:"Example 2: Natural Language Understanding with Pattern Matching",id:"example-2-natural-language-understanding-with-pattern-matching",level:2},{value:"File: <code>nlu_pattern_matcher.py</code>",id:"file-nlu_pattern_matcherpy",level:3},{value:"Example 3: Voice-to-Action Pipeline Integration",id:"example-3-voice-to-action-pipeline-integration",level:2},{value:"File: <code>vla_pipeline.py</code>",id:"file-vla_pipelinepy",level:3},{value:"Example 4: Human-Robot Interaction Manager",id:"example-4-human-robot-interaction-manager",level:2},{value:"File: <code>hri_manager.py</code>",id:"file-hri_managerpy",level:3},{value:"Example 5: Complete VLA System Launch File",id:"example-5-complete-vla-system-launch-file",level:2},{value:"File: <code>vla_system.launch.py</code>",id:"file-vla_systemlaunchpy",level:3},{value:"Example 6: Configuration File",id:"example-6-configuration-file",level:2},{value:"File: <code>vla_config.yaml</code>",id:"file-vla_configyaml",level:3},{value:"Running the Examples",id:"running-the-examples",level:2},{value:"Important Notes",id:"important-notes",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"module-4-examples-vla-robotics-code-samples",children:"Module 4 Examples: VLA Robotics Code Samples"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"This section contains complete, runnable code examples that demonstrate the implementation of Vision-Language-Action (VLA) robotics systems. Each example builds upon the concepts covered in the module chapters and provides practical implementations that can be used as starting points for your own projects."}),"\n",(0,a.jsx)(n.h2,{id:"example-1-basic-voice-command-node",children:"Example 1: Basic Voice Command Node"}),"\n",(0,a.jsx)(n.p,{children:"This example demonstrates a simple voice command system that listens for audio input and publishes movement commands."}),"\n",(0,a.jsxs)(n.h3,{id:"file-voice_command_nodepy",children:["File: ",(0,a.jsx)(n.code,{children:"voice_command_node.py"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport speech_recognition as sr\nimport threading\n\nclass VoiceCommandNode(Node):\n    def __init__(self):\n        super().__init__('voice_command_node')\n\n        # Create publisher for movement commands\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n\n        # Initialize speech recognizer\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Adjust for ambient noise\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        # Start voice listening thread\n        self.listening = True\n        self.voice_thread = threading.Thread(target=self.listen_for_commands)\n        self.voice_thread.daemon = True\n        self.voice_thread.start()\n\n        self.get_logger().info('Voice Command Node initialized')\n\n    def listen_for_commands(self):\n        \"\"\"Listen for voice commands in a separate thread\"\"\"\n        with self.microphone as source:\n            while self.listening:\n                try:\n                    self.get_logger().info('Listening for commands...')\n                    audio = self.recognizer.listen(source, timeout=1.0, phrase_time_limit=3.0)\n\n                    # Recognize speech\n                    command = self.recognizer.recognize_google(audio).lower()\n                    self.get_logger().info(f'Heard: {command}')\n\n                    # Process command\n                    self.process_command(command)\n\n                except sr.WaitTimeoutError:\n                    # No speech detected, continue listening\n                    pass\n                except sr.UnknownValueError:\n                    self.get_logger().info('Could not understand audio')\n                except sr.RequestError as e:\n                    self.get_logger().error(f'Error: {e}')\n\n    def process_command(self, command):\n        \"\"\"Process recognized voice command\"\"\"\n        twist = Twist()\n\n        if 'forward' in command or 'ahead' in command:\n            twist.linear.x = 0.5\n            self.get_logger().info('Moving forward')\n        elif 'backward' in command or 'back' in command:\n            twist.linear.x = -0.5\n            self.get_logger().info('Moving backward')\n        elif 'left' in command:\n            twist.angular.z = 0.5\n            self.get_logger().info('Turning left')\n        elif 'right' in command:\n            twist.angular.z = -0.5\n            self.get_logger().info('Turning right')\n        elif 'stop' in command or 'halt' in command:\n            # All velocities remain zero\n            self.get_logger().info('Stopping')\n        else:\n            self.get_logger().info(f'Unknown command: {command}')\n            return\n\n        # Publish the command\n        self.cmd_vel_pub.publish(twist)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceCommandNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down voice command node')\n        node.listening = False\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"setup-and-running",children:"Setup and Running"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Install required packages:"}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip3 install speechrecognition pyaudio\n"})}),"\n",(0,a.jsxs)(n.ol,{start:"2",children:["\n",(0,a.jsx)(n.li,{children:"Run the node:"}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"ros2 run your_package voice_command_node.py\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"example-2-natural-language-understanding-with-pattern-matching",children:"Example 2: Natural Language Understanding with Pattern Matching"}),"\n",(0,a.jsx)(n.p,{children:"This example shows how to implement basic natural language understanding using pattern matching."}),"\n",(0,a.jsxs)(n.h3,{id:"file-nlu_pattern_matcherpy",children:["File: ",(0,a.jsx)(n.code,{children:"nlu_pattern_matcher.py"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport re\nimport json\n\nclass NLUProcessor(Node):\n    def __init__(self):\n        super().__init__('nlu_processor')\n\n        # Subscribe to voice commands\n        self.command_sub = self.create_subscription(\n            String, 'voice_command', self.command_callback, 10\n        )\n\n        # Publish structured commands\n        self.structured_pub = self.create_publisher(\n            String, 'structured_command', 10\n        )\n\n        # Define patterns for different intents\n        self.patterns = {\n            'navigation': [\n                (r'go\\s+(?P<direction>forward|backward|left|right)', self.handle_navigation),\n                (r'move\\s+(?P<direction>forward|backward|left|right)', self.handle_navigation),\n                (r'go\\s+to\\s+(?P<location>\\w+)', self.handle_location_navigation),\n            ],\n            'manipulation': [\n                (r'pick\\s+up\\s+(?P<object>.+)', self.handle_pick_up),\n                (r'grab\\s+(?P<object>.+)', self.handle_pick_up),\n                (r'put\\s+(?P<object>.+)\\s+down', self.handle_place),\n                (r'place\\s+(?P<object>.+)', self.handle_place),\n            ],\n            'information': [\n                (r'what.*time', self.handle_time_request),\n                (r'hello|hi|hey', self.handle_greeting),\n            ]\n        }\n\n        self.get_logger().info('NLU Processor initialized')\n\n    def command_callback(self, msg):\n        \"\"\"Process incoming voice command\"\"\"\n        command = msg.data.lower()\n        self.get_logger().info(f'Processing: {command}')\n\n        # Try to match patterns\n        for intent, pattern_list in self.patterns.items():\n            for pattern, handler in pattern_list:\n                match = re.search(pattern, command)\n                if match:\n                    result = handler(match)\n                    if result:\n                        # Publish structured command\n                        structured_msg = String()\n                        structured_msg.data = json.dumps(result)\n                        self.structured_pub.publish(structured_msg)\n                        return\n\n        # If no pattern matches, publish unknown command\n        unknown_msg = String()\n        unknown_msg.data = json.dumps({\n            'intent': 'unknown',\n            'original_command': command,\n            'confidence': 0.0\n        })\n        self.structured_pub.publish(unknown_msg)\n\n    def handle_navigation(self, match):\n        \"\"\"Handle navigation commands\"\"\"\n        direction = match.group('direction')\n        return {\n            'intent': 'navigation',\n            'action': 'move_directionally',\n            'parameters': {'direction': direction},\n            'confidence': 0.9\n        }\n\n    def handle_location_navigation(self, match):\n        \"\"\"Handle navigation to specific location\"\"\"\n        location = match.group('location')\n        return {\n            'intent': 'navigation',\n            'action': 'move_to_location',\n            'parameters': {'location': location},\n            'confidence': 0.8\n        }\n\n    def handle_pick_up(self, match):\n        \"\"\"Handle pick up commands\"\"\"\n        obj = match.group('object').strip()\n        return {\n            'intent': 'manipulation',\n            'action': 'pick_up',\n            'parameters': {'object': obj},\n            'confidence': 0.85\n        }\n\n    def handle_place(self, match):\n        \"\"\"Handle place commands\"\"\"\n        obj = match.group('object').strip()\n        return {\n            'intent': 'manipulation',\n            'action': 'place',\n            'parameters': {'object': obj},\n            'confidence': 0.85\n        }\n\n    def handle_time_request(self, match):\n        \"\"\"Handle time requests\"\"\"\n        from datetime import datetime\n        current_time = datetime.now().strftime(\"%H:%M\")\n        return {\n            'intent': 'information',\n            'action': 'speak',\n            'parameters': {'text': f'The current time is {current_time}'},\n            'confidence': 0.95\n        }\n\n    def handle_greeting(self, match):\n        \"\"\"Handle greetings\"\"\"\n        return {\n            'intent': 'information',\n            'action': 'speak',\n            'parameters': {'text': 'Hello! How can I help you today?'},\n            'confidence': 0.95\n        }\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = NLUProcessor()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"example-3-voice-to-action-pipeline-integration",children:"Example 3: Voice-to-Action Pipeline Integration"}),"\n",(0,a.jsx)(n.p,{children:"This example demonstrates how to integrate voice processing, NLU, and action execution."}),"\n",(0,a.jsxs)(n.h3,{id:"file-vla_pipelinepy",children:["File: ",(0,a.jsx)(n.code,{children:"vla_pipeline.py"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import Image\nimport json\nimport time\nfrom enum import Enum\n\nclass PipelineState(Enum):\n    IDLE = 1\n    PROCESSING = 2\n    EXECUTING = 3\n    ERROR = 4\n\nclass VLAPipeline(Node):\n    def __init__(self):\n        super().__init__('vla_pipeline')\n\n        # Initialize pipeline state\n        self.state = PipelineState.IDLE\n\n        # Publishers and subscribers\n        self.voice_sub = self.create_subscription(\n            String, 'voice_command', self.voice_callback, 10\n        )\n        self.nlu_sub = self.create_subscription(\n            String, 'structured_command', self.nlu_callback, 10\n        )\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.feedback_pub = self.create_publisher(String, 'pipeline_feedback', 10)\n\n        # Action execution timer\n        self.action_timer = None\n        self.action_start_time = 0\n        self.action_duration = 0\n\n        self.get_logger().info('VLA Pipeline initialized')\n\n    def voice_callback(self, msg):\n        \"\"\"Handle voice commands\"\"\"\n        if self.state == PipelineState.IDLE:\n            self.state = PipelineState.PROCESSING\n            self.get_logger().info(f'Received voice command: {msg.data}')\n\n            # Publish for NLU processing\n            # In a real system, this would be handled by the NLU node\n            self.process_command_locally(msg.data)\n\n    def nlu_callback(self, msg):\n        \"\"\"Handle structured commands from NLU\"\"\"\n        try:\n            command_data = json.loads(msg.data)\n            self.execute_action(command_data)\n        except json.JSONDecodeError:\n            self.get_logger().error('Invalid JSON in structured command')\n\n    def process_command_locally(self, command_text):\n        \"\"\"Local processing of command (for demonstration)\"\"\"\n        # Simple command processing - in practice, this would call the NLU system\n        command_data = self.simple_nlu(command_text)\n        if command_data:\n            self.execute_action(command_data)\n\n    def simple_nlu(self, command):\n        \"\"\"Simple natural language understanding for demo\"\"\"\n        command_lower = command.lower()\n\n        if 'forward' in command_lower or 'ahead' in command_lower:\n            return {\n                'intent': 'navigation',\n                'action': 'move_linear',\n                'parameters': {'direction': 'forward', 'duration': 2.0, 'speed': 0.5}\n            }\n        elif 'backward' in command_lower or 'back' in command_lower:\n            return {\n                'intent': 'navigation',\n                'action': 'move_linear',\n                'parameters': {'direction': 'backward', 'duration': 2.0, 'speed': 0.5}\n            }\n        elif 'turn left' in command_lower or 'left' in command_lower:\n            return {\n                'intent': 'navigation',\n                'action': 'turn',\n                'parameters': {'direction': 'left', 'angle': 90, 'speed': 0.5}\n            }\n        elif 'turn right' in command_lower or 'right' in command_lower:\n            return {\n                'intent': 'navigation',\n                'action': 'turn',\n                'parameters': {'direction': 'right', 'angle': 90, 'speed': 0.5}\n            }\n        else:\n            return {\n                'intent': 'unknown',\n                'action': 'none',\n                'parameters': {}\n            }\n\n    def execute_action(self, command_data):\n        \"\"\"Execute the planned action\"\"\"\n        self.state = PipelineState.EXECUTING\n        intent = command_data.get('intent', 'unknown')\n\n        if intent == 'navigation':\n            action = command_data.get('action', '')\n            params = command_data.get('parameters', {})\n\n            if action == 'move_linear':\n                self.execute_linear_movement(params)\n            elif action == 'turn':\n                self.execute_turn(params)\n        elif intent == 'unknown':\n            self.get_logger().info('Unknown command, cannot execute')\n            self.state = PipelineState.IDLE\n            return\n        else:\n            self.get_logger().info(f'Intent {intent} not implemented yet')\n            self.state = PipelineState.IDLE\n            return\n\n        # Provide feedback\n        feedback_msg = String()\n        feedback_msg.data = f\"Executing: {command_data.get('action', 'unknown')}\"\n        self.feedback_pub.publish(feedback_msg)\n\n    def execute_linear_movement(self, params):\n        \"\"\"Execute linear movement\"\"\"\n        direction = params.get('direction', 'forward')\n        duration = params.get('duration', 2.0)\n        speed = params.get('speed', 0.5)\n\n        twist = Twist()\n        if direction == 'forward':\n            twist.linear.x = speed\n        else:\n            twist.linear.x = -speed\n\n        self.get_logger().info(f'Moving {direction} at speed {speed} for {duration}s')\n\n        # Publish command and schedule stop\n        self.cmd_vel_pub.publish(twist)\n\n        # Stop after duration\n        timer = self.create_timer(duration, self.stop_movement)\n        self.action_timer = timer\n\n    def execute_turn(self, params):\n        \"\"\"Execute turning movement\"\"\"\n        direction = params.get('direction', 'left')\n        angle = params.get('angle', 90)\n        speed = params.get('speed', 0.5)\n\n        # Convert angle to time (simplified - in reality would use feedback)\n        # Assuming 90 degrees takes 2 seconds at speed 0.5\n        duration = (angle / 90.0) * 2.0\n\n        twist = Twist()\n        if direction == 'left':\n            twist.angular.z = speed\n        else:\n            twist.angular.z = -speed\n\n        self.get_logger().info(f'Turning {direction} for {duration}s')\n\n        # Publish command and schedule stop\n        self.cmd_vel_pub.publish(twist)\n\n        # Stop after duration\n        timer = self.create_timer(duration, self.stop_movement)\n        self.action_timer = timer\n\n    def stop_movement(self):\n        \"\"\"Stop robot movement\"\"\"\n        if self.action_timer:\n            self.action_timer.cancel()\n\n        # Publish zero velocity to stop\n        twist = Twist()\n        self.cmd_vel_pub.publish(twist)\n\n        self.get_logger().info('Movement stopped')\n        self.state = PipelineState.IDLE\n\ndef main(args=None):\n    rclpy.init(args=args)\n    pipeline = VLAPipeline()\n\n    try:\n        rclpy.spin(pipeline)\n    except KeyboardInterrupt:\n        pipeline.get_logger().info('Shutting down VLA pipeline')\n    finally:\n        # Stop any ongoing movement\n        if pipeline.state == PipelineState.EXECUTING:\n            pipeline.stop_movement()\n        pipeline.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"example-4-human-robot-interaction-manager",children:"Example 4: Human-Robot Interaction Manager"}),"\n",(0,a.jsx)(n.p,{children:"This example demonstrates social interaction behaviors and user state management."}),"\n",(0,a.jsxs)(n.h3,{id:"file-hri_managerpy",children:["File: ",(0,a.jsx)(n.code,{children:"hri_manager.py"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom geometry_msgs.msg import Point\nfrom sensor_msgs.msg import LaserScan\nimport math\nfrom enum import Enum\n\nclass UserState(Enum):\n    UNKNOWN = 0\n    APPROACHING = 1\n    ENGAGED = 2\n    INTERACTING = 3\n    LEAVING = 4\n\nclass HRIManager(Node):\n    def __init__(self):\n        super().__init__(\'hri_manager\')\n\n        # User tracking\n        self.users = {}  # Dictionary to track multiple users\n        self.personal_space_radius = 1.0  # meters\n        self.social_space_radius = 2.0    # meters\n\n        # Publishers and subscribers\n        self.laser_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.laser_callback, 10\n        )\n        self.voice_sub = self.create_subscription(\n            String, \'voice_command\', self.voice_callback, 10\n        )\n        self.feedback_pub = self.create_publisher(String, \'hri_feedback\', 10)\n\n        # Timer for user state monitoring\n        self.user_monitor_timer = self.create_timer(0.5, self.monitor_users)\n\n        self.get_logger().info(\'HRI Manager initialized\')\n\n    def laser_callback(self, msg):\n        """Process laser scan to detect users"""\n        # Simple clustering to detect people\n        ranges = msg.ranges\n        angle_increment = msg.angle_increment\n        angle_min = msg.angle_min\n\n        # Convert laser readings to Cartesian coordinates\n        points = []\n        for i, range_val in enumerate(ranges):\n            if not math.isnan(range_val) and range_val < 3.0:  # Only consider points within 3m\n                angle = angle_min + i * angle_increment\n                x = range_val * math.cos(angle)\n                y = range_val * math.sin(angle)\n                points.append(Point(x=x, y=y, z=0.0))\n\n        # Group nearby points into potential users\n        clusters = self.cluster_points(points, threshold=0.5)\n\n        # Update user tracking\n        current_users = {}\n        for i, cluster in enumerate(clusters):\n            # Calculate cluster center\n            avg_x = sum(p.x for p in cluster) / len(cluster)\n            avg_y = sum(p.y for p in cluster) / len(cluster)\n            center = Point(x=avg_x, y=avg_y, z=0.0)\n\n            user_id = f\'user_{i}\'\n            current_users[user_id] = center\n\n        # Update tracked users\n        self.update_tracked_users(current_users)\n\n    def cluster_points(self, points, threshold=0.5):\n        """Simple clustering algorithm to group nearby points"""\n        clusters = []\n\n        for point in points:\n            # Find if this point belongs to an existing cluster\n            assigned = False\n            for cluster in clusters:\n                # Calculate distance to cluster center\n                center_x = sum(p.x for p in cluster) / len(cluster)\n                center_y = sum(p.y for p in cluster) / len(cluster)\n\n                dist = math.sqrt((point.x - center_x)**2 + (point.y - center_y)**2)\n\n                if dist < threshold:\n                    cluster.append(point)\n                    assigned = True\n                    break\n\n            # If not assigned to any cluster, create a new one\n            if not assigned:\n                clusters.append([point])\n\n        return clusters\n\n    def update_tracked_users(self, current_users):\n        """Update tracking information for users"""\n        for user_id, position in current_users.items():\n            # Calculate distance from robot (assuming robot is at origin)\n            distance = math.sqrt(position.x**2 + position.y**2)\n\n            # Determine user state based on distance\n            if distance < self.personal_space_radius:\n                state = UserState.INTERACTING\n            elif distance < self.social_space_radius:\n                state = UserState.ENGAGED\n            else:\n                state = UserState.APPROACHING\n\n            # Update or create user tracking\n            if user_id not in self.users:\n                self.users[user_id] = {\n                    \'position\': position,\n                    \'state\': state,\n                    \'last_seen\': self.get_clock().now().nanoseconds\n                }\n                self.handle_user_appearance(user_id, state)\n            else:\n                old_state = self.users[user_id][\'state\']\n                self.users[user_id][\'position\'] = position\n                self.users[user_id][\'last_seen\'] = self.get_clock().now().nanoseconds\n\n                if old_state != state:\n                    self.handle_user_state_change(user_id, old_state, state)\n\n        # Remove users not seen for a while\n        current_time = self.get_clock().now().nanoseconds\n        users_to_remove = []\n        for user_id, user_data in self.users.items():\n            time_since_seen = (current_time - user_data[\'last_seen\']) / 1e9  # Convert to seconds\n            if time_since_seen > 5.0:  # Remove if not seen for 5 seconds\n                users_to_remove.append(user_id)\n\n        for user_id in users_to_remove:\n            old_state = self.users[user_id][\'state\']\n            del self.users[user_id]\n            self.handle_user_disappearance(user_id, old_state)\n\n    def handle_user_appearance(self, user_id, state):\n        """Handle when a new user appears"""\n        self.get_logger().info(f\'New user detected: {user_id}, state: {state.name}\')\n\n        # Respond based on state\n        if state == UserState.INTERACTING:\n            self.provide_welcome_message(user_id)\n        elif state == UserState.ENGAGED:\n            self.provide_attention_message(user_id)\n\n    def handle_user_state_change(self, user_id, old_state, new_state):\n        """Handle changes in user state"""\n        self.get_logger().info(f\'User {user_id} state changed: {old_state.name} -> {new_state.name}\')\n\n        if old_state == UserState.APPROACHING and new_state == UserState.ENGAGED:\n            self.provide_attention_message(user_id)\n        elif old_state == UserState.ENGAGED and new_state == UserState.INTERACTING:\n            self.provide_interaction_ready_message(user_id)\n        elif old_state == UserState.INTERACTING and new_state in [UserState.ENGAGED, UserState.APPROACHING]:\n            self.provide_interaction_ended_message(user_id)\n\n    def handle_user_disappearance(self, user_id, old_state):\n        """Handle when a user disappears"""\n        self.get_logger().info(f\'User {user_id} disappeared (was {old_state.name})\')\n\n        if old_state == UserState.INTERACTING:\n            self.provide_goodbye_message(user_id)\n\n    def provide_welcome_message(self, user_id):\n        """Provide welcome message when user enters personal space"""\n        feedback_msg = String()\n        feedback_msg.data = f"GREETING: Hello {user_id}! I\'m ready to help. What would you like me to do?"\n        self.feedback_pub.publish(feedback_msg)\n\n    def provide_attention_message(self, user_id):\n        """Provide attention message when user enters social space"""\n        feedback_msg = String()\n        feedback_msg.data = f"ATTENTION: I see you {user_id}. Let me know if you need assistance."\n        self.feedback_pub.publish(feedback_msg)\n\n    def provide_interaction_ready_message(self, user_id):\n        """Provide message when ready for direct interaction"""\n        feedback_msg = String()\n        feedback_msg.data = f"INTERACTION_READY: I\'m ready to talk with you {user_id}. How can I help?"\n        self.feedback_pub.publish(feedback_msg)\n\n    def provide_interaction_ended_message(self, user_id):\n        """Provide message when interaction ends"""\n        feedback_msg = String()\n        feedback_msg.data = f"INTERACTION_ENDED: Thank you for talking with me {user_id}."\n        self.feedback_pub.publish(feedback_msg)\n\n    def provide_goodbye_message(self, user_id):\n        """Provide goodbye message when user leaves"""\n        feedback_msg = String()\n        feedback_msg.data = f"GOODBYE: Goodbye {user_id}. Have a great day!"\n        self.feedback_pub.publish(feedback_msg)\n\n    def voice_callback(self, msg):\n        """Handle voice commands from users"""\n        command = msg.data.lower()\n\n        # Check if any user is in interaction state\n        interacting_users = [uid for uid, data in self.users.items()\n                            if data[\'state\'] == UserState.INTERACTING]\n\n        if interacting_users:\n            # Acknowledge the command\n            feedback_msg = String()\n            feedback_msg.data = f"ACKNOWLEDGMENT: I heard you. Processing command: {command}"\n            self.feedback_pub.publish(feedback_msg)\n\n    def monitor_users(self):\n        """Monitor user states and trigger appropriate behaviors"""\n        # This runs periodically to monitor users\n        active_users = {uid: data for uid, data in self.users.items()\n                       if data[\'state\'] in [UserState.ENGAGED, UserState.INTERACTING]}\n\n        if active_users:\n            self.get_logger().debug(f\'Active users: {len(active_users)}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    hri_manager = HRIManager()\n\n    try:\n        rclpy.spin(hri_manager)\n    except KeyboardInterrupt:\n        hri_manager.get_logger().info(\'Shutting down HRI manager\')\n    finally:\n        hri_manager.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"example-5-complete-vla-system-launch-file",children:"Example 5: Complete VLA System Launch File"}),"\n",(0,a.jsxs)(n.h3,{id:"file-vla_systemlaunchpy",children:["File: ",(0,a.jsx)(n.code,{children:"vla_system.launch.py"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Voice processing node\n        Node(\n            package='your_robot_package',\n            executable='voice_command_node',\n            name='voice_command_node',\n            output='screen',\n            parameters=[\n                # Add any parameters needed\n            ]\n        ),\n\n        # NLU processing node\n        Node(\n            package='your_robot_package',\n            executable='nlu_processor',\n            name='nlu_processor',\n            output='screen'\n        ),\n\n        # VLA pipeline node\n        Node(\n            package='your_robot_package',\n            executable='vla_pipeline',\n            name='vla_pipeline',\n            output='screen'\n        ),\n\n        # HRI manager node\n        Node(\n            package='your_robot_package',\n            executable='hri_manager',\n            name='hri_manager',\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"example-6-configuration-file",children:"Example 6: Configuration File"}),"\n",(0,a.jsxs)(n.h3,{id:"file-vla_configyaml",children:["File: ",(0,a.jsx)(n.code,{children:"vla_config.yaml"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'voice_to_action:\n  voice_processing:\n    sample_rate: 16000\n    chunk_size: 1024\n    vad_aggressiveness: 2\n    energy_threshold: 300\n\n  nlu:\n    confidence_threshold: 0.7\n    max_ambiguity: 0.3\n\n  action_planning:\n    max_planning_time: 5.0\n    enable_validation: true\n\n  hri:\n    personal_space_radius: 1.0\n    social_space_radius: 2.0\n    public_space_radius: 4.0\n\n  pipeline:\n    response_timeout: 10.0\n    enable_feedback: true\n    feedback_language: "en"\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"running-the-examples",children:"Running the Examples"}),"\n",(0,a.jsx)(n.p,{children:"To run these examples:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Set up your ROS 2 workspace:"})}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/vla_examples_ws/src\ncd ~/vla_examples_ws/src\ngit clone <your_robot_package>\ncd ..\ncolcon build\nsource install/setup.bash\n"})}),"\n",(0,a.jsxs)(n.ol,{start:"2",children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Install required Python packages:"})}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip3 install speechrecognition pyaudio\n"})}),"\n",(0,a.jsxs)(n.ol,{start:"3",children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Launch the complete system:"})}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"ros2 launch your_robot_package vla_system.launch.py\n"})}),"\n",(0,a.jsxs)(n.ol,{start:"4",children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Test with voice commands:"})}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Manually publish commands for testing\nros2 topic pub /voice_command std_msgs/String \"data: 'move forward'\"\n"})}),"\n",(0,a.jsx)(n.h2,{id:"important-notes",children:"Important Notes"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"These examples are simplified for educational purposes"}),"\n",(0,a.jsx)(n.li,{children:"In production systems, add proper error handling and safety checks"}),"\n",(0,a.jsx)(n.li,{children:"Consider privacy implications when using voice recognition"}),"\n",(0,a.jsx)(n.li,{children:"Adapt the configuration parameters to your specific robot platform"}),"\n",(0,a.jsx)(n.li,{children:"Test thoroughly in simulation before deploying on physical robots"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"For more advanced implementations, consider integrating with actual speech-to-text APIs, computer vision systems, and proper action planners."})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>o});var t=s(6540);const a={},i=t.createContext(a);function r(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);