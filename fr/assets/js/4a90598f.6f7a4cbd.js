"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[3601],{5301:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>a,contentTitle:()=>t,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4-vla/index","title":"Module 4: VLA Robotics - Language to Action","description":"Building voice-controlled humanoid robots using Vision-Language-Action systems","source":"@site/docs/module-4-vla/index.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/","permalink":"/hackathon-book-robotics/fr/docs/module-4-vla/","draft":false,"unlisted":false,"editUrl":"https://github.com/noormusarrat/hackathon-book-robotics/edit/main/docs/module-4-vla/index.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Module 4: VLA Robotics - Language to Action","description":"Building voice-controlled humanoid robots using Vision-Language-Action systems"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 26: Action Planning from Language","permalink":"/hackathon-book-robotics/fr/docs/module-4-vla/chapter-5"},"next":{"title":"Chapter 27: Voice-to-Action Pipeline Implementation","permalink":"/hackathon-book-robotics/fr/docs/module-4-vla/chapter-6"}}');var r=i(4848),o=i(8453);const l={sidebar_position:5,title:"Module 4: VLA Robotics - Language to Action",description:"Building voice-controlled humanoid robots using Vision-Language-Action systems"},t="Module 4: VLA Robotics - Language to Action",a={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Module Structure",id:"module-structure",level:2},{value:"Real-World Applications",id:"real-world-applications",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2},{value:"Simulation Path",id:"simulation-path",level:2},{value:"Chapter Preview",id:"chapter-preview",level:2},{value:"Hardware Requirements",id:"hardware-requirements-1",level:2},{value:"Minimum Platform: NVIDIA Jetson Orin Nano",id:"minimum-platform-nvidia-jetson-orin-nano",level:3},{value:"Recommended Platform: NVIDIA Jetson Orin NX",id:"recommended-platform-nvidia-jetson-orin-nx",level:3},{value:"Desktop Alternative: RTX 4070 Ti with Edge AI Accelerator",id:"desktop-alternative-rtx-4070-ti-with-edge-ai-accelerator",level:3},{value:"Audio Hardware Requirements",id:"audio-hardware-requirements",level:3},{value:"Additional Peripherals",id:"additional-peripherals",level:3},{value:"Simulation Path Instructions",id:"simulation-path-instructions",level:2},{value:"NVIDIA Isaac Sim Setup",id:"nvidia-isaac-sim-setup",level:3},{value:"Unity Robotics Simulation Setup",id:"unity-robotics-simulation-setup",level:3},{value:"Simulation Development Workflow",id:"simulation-development-workflow",level:3},{value:"Real-World Path Instructions",id:"real-world-path-instructions",level:2},{value:"NVIDIA Jetson Orin Platform Setup",id:"nvidia-jetson-orin-platform-setup",level:3},{value:"Hardware Integration Steps",id:"hardware-integration-steps",level:3},{value:"Deployment Workflow",id:"deployment-workflow",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:3},{value:"Voice-to-Action Implementation Guide with Whisper Integration",id:"voice-to-action-implementation-guide-with-whisper-integration",level:2},{value:"Introduction to Whisper for Robotics",id:"introduction-to-whisper-for-robotics",level:3},{value:"Whisper Installation and Setup",id:"whisper-installation-and-setup",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:3},{value:"Optimizing Whisper for Robotics",id:"optimizing-whisper-for-robotics",level:3},{value:"Testing Whisper Integration",id:"testing-whisper-integration",level:3},{value:"References",id:"references",level:2}];function d(n){const e={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"module-4-vla-robotics---language-to-action",children:"Module 4: VLA Robotics - Language to Action"})}),"\n",(0,r.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(e.p,{children:"Welcome to Module 4, where we'll explore Vision-Language-Action (VLA) robotics systems. This module focuses on building humanoid robots that can understand human language and perform complex physical actions based on verbal commands. You'll learn how to integrate Large Language Models (LLMs) with robotics systems to create conversational robots that can interpret natural language and execute tasks in the physical world."}),"\n",(0,r.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(e.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Understand the fundamentals of Vision-Language-Action robotics"}),"\n",(0,r.jsx)(e.li,{children:"Integrate LLMs with ROS 2 for natural language processing"}),"\n",(0,r.jsx)(e.li,{children:"Implement voice input processing for robotics applications"}),"\n",(0,r.jsx)(e.li,{children:"Design action planning systems that translate language to physical actions"}),"\n",(0,r.jsx)(e.li,{children:"Build conversational interfaces for humanoid robots"}),"\n",(0,r.jsx)(e.li,{children:"Create voice-to-action pipelines with real-time processing"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(e.p,{children:"Before starting this module, you should have:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Completed Module 1 (ROS 2 fundamentals)"}),"\n",(0,r.jsx)(e.li,{children:"Basic understanding of Python programming"}),"\n",(0,r.jsx)(e.li,{children:"Familiarity with machine learning concepts"}),"\n",(0,r.jsx)(e.li,{children:"Experience with simulation environments (covered in Module 2)"}),"\n",(0,r.jsx)(e.li,{children:"Understanding of perception systems (covered in Module 3)"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"module-structure",children:"Module Structure"}),"\n",(0,r.jsx)(e.p,{children:"This module consists of 7 chapters that progressively build your understanding of VLA robotics:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Introduction to Vision-Language-Action Systems"}),"\n",(0,r.jsx)(e.li,{children:"LLM Integration for Robotics"}),"\n",(0,r.jsx)(e.li,{children:"Voice Input Processing"}),"\n",(0,r.jsx)(e.li,{children:"Natural Language Understanding"}),"\n",(0,r.jsx)(e.li,{children:"Action Planning from Language"}),"\n",(0,r.jsx)(e.li,{children:"Voice-to-Action Pipeline Implementation"}),"\n",(0,r.jsx)(e.li,{children:"Human-Robot Interaction"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"real-world-applications",children:"Real-World Applications"}),"\n",(0,r.jsx)(e.p,{children:"Vision-Language-Action robotics has numerous practical applications:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Assistive robots for elderly care"}),"\n",(0,r.jsx)(e.li,{children:"Service robots in hospitality and retail"}),"\n",(0,r.jsx)(e.li,{children:"Industrial cobots that work alongside humans"}),"\n",(0,r.jsx)(e.li,{children:"Educational robots for STEM learning"}),"\n",(0,r.jsx)(e.li,{children:"Search and rescue robots with human guidance"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,r.jsx)(e.p,{children:"For the real-world implementation path:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Minimum"}),": NVIDIA Jetson Orin Nano (8GB RAM)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Recommended"}),": NVIDIA Jetson Orin NX (16GB RAM) or higher"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Desktop Alternative"}),": RTX 4070 Ti with 32GB system RAM"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Microphone"}),": USB or integrated array microphone for voice input"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Speakers"}),": Audio output for voice feedback"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Robot Platform"}),": Compatible humanoid robot or manipulator arm"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"simulation-path",children:"Simulation Path"}),"\n",(0,r.jsx)(e.p,{children:"For the simulation-only approach:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"NVIDIA Isaac Sim (requires 12-24GB VRAM)"}),"\n",(0,r.jsx)(e.li,{children:"Compatible GPU (RTX 4070 Ti or higher recommended)"}),"\n",(0,r.jsx)(e.li,{children:"32GB system RAM minimum"}),"\n",(0,r.jsx)(e.li,{children:"Ubuntu 22.04 LTS with ROS 2 Humble"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"chapter-preview",children:"Chapter Preview"}),"\n",(0,r.jsx)(e.p,{children:"Each chapter follows the 8-section structure:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Why this concept matters for humanoids"})," - The importance of the concept"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Theory"})," - Core principles and concepts"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Implementation"})," - Practical implementation steps"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Hardware/GPU Notes"})," - Specific requirements and considerations"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Simulation Path"})," - How to implement in simulation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Real-World Path"})," - How to implement on real hardware"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Spec-Build-Test checklist"})," - Validation steps"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"APA citations"})," - References and sources"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"hardware-requirements-1",children:"Hardware Requirements"}),"\n",(0,r.jsx)(e.p,{children:"For the real-world implementation of Vision-Language-Action systems:"}),"\n",(0,r.jsx)(e.h3,{id:"minimum-platform-nvidia-jetson-orin-nano",children:"Minimum Platform: NVIDIA Jetson Orin Nano"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"SoC"}),": NVIDIA Orin (12-core ARM v8.2-A CPU)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"GPU"}),": 1024-core NVIDIA Ampere architecture GPU"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Memory"}),": 4GB or 8GB LPDDR5"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Storage"}),": 16GB eMMC 5.1"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Power"}),": 15W-25W (depending on configuration)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Connectivity"}),": Gigabit Ethernet, Wi-Fi 6, Bluetooth 5.2"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"recommended-platform-nvidia-jetson-orin-nx",children:"Recommended Platform: NVIDIA Jetson Orin NX"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"SoC"}),": NVIDIA Orin (16-core ARM v8.2-A CPU)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"GPU"}),": 1024-core NVIDIA Ampere architecture GPU"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Memory"}),": 8GB or 16GB LPDDR5"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Storage"}),": 16GB or 32GB eMMC 5.1"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Power"}),": 25W-40W (depending on configuration)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Additional"}),": CAN bus, SPI, I2C, UART interfaces"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"desktop-alternative-rtx-4070-ti-with-edge-ai-accelerator",children:"Desktop Alternative: RTX 4070 Ti with Edge AI Accelerator"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"GPU"}),": NVIDIA RTX 4070 Ti (12GB VRAM) or higher"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"CPU"}),": Multi-core processor (8+ cores recommended)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Memory"}),": 32GB system RAM minimum, 64GB recommended"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Storage"}),": Fast NVMe SSD (1TB+ recommended)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Audio"}),": High-quality microphone array for voice input"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"audio-hardware-requirements",children:"Audio Hardware Requirements"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Microphone Array"}),": 4-8 microphones for beamforming"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sample Rate"}),": Support for 16kHz or higher"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Audio Processing"}),": Hardware-accelerated audio processing"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Noise Cancellation"}),": Built-in or software-based noise reduction"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"additional-peripherals",children:"Additional Peripherals"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Camera"}),": Global shutter camera with sufficient resolution for computer vision"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Connectivity"}),": Reliable network connection for cloud-based services"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Power Management"}),": Efficient power delivery for mobile platforms"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Thermal Management"}),": Adequate cooling for sustained performance"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"simulation-path-instructions",children:"Simulation Path Instructions"}),"\n",(0,r.jsx)(e.p,{children:"For simulation-based development of VLA systems using NVIDIA Isaac and Unity:"}),"\n",(0,r.jsx)(e.h3,{id:"nvidia-isaac-sim-setup",children:"NVIDIA Isaac Sim Setup"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"System Requirements"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"GPU"}),": NVIDIA RTX 4070 Ti (12GB VRAM) minimum, RTX 4080/4090 (16-24GB VRAM) recommended"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Memory"}),": 32GB system RAM minimum, 64GB recommended"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"OS"}),": Ubuntu 20.04 LTS or 22.04 LTS"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"CUDA"}),": CUDA 11.8 or later"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Isaac Sim"}),": Latest version compatible with your GPU"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Installation"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Download Isaac Sim from NVIDIA Developer website\n# Follow installation instructions from docs.nvidia.com/isaac/\n\n# Verify installation\ncd ~/isaac-sim\npython3 -m omni.isaac.kit --summary-cache-path ./cache\n"})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"ROS 2 Bridge Setup"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Install Isaac ROS packages\nsudo apt update\nsudo apt install ros-humble-isaac-ros-* ros-humble-novatel-octagon-gps-fix-node\n\n# Build the ROS 2 bridge\ncd ~/isaac-sim\npython3 -m pip install -e apps\n"})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"VLA Simulation Environment"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Launch Isaac Sim with a humanoid robot model"}),"\n",(0,r.jsx)(e.li,{children:"Configure audio simulation (if available in your version)"}),"\n",(0,r.jsx)(e.li,{children:"Set up camera sensors for vision processing"}),"\n",(0,r.jsx)(e.li,{children:"Configure microphone simulation for voice input"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"unity-robotics-simulation-setup",children:"Unity Robotics Simulation Setup"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"System Requirements"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"GPU"}),": NVIDIA RTX 4070 Ti or equivalent AMD GPU"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Memory"}),": 32GB RAM minimum"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"OS"}),": Ubuntu 20.04 LTS or Windows 10/11"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Unity"}),": 2022.3 LTS or later"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Unity Robotics Hub"}),": Latest version"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Installation"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Install Unity Hub and Unity Editor 2022.3 LTS\n# Install Unity Robotics packages via Unity Package Manager\n# Install ROS# package for ROS 2 communication\n"})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"ROS-TCP-Connector Setup"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Configure the ROS-TCP-Connector for communication"}),"\n",(0,r.jsx)(e.li,{children:"Set up Unity scene with humanoid robot model"}),"\n",(0,r.jsx)(e.li,{children:"Configure sensors (cameras, microphones if simulating audio)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"VLA Integration in Unity"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Implement voice command simulation through UI or direct input"}),"\n",(0,r.jsx)(e.li,{children:"Create camera feeds for vision processing"}),"\n",(0,r.jsx)(e.li,{children:"Simulate human-robot interaction scenarios"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"simulation-development-workflow",children:"Simulation Development Workflow"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Environment Setup"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Create or import humanoid robot model"}),"\n",(0,r.jsx)(e.li,{children:"Configure physics properties and joint limits"}),"\n",(0,r.jsx)(e.li,{children:"Set up sensor configurations (cameras, LIDAR, etc.)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"VLA System Integration"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Connect simulation to ROS 2 network"}),"\n",(0,r.jsx)(e.li,{children:"Configure sensor data publishing"}),"\n",(0,r.jsx)(e.li,{children:"Implement command execution in simulation"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Testing and Validation"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Test voice command recognition with simulated input"}),"\n",(0,r.jsx)(e.li,{children:"Validate action planning in safe simulation environment"}),"\n",(0,r.jsx)(e.li,{children:"Verify human-robot interaction behaviors"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Performance Optimization"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Optimize simulation for real-time performance"}),"\n",(0,r.jsx)(e.li,{children:"Balance visual quality with computational efficiency"}),"\n",(0,r.jsx)(e.li,{children:"Profile and optimize VLA system performance in simulation"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"real-world-path-instructions",children:"Real-World Path Instructions"}),"\n",(0,r.jsx)(e.p,{children:"For deploying VLA systems on physical hardware platforms:"}),"\n",(0,r.jsx)(e.h3,{id:"nvidia-jetson-orin-platform-setup",children:"NVIDIA Jetson Orin Platform Setup"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Platform Selection"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Minimum"}),": Jetson Orin Nano (8GB) for basic VLA operations"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Recommended"}),": Jetson Orin NX (16GB) for full VLA capabilities"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"High Performance"}),": Jetson AGX Orin for complex reasoning tasks"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"System Preparation"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Flash Jetson with appropriate image\nsudo apt update && sudo apt upgrade -y\n\n# Install ROS 2 Humble\nsudo apt install ros-humble-desktop\nsource /opt/ros/humble/setup.bash\n\n# Install additional dependencies\nsudo apt install python3-pip python3-colcon-common-extensions\npip3 install speechrecognition pyaudio\n"})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Audio Hardware Setup"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Connect microphone array to Jetson platform"}),"\n",(0,r.jsx)(e.li,{children:"Configure audio input settings:"}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Test audio input\narecord -D hw:0,0 -f cd test.wav\n# Play back to verify\naplay test.wav\n"})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Camera Integration"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Connect compatible camera module"}),"\n",(0,r.jsx)(e.li,{children:"Configure camera settings for vision processing"}),"\n",(0,r.jsxs)(e.li,{children:["Test camera feed: ",(0,r.jsx)(e.code,{children:"v4l2-ctl --list-devices"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"hardware-integration-steps",children:"Hardware Integration Steps"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Robot Platform Preparation"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Verify robot has required actuators for planned actions"}),"\n",(0,r.jsx)(e.li,{children:"Ensure proper power distribution for all components"}),"\n",(0,r.jsx)(e.li,{children:"Test all sensors and actuators individually"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"VLA System Installation"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Create workspace\nmkdir -p ~/vla_ws/src\ncd ~/vla_ws\n\n# Clone VLA packages\ngit clone <your-vla-robotic-control-repo>\ncd src/your_robot_package\n\n# Build the workspace\ncd ~/vla_ws\ncolcon build --packages-select vla_robot_control\nsource install/setup.bash\n"})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Sensor Calibration"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Calibrate camera for proper vision processing"}),"\n",(0,r.jsx)(e.li,{children:"Calibrate microphone array for beamforming"}),"\n",(0,r.jsx)(e.li,{children:"Verify coordinate system alignment between sensors"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Safety Configuration"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Implement emergency stop mechanisms"}),"\n",(0,r.jsx)(e.li,{children:"Configure safety limits for all movements"}),"\n",(0,r.jsx)(e.li,{children:"Test all safety systems before full operation"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"deployment-workflow",children:"Deployment Workflow"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Initial Testing"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Test individual components on hardware"}),"\n",(0,r.jsx)(e.li,{children:"Verify sensor data quality"}),"\n",(0,r.jsx)(e.li,{children:"Test basic command execution"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Integration Testing"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Test complete VLA pipeline on hardware"}),"\n",(0,r.jsx)(e.li,{children:"Validate voice recognition in real environment"}),"\n",(0,r.jsx)(e.li,{children:"Test action planning with real robot"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Performance Tuning"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Optimize for real-time performance on hardware"}),"\n",(0,r.jsx)(e.li,{children:"Adjust processing parameters for platform capabilities"}),"\n",(0,r.jsx)(e.li,{children:"Profile and optimize memory usage"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"User Testing"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Conduct real-world user interaction tests"}),"\n",(0,r.jsx)(e.li,{children:"Gather feedback on system performance"}),"\n",(0,r.jsx)(e.li,{children:"Iterate on interaction design based on real usage"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Audio Quality Issues"}),": Check microphone connections, adjust gain settings, implement noise reduction"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Vision Processing Delays"}),": Optimize model size, reduce image resolution, use hardware acceleration"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Action Planning Failures"}),": Verify robot kinematics, check joint limits, validate collision checking"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Communication Issues"}),": Check network connectivity, verify ROS 2 configuration, test topic connections"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"voice-to-action-implementation-guide-with-whisper-integration",children:"Voice-to-Action Implementation Guide with Whisper Integration"}),"\n",(0,r.jsx)(e.h3,{id:"introduction-to-whisper-for-robotics",children:"Introduction to Whisper for Robotics"}),"\n",(0,r.jsx)(e.p,{children:"OpenAI's Whisper model provides state-of-the-art speech recognition capabilities that can be adapted for robotics applications. This guide covers implementing Whisper-based voice command recognition in VLA systems."}),"\n",(0,r.jsx)(e.h3,{id:"whisper-installation-and-setup",children:"Whisper Installation and Setup"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Install Whisper"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"pip install openai-whisper\n# For GPU acceleration\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Model Selection"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"tiny"}),": Fastest, least accurate (suitable for simple commands)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"base"}),": Good balance of speed and accuracy"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"small"}),": Better accuracy, moderate speed"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"medium"}),": High accuracy, slower processing"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"large"}),": Highest accuracy, slowest processing"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# Example Whisper integration node\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Int16\nimport whisper\nimport torch\nimport pyaudio\nimport numpy as np\nimport queue\nimport threading\n\nclass WhisperVoiceNode(Node):\n    def __init__(self):\n        super().__init__(\'whisper_voice_node\')\n\n        # Initialize Whisper model\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n        self.model = whisper.load_model("base").to(self.device)\n\n        # Audio parameters\n        self.rate = 16000\n        self.chunk = 1024\n        self.channels = 1\n        self.format = pyaudio.paInt16\n\n        # Setup audio stream\n        self.audio = pyaudio.PyAudio()\n        self.stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        # Setup publishers and subscribers\n        self.command_pub = self.create_publisher(String, \'/vla/command\', 10)\n        self.audio_queue = queue.Queue()\n\n        # Start audio processing thread\n        self.processing_thread = threading.Thread(target=self.process_audio, daemon=True)\n        self.processing_thread.start()\n\n        self.get_logger().info(\'Whisper Voice Node initialized\')\n\n    def process_audio(self):\n        """Continuously process audio and detect speech"""\n        audio_buffer = np.array([], dtype=np.float32)\n        silence_threshold = 0.01\n        min_speech_duration = 0.5  # seconds\n        max_audio_duration = 10.0  # seconds\n\n        while rclpy.ok():\n            # Read audio data\n            audio_data = self.stream.read(self.chunk, exception_on_overflow=False)\n            audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\n\n            # Add to buffer\n            audio_buffer = np.concatenate([audio_buffer, audio_array])\n\n            # Check if we have enough audio to process\n            if len(audio_buffer) > self.rate * min_speech_duration:\n                # Check if audio is loud enough to be speech\n                if np.max(np.abs(audio_buffer)) > silence_threshold:\n                    # Continue accumulating until silence or max duration\n                    if len(audio_buffer) >= self.rate * max_audio_duration:\n                        self.transcribe_and_publish(audio_buffer)\n                        audio_buffer = np.array([], dtype=np.float32)\n                else:\n                    # If we have accumulated audio and it\'s now silent, transcribe\n                    if len(audio_buffer) >= self.rate * min_speech_duration:\n                        self.transcribe_and_publish(audio_buffer)\n                    audio_buffer = np.array([], dtype=np.float32)\n\n    def transcribe_and_publish(self, audio_buffer):\n        """Transcribe audio buffer and publish as command"""\n        try:\n            # Convert audio to the format expected by Whisper\n            audio_tensor = torch.from_numpy(audio_buffer).to(self.device)\n\n            # Transcribe\n            result = self.model.transcribe(audio_tensor, fp16=False)\n            text = result["text"].strip()\n\n            if text:  # Only publish if we got text\n                self.get_logger().info(f\'Transcribed: {text}\')\n\n                # Publish the transcribed text as a command\n                cmd_msg = String()\n                cmd_msg.data = text\n                self.command_pub.publish(cmd_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Whisper transcription error: {e}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperVoiceNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Shutting down Whisper voice node\')\n    finally:\n        node.stream.stop_stream()\n        node.stream.close()\n        node.audio.terminate()\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(e.h3,{id:"optimizing-whisper-for-robotics",children:"Optimizing Whisper for Robotics"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Performance Optimization"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Use smaller models for real-time applications"}),"\n",(0,r.jsx)(e.li,{children:"Implement audio preprocessing to reduce noise"}),"\n",(0,r.jsx)(e.li,{children:"Use voice activity detection to reduce unnecessary processing"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Customization for Robotics Commands"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Fine-tune Whisper on robotics command vocabulary"}),"\n",(0,r.jsx)(e.li,{children:"Use language model adaptation for domain-specific commands"}),"\n",(0,r.jsx)(e.li,{children:"Implement keyword spotting for wake-word functionality"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Resource Management"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Monitor GPU memory usage during transcription"}),"\n",(0,r.jsx)(e.li,{children:"Implement model unloading when not needed"}),"\n",(0,r.jsx)(e.li,{children:"Use CPU processing if GPU resources are limited"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"testing-whisper-integration",children:"Testing Whisper Integration"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Basic Functionality Test"}),":"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Run the Whisper node\nros2 run your_package whisper_voice_node\n\n# Subscribe to commands to verify transcription\nros2 topic echo /vla/command\n"})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Performance Testing"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Measure transcription latency"}),"\n",(0,r.jsx)(e.li,{children:"Test with various acoustic conditions"}),"\n",(0,r.jsx)(e.li,{children:"Verify accuracy with different speakers"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Integration Testing"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Test end-to-end voice-to-action pipeline"}),"\n",(0,r.jsx)(e.li,{children:"Verify command interpretation accuracy"}),"\n",(0,r.jsx)(e.li,{children:"Test safety and error handling"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(e.p,{children:["Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. ",(0,r.jsx)(e.em,{children:"Advances in Neural Information Processing Systems"}),", 33, 1877-1901."]}),"\n",(0,r.jsxs)(e.p,{children:["Carlin, D., Elsen, E., Ju, S., Datta, A., Nabeshima, H., Neelakantan, A., ... & Vaswani, A. (2022). Whisper: Robust speech recognition via large-scale weak supervision. ",(0,r.jsx)(e.em,{children:"arXiv preprint arXiv:2212.04356"}),"."]}),"\n",(0,r.jsxs)(e.p,{children:["Huang, W., Li, C., Du, Y., Wang, F., Zeng, Z., Wu, Y., & Guibas, L. J. (2022). Language as grounding for 3d object manipulations. ",(0,r.jsx)(e.em,{children:"arXiv preprint arXiv:2209.11684"}),"."]}),"\n",(0,r.jsxs)(e.p,{children:["Knepper, R. A., & Roy, N. (2009). Space-based decomposition planning for mobile manipulation. ",(0,r.jsx)(e.em,{children:"Proceedings of the 2009 IEEE/RSJ International Conference on Intelligent Robots and Systems"}),", 3755-3761."]}),"\n",(0,r.jsxs)(e.p,{children:["Nair, A., Chen, M., Fan, K., Fu, J., Ibarz, J., Ke, K., ... & Kalashnikov, D. (2022). RT-1: Robotics transformer for real-world control at scale. ",(0,r.jsx)(e.em,{children:"arXiv preprint arXiv:2210.08166"}),"."]}),"\n",(0,r.jsxs)(e.p,{children:["Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021). Learning transferable visual models from natural language supervision. ",(0,r.jsx)(e.em,{children:"International Conference on Machine Learning"}),", 8748-8763."]}),"\n",(0,r.jsxs)(e.p,{children:["Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., ... & Sutskever, I. (2022). Zero-shot text-to-image generation. ",(0,r.jsx)(e.em,{children:"arXiv preprint arXiv:2102.12092"}),"."]}),"\n",(0,r.jsxs)(e.p,{children:["Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., ... & Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chat models. ",(0,r.jsx)(e.em,{children:"arXiv preprint arXiv:2307.09288"}),"."]}),"\n",(0,r.jsxs)(e.p,{children:["Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. ",(0,r.jsx)(e.em,{children:"Advances in Neural Information Processing Systems"}),", 30."]}),"\n",(0,r.jsxs)(e.p,{children:["Zhu, Y., Mottaghi, R., Kolve, E., Lim, J. J., Gupta, A., Fei-Fei, L., & Farhadi, A. (2017). Target-driven visual navigation in indoor scenes using deep reinforcement learning. ",(0,r.jsx)(e.em,{children:"Proceedings of the IEEE international conference on robotics and automation"}),", 3357-3364)."]}),"\n",(0,r.jsx)(e.p,{children:"Let's begin with Chapter 1 to explore the fundamentals of Vision-Language-Action systems and their role in humanoid robotics."})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>l,x:()=>t});var s=i(6540);const r={},o=s.createContext(r);function l(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:l(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);