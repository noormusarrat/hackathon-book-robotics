"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[5748],{2314:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>t,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"module-3-isaac/chapter-3","title":"Chapter 17: Perception Pipelines with Isaac","description":"Building perception pipelines using NVIDIA Isaac for humanoid robotics applications","source":"@site/docs/module-3-isaac/chapter-3.md","sourceDirName":"module-3-isaac","slug":"/module-3-isaac/chapter-3","permalink":"/hackathon-book-robotics/fr/docs/module-3-isaac/chapter-3","draft":false,"unlisted":false,"editUrl":"https://github.com/noormusarrat/hackathon-book-robotics/edit/main/docs/module-3-isaac/chapter-3.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Chapter 17: Perception Pipelines with Isaac","description":"Building perception pipelines using NVIDIA Isaac for humanoid robotics applications"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 16: Isaac ROS Integration","permalink":"/hackathon-book-robotics/fr/docs/module-3-isaac/chapter-2"},"next":{"title":"Chapter 18: Computer Vision for Robotics","permalink":"/hackathon-book-robotics/fr/docs/module-3-isaac/chapter-4"}}');var r=i(4848),a=i(8453);const t={sidebar_position:3,title:"Chapter 17: Perception Pipelines with Isaac",description:"Building perception pipelines using NVIDIA Isaac for humanoid robotics applications"},o="Chapter 17: Perception Pipelines with Isaac",c={},l=[{value:"1. Why this concept matters for humanoids",id:"1-why-this-concept-matters-for-humanoids",level:2},{value:"2. Theory",id:"2-theory",level:2},{value:"Isaac Perception Pipeline Architecture",id:"isaac-perception-pipeline-architecture",level:3},{value:"Isaac Perception Processing Graphs",id:"isaac-perception-processing-graphs",level:3},{value:"Hardware Acceleration in Perception Pipelines",id:"hardware-acceleration-in-perception-pipelines",level:3},{value:"3. Implementation",id:"3-implementation",level:2},{value:"4. Hardware/GPU Notes",id:"4-hardwaregpu-notes",level:2},{value:"Perception Pipeline GPU Requirements",id:"perception-pipeline-gpu-requirements",level:3},{value:"Memory Management Strategies",id:"memory-management-strategies",level:3},{value:"Jetson Platform Considerations",id:"jetson-platform-considerations",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"5. Simulation Path",id:"5-simulation-path",level:2},{value:"6. Real-World Path",id:"6-real-world-path",level:2},{value:"7. Spec-Build-Test checklist",id:"7-spec-build-test-checklist",level:2},{value:"8. APA citations",id:"8-apa-citations",level:2}];function p(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-17-perception-pipelines-with-isaac",children:"Chapter 17: Perception Pipelines with Isaac"})}),"\n",(0,r.jsx)(n.h2,{id:"1-why-this-concept-matters-for-humanoids",children:"1. Why this concept matters for humanoids"}),"\n",(0,r.jsx)(n.p,{children:"Perception pipelines are fundamental to humanoid robotics as they enable robots to understand and interact with their environment. For humanoid robots specifically, perception pipelines must process complex, multi-modal sensor data in real-time to enable safe navigation, object manipulation, and human interaction. Isaac's perception pipelines provide hardware-accelerated processing that allows humanoid robots to perform sophisticated perception tasks like real-time object detection, semantic segmentation, and 3D scene understanding. Without robust perception pipelines, humanoid robots would be unable to operate safely in dynamic human environments, limiting their utility for service, assistance, and collaboration tasks. Isaac's GPU-accelerated perception capabilities make it possible to run these computationally intensive algorithms on humanoid robots while maintaining real-time performance."}),"\n",(0,r.jsx)(n.h2,{id:"2-theory",children:"2. Theory"}),"\n",(0,r.jsx)(n.h3,{id:"isaac-perception-pipeline-architecture",children:"Isaac Perception Pipeline Architecture"}),"\n",(0,r.jsx)(n.p,{children:"Isaac's perception pipelines are designed as modular, hardware-accelerated processing chains that can be customized for specific robotic applications. The architecture consists of several interconnected components:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sensor Interface Layer"}),": Handles raw sensor data ingestion from cameras, LIDAR, IMU, and other sensors. This layer includes sensor-specific drivers and data preprocessing modules."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Preprocessing Layer"}),": Performs initial data conditioning such as image rectification, noise reduction, and calibration correction. This layer prepares sensor data for downstream processing."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Feature Extraction Layer"}),": Extracts relevant features from sensor data using classical computer vision algorithms or deep learning models. This includes edge detection, corner detection, and feature matching."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Deep Learning Layer"}),": Applies neural networks for complex perception tasks such as object detection, semantic segmentation, and pose estimation. This layer leverages Isaac's GPU acceleration for real-time inference."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Fusion Layer"}),": Combines information from multiple sensors and processing stages to create a coherent understanding of the environment. This includes sensor fusion and temporal consistency."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Output Layer"}),": Generates structured perception results such as object lists, semantic maps, and tracking information that can be consumed by other robot systems."]}),"\n",(0,r.jsx)(n.h3,{id:"isaac-perception-processing-graphs",children:"Isaac Perception Processing Graphs"}),"\n",(0,r.jsx)(n.p,{children:"Isaac uses processing graphs to define perception pipelines, where nodes represent processing modules and edges represent data flow. This approach allows for:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Modularity"}),": Individual processing modules can be swapped or modified independently"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Parallelization"}),": Multiple processing paths can run simultaneously on different GPU cores"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Optimization"}),": Processing graphs can be optimized for specific hardware configurations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Flexibility"}),": Different perception tasks can share common processing components"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"hardware-acceleration-in-perception-pipelines",children:"Hardware Acceleration in Perception Pipelines"}),"\n",(0,r.jsx)(n.p,{children:"Isaac perception pipelines leverage multiple levels of hardware acceleration:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"GPU Acceleration"}),": CUDA cores handle general-purpose parallel processing for computer vision algorithms\n",(0,r.jsx)(n.strong,{children:"Tensor Cores"}),": Specialized for deep learning inference operations\n",(0,r.jsx)(n.strong,{children:"Hardware Video Processing"}),": Dedicated units for video encoding/decoding and image processing\n",(0,r.jsx)(n.strong,{children:"Memory Bandwidth"}),": Optimized data paths between CPU, GPU, and memory systems"]}),"\n",(0,r.jsx)(n.h2,{id:"3-implementation",children:"3. Implementation"}),"\n",(0,r.jsx)(n.p,{children:"Let's implement a comprehensive Isaac perception pipeline for humanoid robotics:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# isaac_humanoid_perception/isaac_humanoid_perception/perception_pipeline.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2, Imu, LaserScan\nfrom geometry_msgs.msg import PointStamped, PoseStamped\nfrom visualization_msgs.msg import MarkerArray\nfrom std_msgs.msg import Header, Bool\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport threading\nfrom typing import Dict, Any, Optional, List, Tuple\nimport time\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass PerceptionMode(Enum):\n    """Perception pipeline operating modes"""\n    OBJECT_DETECTION = "object_detection"\n    SEMANTIC_SEGMENTATION = "semantic_segmentation"\n    POSE_ESTIMATION = "pose_estimation"\n    FEATURE_TRACKING = "feature_tracking"\n    SLAM = "slam"\n\n@dataclass\nclass PerceptionResult:\n    """Data structure for perception results"""\n    timestamp: float\n    objects: List[Dict[str, Any]]\n    features: List[Tuple[float, float]]  # (x, y) coordinates\n    pose: Optional[PoseStamped]\n    point_cloud: Optional[np.ndarray]\n    confidence: float\n\nclass IsaacPerceptionPipeline(Node):\n    """\n    Isaac perception pipeline for humanoid robotics\n    """\n    def __init__(self):\n        super().__init__(\'isaac_perception_pipeline\')\n\n        # Initialize components\n        self.bridge = CvBridge()\n        self.pipeline_lock = threading.Lock()\n        self.perception_mode = PerceptionMode.OBJECT_DETECTION\n        self.camera_matrix = None\n        self.distortion_coeffs = None\n\n        # Sensor data storage\n        self.latest_images = {}\n        self.latest_point_cloud = None\n        self.latest_imu_data = None\n\n        # Configuration parameters\n        self.object_detection_threshold = 0.7\n        self.feature_tracking_enabled = True\n        self.semantic_segmentation_enabled = False\n\n        # Publishers for perception results\n        self.object_pub = self.create_publisher(MarkerArray, \'/isaac/perception/objects\', 10)\n        self.feature_pub = self.create_publisher(MarkerArray, \'/isaac/perception/features\', 10)\n        self.pose_pub = self.create_publisher(PoseStamped, \'/isaac/perception/pose\', 10)\n        self.status_pub = self.create_publisher(Bool, \'/isaac/perception/ready\', 10)\n\n        # Subscribers for sensor data\n        self.rgb_sub = self.create_subscription(\n            Image, \'/camera/rgb/image_raw\', self.rgb_callback, 10\n        )\n        self.depth_sub = self.create_subscription(\n            Image, \'/camera/depth/image_raw\', self.depth_callback, 10\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, \'/camera/rgb/camera_info\', self.camera_info_callback, 10\n        )\n        self.imu_sub = self.create_subscription(\n            Imu, \'/imu/data\', self.imu_callback, 10\n        )\n        self.lidar_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.lidar_callback, 10\n        )\n\n        # Timer for perception processing\n        self.pipeline_timer = self.create_timer(0.1, self.process_perception_pipeline)\n\n        # Initialize perception components\n        self.initialize_perception_components()\n\n        self.get_logger().info(\'Isaac Perception Pipeline initialized\')\n\n    def camera_info_callback(self, msg):\n        """Store camera calibration parameters"""\n        with self.pipeline_lock:\n            self.camera_matrix = np.array(msg.k).reshape(3, 3)\n            self.distortion_coeffs = np.array(msg.d)\n\n    def rgb_callback(self, msg):\n        """Process RGB camera data"""\n        with self.pipeline_lock:\n            try:\n                cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n                self.latest_images[\'rgb\'] = {\n                    \'image\': cv_image,\n                    \'timestamp\': msg.header.stamp,\n                    \'encoding\': msg.encoding\n                }\n            except Exception as e:\n                self.get_logger().error(f\'Error processing RGB image: {e}\')\n\n    def depth_callback(self, msg):\n        """Process depth camera data"""\n        with self.pipeline_lock:\n            try:\n                cv_depth = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'32FC1\')\n                self.latest_images[\'depth\'] = {\n                    \'image\': cv_depth,\n                    \'timestamp\': msg.header.stamp,\n                    \'encoding\': msg.encoding\n                }\n            except Exception as e:\n                self.get_logger().error(f\'Error processing depth image: {e}\')\n\n    def imu_callback(self, msg):\n        """Process IMU data"""\n        with self.pipeline_lock:\n            self.latest_imu_data = msg\n\n    def lidar_callback(self, msg):\n        """Process LIDAR data"""\n        with self.pipeline_lock:\n            # Convert LaserScan to simplified point cloud representation\n            ranges = msg.ranges\n            angles = [msg.angle_min + i * msg.angle_increment for i in range(len(ranges))]\n\n            points = []\n            for i, r in enumerate(ranges):\n                if not np.isnan(r) and r < msg.range_max:\n                    x = r * np.cos(angles[i])\n                    y = r * np.sin(angles[i])\n                    points.append([x, y, 0.0])  # Simplified 2D representation\n\n            self.latest_point_cloud = np.array(points) if points else None\n\n    def initialize_perception_components(self):\n        """Initialize perception pipeline components"""\n        self.get_logger().info(\'Initializing perception pipeline components...\')\n\n        # Initialize object detection model\n        self.initialize_object_detection()\n\n        # Initialize feature tracking\n        self.initialize_feature_tracking()\n\n        # Initialize pose estimation\n        self.initialize_pose_estimation()\n\n        # Publish ready status\n        ready_msg = Bool()\n        ready_msg.data = True\n        self.status_pub.publish(ready_msg)\n\n        self.get_logger().info(\'Perception pipeline components initialized\')\n\n    def initialize_object_detection(self):\n        """Initialize object detection model"""\n        self.get_logger().info(\'Initializing object detection model...\')\n        # In a real implementation, this would load a pre-trained model\n        # For example: self.od_model = load_trt_model(\'yolo.trt\')\n        time.sleep(0.2)  # Simulate model loading time\n\n    def initialize_feature_tracking(self):\n        """Initialize feature tracking components"""\n        self.get_logger().info(\'Initializing feature tracking...\')\n        # In a real implementation, this would initialize feature tracking algorithms\n        # For example: self.feature_tracker = FeatureTracker()\n        time.sleep(0.1)  # Simulate initialization time\n\n    def initialize_pose_estimation(self):\n        """Initialize pose estimation components"""\n        self.get_logger().info(\'Initializing pose estimation...\')\n        # In a real implementation, this would initialize pose estimation models\n        # For example: self.pose_estimator = PoseEstimator()\n        time.sleep(0.1)  # Simulate initialization time\n\n    def process_perception_pipeline(self):\n        """Main perception pipeline processing loop"""\n        with self.pipeline_lock:\n            if not self.camera_matrix or \'rgb\' not in self.latest_images:\n                return\n\n            # Get latest RGB image\n            rgb_data = self.latest_images[\'rgb\']\n            rgb_image = rgb_data[\'image\']\n\n            # Process based on current mode\n            if self.perception_mode == PerceptionMode.OBJECT_DETECTION:\n                result = self.process_object_detection(rgb_image)\n            elif self.perception_mode == PerceptionMode.FEATURE_TRACKING:\n                result = self.process_feature_tracking(rgb_image)\n            elif self.perception_mode == PerceptionMode.SEMANTIC_SEGMENTATION:\n                result = self.process_semantic_segmentation(rgb_image)\n            elif self.perception_mode == PerceptionMode.POSE_ESTIMATION:\n                result = self.process_pose_estimation(rgb_image)\n            elif self.perception_mode == PerceptionMode.SLAM:\n                result = self.process_slam(rgb_image)\n            else:\n                return\n\n            if result:\n                self.publish_perception_results(result)\n\n    def process_object_detection(self, image):\n        """Process object detection using Isaac\'s optimized algorithms"""\n        # In a real implementation, this would use Isaac ROS DNN packages\n        # For this example, we\'ll implement a simple color-based detection\n        detections = []\n\n        # Convert to HSV for color-based detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define color ranges for common objects\n        color_ranges = [\n            (np.array([0, 50, 50]), np.array([10, 255, 255]), \'red_object\'),    # Red\n            (np.array([100, 50, 50]), np.array([130, 255, 255]), \'blue_object\'), # Blue\n            (np.array([30, 50, 50]), np.array([80, 255, 255]), \'green_object\'),  # Green\n        ]\n\n        for lower, upper, obj_type in color_ranges:\n            mask = cv2.inRange(hsv, lower, upper)\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            for contour in contours:\n                area = cv2.contourArea(contour)\n                if area > 1000:  # Filter small contours\n                    x, y, w, h = cv2.boundingRect(contour)\n                    detections.append({\n                        \'class\': obj_type,\n                        \'confidence\': 0.8,\n                        \'bbox\': (x, y, w, h),\n                        \'area\': area,\n                        \'center\': (x + w//2, y + h//2)\n                    })\n\n        return PerceptionResult(\n            timestamp=time.time(),\n            objects=detections,\n            features=[],\n            pose=None,\n            point_cloud=None,\n            confidence=0.8 if detections else 0.0\n        )\n\n    def process_feature_tracking(self, image):\n        """Process feature tracking using Isaac\'s optimized algorithms"""\n        # In a real implementation, this would use Isaac ROS feature tracking\n        # For this example, we\'ll implement simple Shi-Tomasi corner detection\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Detect good features to track\n        corners = cv2.goodFeaturesToTrack(\n            gray,\n            maxCorners=100,\n            qualityLevel=0.01,\n            minDistance=10,\n            blockSize=3\n        )\n\n        features = []\n        if corners is not None:\n            for corner in corners:\n                x, y = corner.ravel()\n                features.append((float(x), float(y)))\n\n        return PerceptionResult(\n            timestamp=time.time(),\n            objects=[],\n            features=features,\n            pose=None,\n            point_cloud=None,\n            confidence=0.9\n        )\n\n    def process_semantic_segmentation(self, image):\n        """Process semantic segmentation (placeholder implementation)"""\n        # In a real implementation, this would use Isaac ROS segmentation packages\n        # For now, return a simple result\n        return PerceptionResult(\n            timestamp=time.time(),\n            objects=[],\n            features=[],\n            pose=None,\n            point_cloud=None,\n            confidence=0.85\n        )\n\n    def process_pose_estimation(self, image):\n        """Process pose estimation (placeholder implementation)"""\n        # In a real implementation, this would use Isaac ROS pose estimation\n        # For now, return a simple result\n        pose_msg = PoseStamped()\n        pose_msg.header.frame_id = "camera_rgb_optical_frame"\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\n        pose_msg.pose.position.x = 1.0\n        pose_msg.pose.position.y = 0.0\n        pose_msg.pose.position.z = 0.0\n        pose_msg.pose.orientation.w = 1.0\n\n        return PerceptionResult(\n            timestamp=time.time(),\n            objects=[],\n            features=[],\n            pose=pose_msg,\n            point_cloud=None,\n            confidence=0.9\n        )\n\n    def process_slam(self, image):\n        """Process SLAM (placeholder implementation)"""\n        # In a real implementation, this would use Isaac ROS Visual SLAM\n        # For now, return a simple result\n        return PerceptionResult(\n            timestamp=time.time(),\n            objects=[],\n            features=[],\n            pose=None,\n            point_cloud=None,\n            confidence=0.95\n        )\n\n    def publish_perception_results(self, result: PerceptionResult):\n        """Publish perception results to ROS topics"""\n        # Publish object detections\n        if result.objects:\n            marker_array = self.create_object_markers(result.objects)\n            self.object_pub.publish(marker_array)\n\n        # Publish features\n        if result.features:\n            feature_markers = self.create_feature_markers(result.features)\n            self.feature_pub.publish(feature_markers)\n\n        # Publish pose\n        if result.pose:\n            self.pose_pub.publish(result.pose)\n\n    def create_object_markers(self, objects):\n        """Create visualization markers for object detections"""\n        marker_array = MarkerArray()\n\n        for i, obj in enumerate(objects):\n            marker = self.create_object_marker(obj, i)\n            marker_array.markers.append(marker)\n\n        return marker_array\n\n    def create_object_marker(self, obj, id_num):\n        """Create a single object marker"""\n        from visualization_msgs.msg import Marker\n        from geometry_msgs.msg import Point\n\n        marker = Marker()\n        marker.header.frame_id = "camera_rgb_optical_frame"\n        marker.header.stamp = self.get_clock().now().to_msg()\n        marker.ns = "perception_objects"\n        marker.id = id_num\n        marker.type = Marker.CUBE\n        marker.action = Marker.ADD\n\n        # Set position based on bounding box center\n        x, y, w, h = obj[\'bbox\']\n        marker.pose.position.x = (x + w/2) / 100.0  # Scale down for visualization\n        marker.pose.position.y = (y + h/2) / 100.0\n        marker.pose.position.z = 1.0  # Fixed depth for visualization\n\n        # Set scale based on bounding box size\n        marker.scale.x = w / 100.0\n        marker.scale.y = h / 100.0\n        marker.scale.z = 0.2  # Fixed height\n\n        # Set color based on object class\n        if \'red\' in obj[\'class\']:\n            marker.color.r = 1.0\n            marker.color.g = 0.0\n            marker.color.b = 0.0\n        elif \'blue\' in obj[\'class\']:\n            marker.color.r = 0.0\n            marker.color.g = 0.0\n            marker.color.b = 1.0\n        elif \'green\' in obj[\'class\']:\n            marker.color.r = 0.0\n            marker.color.g = 1.0\n            marker.color.b = 0.0\n        else:\n            marker.color.r = 1.0\n            marker.color.g = 1.0\n            marker.color.b = 0.0\n\n        marker.color.a = 0.7\n\n        # Set object label\n        marker.text = f"{obj[\'class\']}: {obj[\'confidence\']:.2f}"\n\n        return marker\n\n    def create_feature_markers(self, features):\n        """Create visualization markers for features"""\n        marker_array = MarkerArray()\n\n        for i, (x, y) in enumerate(features):\n            marker = self.create_feature_marker(x, y, i)\n            marker_array.markers.append(marker)\n\n        return marker_array\n\n    def create_feature_marker(self, x, y, id_num):\n        """Create a single feature marker"""\n        from visualization_msgs.msg import Marker\n\n        marker = Marker()\n        marker.header.frame_id = "camera_rgb_optical_frame"\n        marker.header.stamp = self.get_clock().now().to_msg()\n        marker.ns = "perception_features"\n        marker.id = id_num\n        marker.type = Marker.SPHERE\n        marker.action = Marker.ADD\n\n        # Set position\n        marker.pose.position.x = x / 100.0  # Scale down for visualization\n        marker.pose.position.y = y / 100.0\n        marker.pose.position.z = 0.5  # Fixed depth for visualization\n\n        # Set scale\n        marker.scale.x = 0.02\n        marker.scale.y = 0.02\n        marker.scale.z = 0.02\n\n        # Set color\n        marker.color.r = 0.0\n        marker.color.g = 1.0\n        marker.color.b = 0.0\n        marker.color.a = 0.8\n\n        return marker\n\n    def set_perception_mode(self, mode: PerceptionMode):\n        """Set the current perception mode"""\n        with self.pipeline_lock:\n            self.perception_mode = mode\n            self.get_logger().info(f\'Switched to perception mode: {mode.value}\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacPerceptionPipeline()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Shutting down Isaac Perception Pipeline\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.p,{children:"Create the perception pipeline configuration:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# isaac_humanoid_perception/config/perception_pipeline_config.yaml\nisaac_perception_pipeline:\n  ros__parameters:\n    # Perception modes\n    modes:\n      object_detection_enabled: true\n      semantic_segmentation_enabled: false\n      feature_tracking_enabled: true\n      pose_estimation_enabled: true\n      slam_enabled: true\n\n    # Object detection parameters\n    object_detection:\n      model_type: "tensorrt_yolo"\n      model_path: "/path/to/yolo_model.trt"\n      confidence_threshold: 0.7\n      nms_threshold: 0.4\n      max_objects: 50\n      tracking_enabled: true\n\n    # Feature tracking parameters\n    feature_tracking:\n      max_features: 1000\n      quality_level: 0.01\n      min_distance: 10\n      block_size: 3\n      use_harris_detector: false\n      k: 0.04\n\n    # Semantic segmentation parameters\n    semantic_segmentation:\n      model_type: "tensorrt_seg"\n      model_path: "/path/to/segmentation_model.trt"\n      confidence_threshold: 0.8\n      classes: ["person", "chair", "table", "floor", "wall"]\n\n    # Pose estimation parameters\n    pose_estimation:\n      model_type: "tensorrt_pose"\n      model_path: "/path/to/pose_model.trt"\n      confidence_threshold: 0.8\n      max_poses: 10\n\n    # SLAM parameters\n    slam:\n      enable_rectification: true\n      enable_imu_fusion: true\n      map_frame: "map"\n      odom_frame: "odom"\n      base_frame: "base_link"\n      publish_tf: true\n\n    # Processing parameters\n    processing:\n      frame_rate: 10.0  # Hz\n      queue_size: 10\n      max_queue_size: 100\n      enable_multithreading: true\n      synchronization_window: 0.1  # seconds\n\n    # GPU acceleration settings\n    gpu:\n      device_id: 0\n      memory_fraction: 0.8  # 80% of available GPU memory\n      enable_tensorrt: true\n      tensorrt_precision: "fp16"\n      use_cuda_graph: true\n\n    # Sensor fusion parameters\n    sensor_fusion:\n      enable_camera_imu: true\n      enable_camera_lidar: false\n      enable_multi_camera: false\n      timestamp_tolerance: 0.05  # seconds\n\n    # Performance monitoring\n    performance:\n      enable_profiling: true\n      publish_statistics: true\n      statistics_topic: "/isaac/perception/performance"\n      warning_threshold: 0.8  # 80% of target frame rate\n'})}),"\n",(0,r.jsx)(n.p,{children:"Create the launch file for the perception pipeline:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:"\x3c!-- isaac_humanoid_perception/launch/isaac_perception_pipeline.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    namespace = LaunchConfiguration('namespace')\n    perception_mode = LaunchConfiguration('perception_mode')\n\n    return LaunchDescription([\n        # Declare launch arguments\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation (Gazebo) clock if true'\n        ),\n        DeclareLaunchArgument(\n            'namespace',\n            default_value='',\n            description='Robot namespace'\n        ),\n        DeclareLaunchArgument(\n            'perception_mode',\n            default_value='object_detection',\n            description='Perception mode: object_detection, semantic_segmentation, feature_tracking, pose_estimation, slam'\n        ),\n\n        # Isaac Perception Pipeline\n        Node(\n            package='isaac_humanoid_perception',\n            executable='isaac_perception_pipeline',\n            name='isaac_perception_pipeline',\n            namespace=namespace,\n            parameters=[\n                os.path.join(\n                    get_package_share_directory('isaac_humanoid_perception'),\n                    'config',\n                    'perception_pipeline_config.yaml'\n                ),\n                {'use_sim_time': use_sim_time}\n            ],\n            output='screen',\n            respawn=True,\n            respawn_delay=2\n        ),\n\n        # Isaac ROS Image Pipeline (Color Conversion)\n        Node(\n            package='isaac_ros_image_pipeline',\n            executable='isaac_ros_color_convert',\n            name='color_convert_node',\n            namespace=namespace,\n            parameters=[\n                {\n                    'input_encoding': 'rgb8',\n                    'output_encoding': 'bgr8',\n                    'use_sim_time': use_sim_time\n                }\n            ],\n            remappings=[\n                ('image_raw', '/camera/rgb/image_raw'),\n                ('image_color_converted', '/camera/rgb/image_converted')\n            ],\n            output='screen'\n        ),\n\n        # Isaac ROS Depth Preprocessor\n        Node(\n            package='isaac_ros_depth_preprocessor',\n            executable='isaac_ros_depth_preprocessor',\n            name='depth_preprocessor_node',\n            namespace=namespace,\n            parameters=[\n                {\n                    'input_encoding': '16UC1',\n                    'output_encoding': '32FC1',\n                    'use_sim_time': use_sim_time\n                }\n            ],\n            remappings=[\n                ('depth/image', '/camera/depth/image_raw'),\n                ('depth/image_processed', '/camera/depth/image_processed')\n            ],\n            output='screen'\n        ),\n\n        # Isaac ROS AprilTag Detector (for pose estimation)\n        Node(\n            package='isaac_ros_apriltag',\n            executable='isaac_ros_apriltag',\n            name='apriltag_node',\n            namespace=namespace,\n            parameters=[\n                {\n                    'family': 'tag36h11',\n                    'size': 0.1,\n                    'max_tags': 10,\n                    'use_sim_time': use_sim_time\n                }\n            ],\n            remappings=[\n                ('image', '/camera/rgb/image_raw'),\n                ('camera_info', '/camera/rgb/camera_info'),\n                ('detections', '/isaac/perception/apriltag_detections')\n            ],\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,r.jsx)(n.p,{children:"Create a perception pipeline monitor:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# isaac_humanoid_perception/isaac_humanoid_perception/perception_monitor.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom visualization_msgs.msg import MarkerArray\nfrom std_msgs.msg import Bool, Float32\nfrom diagnostic_msgs.msg import DiagnosticArray, DiagnosticStatus\nimport time\nfrom datetime import datetime\nimport threading\n\nclass IsaacPerceptionMonitor(Node):\n    \"\"\"\n    Monitor for Isaac perception pipeline performance and diagnostics\n    \"\"\"\n    def __init__(self):\n        super().__init__('isaac_perception_monitor')\n\n        # Initialize monitoring components\n        self.monitoring_data = {\n            'object_detections': 0,\n            'feature_detections': 0,\n            'processing_times': [],\n            'frame_rates': [],\n            'memory_usage': [],\n            'last_update': time.time()\n        }\n        self.monitoring_lock = threading.Lock()\n\n        # Publishers for monitoring data\n        self.fps_pub = self.create_publisher(Float32, '/isaac/perception/fps', 10)\n        self.diag_pub = self.create_publisher(DiagnosticArray, '/isaac/perception/diagnostics', 10)\n\n        # Subscribers for perception outputs\n        self.object_sub = self.create_subscription(\n            MarkerArray, '/isaac/perception/objects', self.object_callback, 10\n        )\n        self.feature_sub = self.create_subscription(\n            MarkerArray, '/isaac/perception/features', self.feature_callback, 10\n        )\n        self.ready_sub = self.create_subscription(\n            Bool, '/isaac/perception/ready', self.ready_callback, 10\n        )\n\n        # Timer for performance monitoring\n        self.monitor_timer = self.create_timer(1.0, self.publish_monitoring_data)\n\n        self.get_logger().info('Isaac Perception Monitor initialized')\n\n    def object_callback(self, msg):\n        \"\"\"Track object detection frequency\"\"\"\n        with self.monitoring_lock:\n            self.monitoring_data['object_detections'] += len(msg.markers)\n            self.monitoring_data['last_update'] = time.time()\n\n    def feature_callback(self, msg):\n        \"\"\"Track feature detection frequency\"\"\"\n        with self.monitoring_lock:\n            self.monitoring_data['feature_detections'] += len(msg.markers)\n            self.monitoring_data['last_update'] = time.time()\n\n    def ready_callback(self, msg):\n        \"\"\"Handle perception pipeline ready status\"\"\"\n        if msg.data:\n            self.get_logger().info('Isaac Perception Pipeline is ready')\n\n    def publish_monitoring_data(self):\n        \"\"\"Publish monitoring and diagnostic information\"\"\"\n        with self.monitoring_lock:\n            # Calculate current frame rate\n            current_time = time.time()\n            time_diff = current_time - self.monitoring_data['last_update']\n\n            if time_diff > 0:\n                fps = len(self.monitoring_data['processing_times']) / time_diff if time_diff > 0 else 0.0\n            else:\n                fps = 0.0\n\n            # Publish FPS\n            fps_msg = Float32()\n            fps_msg.data = fps\n            self.fps_pub.publish(fps_msg)\n\n            # Publish diagnostics\n            self.publish_diagnostics()\n\n            # Reset counters\n            self.monitoring_data['object_detections'] = 0\n            self.monitoring_data['feature_detections'] = 0\n            self.monitoring_data['last_update'] = current_time\n\n    def publish_diagnostics(self):\n        \"\"\"Publish diagnostic information\"\"\"\n        diag_array = DiagnosticArray()\n        diag_array.header.stamp = self.get_clock().now().to_msg()\n\n        # Perception performance diagnostic\n        perf_diag = DiagnosticStatus()\n        perf_diag.name = 'Isaac Perception Performance'\n        perf_diag.hardware_id = 'perception_pipeline'\n\n        # Calculate average processing time if available\n        avg_processing_time = 0.0\n        if self.monitoring_data['processing_times']:\n            avg_processing_time = sum(self.monitoring_data['processing_times']) / len(self.monitoring_data['processing_times'])\n\n        # Determine status based on performance\n        if avg_processing_time > 0.1:  # > 100ms processing time\n            perf_diag.level = DiagnosticStatus.ERROR\n            perf_diag.message = 'High processing time detected'\n        elif avg_processing_time > 0.05:  # > 50ms processing time\n            perf_diag.level = DiagnosticStatus.WARN\n            perf_diag.message = 'Elevated processing time'\n        else:\n            perf_diag.level = DiagnosticStatus.OK\n            perf_diag.message = 'Processing time nominal'\n\n        # Add performance metrics\n        perf_diag.values = [\n            {'key': 'Average Processing Time (ms)', 'value': f'{avg_processing_time * 1000:.2f}'},\n            {'key': 'Current FPS', 'value': f'{len(self.monitoring_data[\"frame_rates\"]):.2f}'},\n            {'key': 'Objects Detected (last sec)', 'value': str(self.monitoring_data['object_detections'])},\n            {'key': 'Features Detected (last sec)', 'value': str(self.monitoring_data['feature_detections'])},\n        ]\n\n        diag_array.status.append(perf_diag)\n\n        # Publish diagnostics\n        self.diag_pub.publish(diag_array)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacPerceptionMonitor()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down Isaac Perception Monitor')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"4-hardwaregpu-notes",children:"4. Hardware/GPU Notes"}),"\n",(0,r.jsx)(n.h3,{id:"perception-pipeline-gpu-requirements",children:"Perception Pipeline GPU Requirements"}),"\n",(0,r.jsx)(n.p,{children:"Isaac perception pipelines have varying GPU requirements based on the specific perception tasks:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Object Detection"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Minimum"}),": RTX 4070 Ti (12GB VRAM) for basic models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Recommended"}),": RTX 4080/4090 (16-24GB VRAM) for complex models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory"}),": 2-8GB depending on model size and input resolution"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Compute"}),": Tensor cores recommended for INT8/FP16 inference"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Semantic Segmentation"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory"}),": 4-12GB VRAM for high-resolution segmentation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Compute"}),": High memory bandwidth required for pixel-level processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model Size"}),": Larger models require more VRAM but provide better accuracy"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Feature Tracking"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory"}),": 1-4GB VRAM for feature extraction and matching"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Compute"}),": CUDA cores optimized for parallel feature processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time Requirements"}),": High frame rate processing for tracking"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Pose Estimation"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory"}),": 2-6GB VRAM for pose estimation models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Compute"}),": Mixed precision operations for efficiency"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Latency"}),": Low latency critical for real-time applications"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"memory-management-strategies",children:"Memory Management Strategies"}),"\n",(0,r.jsx)(n.p,{children:"For optimal perception pipeline performance:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory Pooling"}),": Pre-allocate GPU memory pools for different processing stages"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Zero-Copy Memory"}),": Use CUDA zero-copy for frequent CPU-GPU transfers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unified Memory"}),": Leverage CUDA unified memory for automatic management"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory Monitoring"}),": Continuously monitor GPU memory usage to prevent overflow"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"jetson-platform-considerations",children:"Jetson Platform Considerations"}),"\n",(0,r.jsx)(n.p,{children:"When running perception pipelines on Jetson platforms:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory Architecture"}),": Unified memory architecture simplifies memory management"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Power Efficiency"}),": Perception pipelines optimized for power-constrained environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Thermal Management"}),": Monitor temperature during intensive perception tasks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"I/O Bandwidth"}),": Maximize sensor data bandwidth for real-time processing"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"TensorRT Integration"}),": Use TensorRT for optimized deep learning inference"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"CUDA Streams"}),": Use multiple CUDA streams for overlapping operations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Async Processing"}),": Implement asynchronous processing to maximize GPU utilization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Batch Processing"}),": Process multiple inputs simultaneously when possible"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model Quantization"}),": Use INT8 quantization to reduce memory usage and increase speed"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"5-simulation-path",children:"5. Simulation Path"}),"\n",(0,r.jsx)(n.p,{children:"To implement Isaac perception pipelines in simulation:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Isaac Sim Setup"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Launch Isaac Sim with perception sensors\ncd ~/isaac-sim\npython3 -m omni.isaac.kit --summary-cache-path ./cache\n\n# Configure perception sensors in simulation\n# Set up RGB-D cameras, IMU, LIDAR for humanoid robot\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Perception Pipeline Testing"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Launch perception pipeline in simulation\nros2 launch isaac_humanoid_perception isaac_perception_pipeline_sim.launch.py\n\n# Test different perception modes\nros2 run isaac_humanoid_perception perception_mode_switcher --mode object_detection\nros2 run isaac_humanoid_perception perception_mode_switcher --mode feature_tracking\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance Validation"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Test perception accuracy in simulated environments"}),"\n",(0,r.jsx)(n.li,{children:"Validate processing frame rates and latency"}),"\n",(0,r.jsx)(n.li,{children:"Measure GPU memory usage and performance"}),"\n",(0,r.jsx)(n.li,{children:"Verify safety systems in simulation"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"6-real-world-path",children:"6. Real-World Path"}),"\n",(0,r.jsx)(n.p,{children:"For real-world deployment of Isaac perception pipelines:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Hardware Integration"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Integrate perception sensors with humanoid robot platform"}),"\n",(0,r.jsx)(n.li,{children:"Calibrate cameras and depth sensors"}),"\n",(0,r.jsx)(n.li,{children:"Configure IMU and other perception sensors"}),"\n",(0,r.jsx)(n.li,{children:"Validate sensor data quality and timing"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"System Integration"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Build Isaac perception workspace\ncd ~/isaac_perception_ws\ncolcon build --packages-select isaac_humanoid_perception\nsource install/setup.bash\n\n# Launch perception pipeline on robot\nros2 launch isaac_humanoid_perception isaac_perception_pipeline.launch.py\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Validation and Testing"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Test perception accuracy in real environments"}),"\n",(0,r.jsx)(n.li,{children:"Validate real-time performance requirements"}),"\n",(0,r.jsx)(n.li,{children:"Verify safety systems and emergency stops"}),"\n",(0,r.jsx)(n.li,{children:"Ensure system stability and reliability"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"7-spec-build-test-checklist",children:"7. Spec-Build-Test checklist"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Isaac perception pipeline node implemented and functional"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Multi-modal sensor integration working correctly"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Object detection implementation functional"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Feature tracking implementation working"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Semantic segmentation placeholder implemented"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Pose estimation placeholder implemented"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","SLAM placeholder implemented"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Perception mode switching implemented"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Visualization markers published correctly"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Configuration parameters properly set"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Launch files created and tested"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Performance monitoring implemented"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Diagnostic reporting functional"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","GPU memory management implemented"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Isaac perception pipeline validated in simulation"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"8-apa-citations",children:"8. APA citations"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["NVIDIA Corporation. (2023). ",(0,r.jsx)(n.em,{children:"Isaac ROS: Perception and Navigation Packages"}),". NVIDIA Developer Documentation. Retrieved from ",(0,r.jsx)(n.a,{href:"https://docs.nvidia.com/isaac/isaac_ros/",children:"https://docs.nvidia.com/isaac/isaac_ros/"})]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Redmon, J., & Farhadi, A. (2018). YOLOv3: An incremental improvement. ",(0,r.jsx)(n.em,{children:"arXiv preprint arXiv:1804.02767"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: Towards real-time object detection with region proposal networks. ",(0,r.jsx)(n.em,{children:"Advances in Neural Information Processing Systems"}),", 28, 91-99."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Long, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic segmentation. ",(0,r.jsx)(n.em,{children:"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"}),", 3431-3440."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Newcombe, R. A., Lovegrove, S. J., & Davison, A. J. (2011). DTAM: Dense tracking and mapping in real-time. ",(0,r.jsx)(n.em,{children:"IEEE International Conference on Computer Vision"}),", 2320-2327."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Geiger, A., Lenz, P., & Urtasun, R. (2012). Are we ready for autonomous driving? The KITTI vision benchmark suite. ",(0,r.jsx)(n.em,{children:"Conference on Computer Vision and Pattern Recognition"}),", 3354-3361."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Rublee, E., Rabaud, V., Konolige, K., & Bradski, G. (2011). ORB: An efficient alternative to SIFT or SURF. ",(0,r.jsx)(n.em,{children:"IEEE International Conference on Computer Vision"}),", 2564-2571."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. ",(0,r.jsx)(n.em,{children:"International Journal of Computer Vision"}),", 60(2), 91-110."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Bay, H., Tuytelaars, T., & Van Gool, L. (2006). SURF: Speeded up robust features. ",(0,r.jsx)(n.em,{children:"European Conference on Computer Vision"}),", 404-417."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Mur-Artal, R., Montiel, J. M. M., & Tard\xf3s, J. D. (2015). ORB-SLAM: A versatile and accurate monocular SLAM system. ",(0,r.jsx)(n.em,{children:"IEEE Transactions on Robotics"}),", 31(5), 1147-1163."]}),"\n"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(6540);const r={},a=s.createContext(r);function t(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);