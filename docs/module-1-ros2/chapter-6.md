---
title: Chapter 6 - ROS 2 Pipeline Implementation
sidebar_position: 6
---

# Chapter 6: ROS 2 Pipeline Implementation

## Why This Concept Matters for Humanoids

Creating a complete ROS 2 pipeline is essential for humanoid robotics as it establishes the communication framework that enables coordinated movement, sensory processing, and intelligent decision-making. A well-designed pipeline ensures that data flows efficiently between perception, planning, and actuation systems, which is crucial for the responsive and coordinated behavior required in humanoid robots. Understanding pipeline implementation allows robotics engineers to build robust systems that can handle the complex interplay of multiple subsystems required for humanoid locomotion and interaction.

## Theory

A ROS 2 pipeline consists of interconnected nodes that process data through topics, services, and actions. The pipeline typically follows a pattern where sensor data is collected, processed through various algorithms, and then used to control actuators or make decisions. In humanoid robotics, this might involve:

- **Data Ingestion**: Collecting sensor data from cameras, IMUs, joint encoders, and other sensors
- **Preprocessing**: Filtering, calibration, and initial processing of raw sensor data
- **Fusion**: Combining data from multiple sensors to create a coherent understanding of the environment
- **Planning**: Generating motion plans, trajectories, or behavioral decisions
- **Execution**: Sending commands to actuators and monitoring their execution
- **Feedback**: Monitoring system state and adjusting plans as needed

The pipeline must handle real-time constraints, manage computational resources efficiently, and provide fault tolerance to ensure the safety and stability of humanoid robots.

## Implementation

Let's implement a complete ROS 2 pipeline for humanoid robot control. This example will demonstrate a simple perception-action pipeline that processes camera input to generate joint commands.

First, create the main pipeline launch file:

```xml
<!-- my-website/docs/module-1-ros2/examples/pipeline_launch.py -->
from launch import LaunchDescription
from launch.actions import DeclareLaunchArgument
from launch.substitutions import LaunchConfiguration
from launch_ros.actions import Node

def generate_launch_description():
    return LaunchDescription([
        # Camera input node
        Node(
            package='image_publisher',
            executable='image_publisher_node',
            name='camera_driver',
            parameters=[
                {'image_path': '/path/to/camera'},
                {'publish_rate': 30.0}
            ]
        ),

        # Perception processing node
        Node(
            package='my_robot_perception',
            executable='object_detector',
            name='object_detector',
            parameters=[
                {'model_path': 'yolov8n.pt'},
                {'confidence_threshold': 0.5}
            ]
        ),

        # Motion planning node
        Node(
            package='my_robot_planning',
            executable='motion_planner',
            name='motion_planner',
            parameters=[
                {'planning_frequency': 10.0},
                {'max_velocity': 0.5}
            ]
        ),

        # Robot controller
        Node(
            package='my_robot_control',
            executable='joint_controller',
            name='joint_controller',
            parameters=[
                {'control_rate': 100.0},
                {'max_effort': 100.0}
            ]
        )
    ])
```

Now, let's implement a perception node that processes camera data:

```python
#!/usr/bin/env python3
# my-website/docs/module-1-ros2/examples/perception_node.py

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from vision_msgs.msg import Detection2DArray
from cv_bridge import CvBridge
import cv2
import numpy as np

class ObjectDetector(Node):
    def __init__(self):
        super().__init__('object_detector')

        # Create publisher and subscriber
        self.subscription = self.create_subscription(
            Image,
            'camera/image_raw',
            self.image_callback,
            10)

        self.publisher = self.create_publisher(
            Detection2DArray,
            'detections',
            10)

        # Initialize OpenCV bridge
        self.bridge = CvBridge()

        # Log initialization
        self.get_logger().info('Object Detector Node Initialized')

    def image_callback(self, msg):
        try:
            # Convert ROS Image message to OpenCV image
            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")

            # Process image (simple example: detect circles)
            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
            circles = cv2.HoughCircles(
                gray,
                cv2.HOUGH_GRADIENT,
                1,
                20,
                param1=50,
                param2=30,
                minRadius=10,
                maxRadius=100
            )

            # Create detection message
            detection_msg = Detection2DArray()
            detection_msg.header = msg.header

            if circles is not None:
                circles = np.round(circles[0, :]).astype("int")
                for (x, y, r) in circles:
                    # Draw circle on image for visualization
                    cv2.circle(cv_image, (x, y), r, (0, 255, 0), 4)

            # Publish detections
            self.publisher.publish(detection_msg)

        except Exception as e:
            self.get_logger().error(f'Error processing image: {str(e)}')

def main(args=None):
    rclpy.init(args=args)
    detector = ObjectDetector()

    try:
        rclpy.spin(detector)
    except KeyboardInterrupt:
        pass
    finally:
        detector.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

Next, let's create a motion planning node that takes detections and generates movement commands:

```python
#!/usr/bin/env python3
# my-website/docs/module-1-ros2/examples/planning_node.py

import rclpy
from rclpy.node import Node
from vision_msgs.msg import Detection2DArray
from geometry_msgs.msg import Twist
from std_msgs.msg import Bool

class MotionPlanner(Node):
    def __init__(self):
        super().__init__('motion_planner')

        # Subscribers
        self.detection_sub = self.create_subscription(
            Detection2DArray,
            'detections',
            self.detection_callback,
            10)

        # Publishers
        self.cmd_vel_pub = self.create_publisher(
            Twist,
            'cmd_vel',
            10)

        self.emergency_stop_pub = self.create_publisher(
            Bool,
            'emergency_stop',
            10)

        # Parameters
        self.target_distance = self.declare_parameter(
            'target_distance', 1.0).value
        self.max_linear_speed = self.declare_parameter(
            'max_linear_speed', 0.5).value

        # Internal state
        self.detection_received = False
        self.last_detection_time = self.get_clock().now()

        # Create timer for safety checks
        self.timer = self.create_timer(0.1, self.safety_timer_callback)

        self.get_logger().info('Motion Planner Node Initialized')

    def detection_callback(self, msg):
        self.detection_received = True
        self.last_detection_time = self.get_clock().now()

        # Process detections to generate motion commands
        if len(msg.detections) > 0:
            # Example: Move towards detected object
            cmd_vel = Twist()
            cmd_vel.linear.x = self.max_linear_speed * 0.5  # Move forward slowly
            cmd_vel.angular.z = 0.0  # No rotation for now

            self.cmd_vel_pub.publish(cmd_vel)
        else:
            # Stop if no detections
            cmd_vel = Twist()
            cmd_vel.linear.x = 0.0
            cmd_vel.angular.z = 0.0
            self.cmd_vel_pub.publish(cmd_vel)

    def safety_timer_callback(self):
        # Check if we've received detections recently
        time_since_last_detection = (
            self.get_clock().now() - self.last_detection_time
        ).nanoseconds / 1e9

        if time_since_last_detection > 2.0:  # 2 seconds without detection
            # Emergency stop
            stop_msg = Bool()
            stop_msg.data = True
            self.emergency_stop_pub.publish(stop_msg)

            # Stop movement
            cmd_vel = Twist()
            cmd_vel.linear.x = 0.0
            cmd_vel.angular.z = 0.0
            self.cmd_vel_pub.publish(cmd_vel)

def main(args=None):
    rclpy.init(args=args)
    planner = MotionPlanner()

    try:
        rclpy.spin(planner)
    except KeyboardInterrupt:
        pass
    finally:
        planner.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

Finally, let's create a complete pipeline configuration file:

```yaml
# my-website/docs/module-1-ros2/examples/pipeline_config.yaml
# Pipeline Configuration for Humanoid Robot

robot_name: "humanoid_robot"
pipeline_name: "perception_action_pipeline"

nodes:
  camera_driver:
    package: "image_publisher"
    executable: "image_publisher_node"
    parameters:
      image_path: "/dev/video0"
      publish_rate: 30.0
      camera_info_url: "package://my_robot_description/config/camera_info.yaml"

  object_detector:
    package: "my_robot_perception"
    executable: "object_detector"
    parameters:
      model_path: "yolov8n.pt"
      confidence_threshold: 0.5
      max_detections: 10

  motion_planner:
    package: "my_robot_planning"
    executable: "motion_planner"
    parameters:
      target_distance: 1.0
      max_linear_speed: 0.5
      max_angular_speed: 1.0

  joint_controller:
    package: "my_robot_control"
    executable: "joint_controller"
    parameters:
      control_rate: 100.0
      max_effort: 100.0
      joint_names: ["hip_joint", "knee_joint", "ankle_joint", "shoulder_joint", "elbow_joint"]

topics:
  camera/image_raw:
    type: "sensor_msgs/Image"
    qos:
      history: "keep_last"
      depth: 10
      reliability: "reliable"
      durability: "volatile"

  detections:
    type: "vision_msgs/Detection2DArray"
    qos:
      history: "keep_last"
      depth: 10
      reliability: "best_effort"

  cmd_vel:
    type: "geometry_msgs/Twist"
    qos:
      history: "keep_last"
      depth: 1
      reliability: "reliable"

safety:
  emergency_stop_timeout: 2.0  # seconds
  max_velocity_threshold: 1.0  # m/s
  collision_distance_threshold: 0.5  # meters

performance:
  pipeline_frequency: 30.0  # Hz
  max_pipeline_latency: 0.1  # seconds
```

## Hardware/GPU Notes

For implementing ROS 2 pipelines on humanoid robots, consider these hardware requirements:

- **CPU**: Multi-core processor (8+ cores recommended) for handling multiple concurrent nodes
- **RAM**: 16GB minimum, 32GB+ recommended for complex perception tasks
- **GPU**: For perception-intensive pipelines, NVIDIA GPU with CUDA support (RTX 4070 Ti minimum) for accelerated computer vision
- **Network**: Gigabit Ethernet or high-bandwidth wireless for sensor data transmission
- **Storage**: SSD with 500GB+ for logs, maps, and runtime data
- **Real-time capability**: Consider using PREEMPT_RT kernel for deterministic timing

For humanoid robots specifically, joint control requires high-frequency communication (100Hz+) with low latency to maintain stability.

## Simulation Path

In simulation, you can test your pipeline using Gazebo or Isaac Sim:

1. **Start the simulation environment**:
   ```bash
   ros2 launch my_robot_gazebo humanoid_world.launch.py
   ```

2. **Launch your pipeline**:
   ```bash
   ros2 launch my_robot_pipeline pipeline.launch.py
   ```

3. **Monitor the pipeline**:
   ```bash
   # Check node connections
   ros2 run rqt_graph rqt_graph

   # Monitor topics
   ros2 topic echo /detections

   # Check performance
   ros2 run topic_tools relay /camera/image_raw /monitor
   ```

4. **Visualize in RViz2**:
   ```bash
   ros2 run rviz2 rviz2 -d my_robot_pipeline.rviz
   ```

## Real-World Path

For deployment on real humanoid hardware:

1. **Validate safety systems**:
   - Ensure emergency stop functionality works
   - Verify joint limits and effort constraints
   - Test communication timeouts

2. **Calibrate sensors**:
   - Camera intrinsic/extrinsic calibration
   - IMU alignment and bias correction
   - Joint encoder zero positions

3. **Optimize for real-time performance**:
   - Profile node execution times
   - Adjust QoS settings for reliability
   - Implement watchdog timers

4. **Test incrementally**:
   - Start with simple single-node tests
   - Gradually add pipeline components
   - Test safety scenarios thoroughly

## Spec-Build-Test Checklist

- [ ] Pipeline launch file correctly defines all required nodes
- [ ] Node parameters are configurable via YAML
- [ ] Topic connections match expected message types
- [ ] QoS policies are appropriate for real-time requirements
- [ ] Safety mechanisms (emergency stop, timeouts) are implemented
- [ ] Error handling and logging are comprehensive
- [ ] Performance metrics are monitored and logged
- [ ] Pipeline can be started/stopped cleanly
- [ ] All dependencies are properly declared in package.xml
- [ ] Pipeline works in both simulation and real hardware (when applicable)

## APA Citations

- Foote, T., Lalancette, C., & Quigley, J. (2016). ROS 2: Towards a robot platform for next generation robots. *Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, 4698-4704.

- Quigley, M., Gerkey, B., & Smart, W. D. (2009). Programming robots with ROS: A practical introduction to the Robot Operating System. *Communications of the ACM*, 57(9), 82-91.

- Kammerl, J., Holzer, S., Rusu, R. B., & Konolige, K. (2012). Real-time automated parameter tuning for multi-dimensional point cloud processing. *Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)*, 2459-2465.

- Open Robotics. (2021). ROS 2 Documentation: Composable Nodes. Retrieved from https://docs.ros.org/en/rolling/Tutorials/Composable-Nodes.html

- LÃ¼tkebohle, I., & Axer, P. (2018). Design patterns for ROS-based systems: Engineering software for robots. *Proceedings of the Workshop on Software Engineering for Robotics*, 1-6.